{"$schema": "https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json", "version": "2.1.0", "runs": [{"tool": {"driver": {"name": "Checkov", "version": "2.5.0", "informationUri": "https://checkov.io", "rules": [{"id": "CKV_K8S_37", "name": "Minimize the admission of containers with capabilities assigned", "shortDescription": {"text": "Minimize the admission of containers with capabilities assigned"}, "fullDescription": {"text": "Minimize the admission of containers with capabilities assigned"}, "help": {"text": "Minimize the admission of containers with capabilities assigned\nResource: Deployment.default.networking-flow-logs"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-34.html"}, {"id": "CKV_K8S_31", "name": "Ensure that the seccomp profile is set to docker/default or runtime/default", "shortDescription": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "fullDescription": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "help": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default\nResource: Deployment.default.networking-flow-logs"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-29.html"}, {"id": "CKV_K8S_15", "name": "Image Pull Policy should be Always", "shortDescription": {"text": "Image Pull Policy should be Always"}, "fullDescription": {"text": "Image Pull Policy should be Always"}, "help": {"text": "Image Pull Policy should be Always\nResource: Deployment.default.networking-flow-logs"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-14.html"}, {"id": "CKV_K8S_13", "name": "Memory limits should be set", "shortDescription": {"text": "Memory limits should be set"}, "fullDescription": {"text": "Memory limits should be set"}, "help": {"text": "Memory limits should be set\nResource: Deployment.default.networking-flow-logs"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-12.html"}, {"id": "CKV_K8S_40", "name": "Containers should run as a high UID to avoid host conflict", "shortDescription": {"text": "Containers should run as a high UID to avoid host conflict"}, "fullDescription": {"text": "Containers should run as a high UID to avoid host conflict"}, "help": {"text": "Containers should run as a high UID to avoid host conflict\nResource: Deployment.default.networking-flow-logs"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-37.html"}, {"id": "CKV_K8S_22", "name": "Use read-only filesystem for containers where possible", "shortDescription": {"text": "Use read-only filesystem for containers where possible"}, "fullDescription": {"text": "Use read-only filesystem for containers where possible"}, "help": {"text": "Use read-only filesystem for containers where possible\nResource: Deployment.default.networking-flow-logs"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-21.html"}, {"id": "CKV_K8S_35", "name": "Prefer using secrets as files over secrets as environment variables", "shortDescription": {"text": "Prefer using secrets as files over secrets as environment variables"}, "fullDescription": {"text": "Prefer using secrets as files over secrets as environment variables"}, "help": {"text": "Prefer using secrets as files over secrets as environment variables\nResource: Deployment.default.networking-flow-logs"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-33.html"}, {"id": "CKV_K8S_28", "name": "Minimize the admission of containers with the NET_RAW capability", "shortDescription": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "fullDescription": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "help": {"text": "Minimize the admission of containers with the NET_RAW capability\nResource: Deployment.default.networking-flow-logs"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-27.html"}, {"id": "CKV_K8S_14", "name": "Image Tag should be fixed - not latest or blank", "shortDescription": {"text": "Image Tag should be fixed - not latest or blank"}, "fullDescription": {"text": "Image Tag should be fixed - not latest or blank"}, "help": {"text": "Image Tag should be fixed - not latest or blank\nResource: Deployment.default.networking-flow-logs"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-13.html"}, {"id": "CKV_K8S_38", "name": "Ensure that Service Account Tokens are only mounted where necessary", "shortDescription": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "fullDescription": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "help": {"text": "Ensure that Service Account Tokens are only mounted where necessary\nResource: Deployment.default.networking-flow-logs"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-35.html"}, {"id": "CKV_K8S_21", "name": "The default namespace should not be used", "shortDescription": {"text": "The default namespace should not be used"}, "fullDescription": {"text": "The default namespace should not be used"}, "help": {"text": "The default namespace should not be used\nResource: Deployment.default.networking-flow-logs"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-20.html"}, {"id": "CKV_K8S_23", "name": "Minimize the admission of root containers", "shortDescription": {"text": "Minimize the admission of root containers"}, "fullDescription": {"text": "Minimize the admission of root containers"}, "help": {"text": "Minimize the admission of root containers\nResource: Deployment.default.networking-flow-logs"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-22.html"}, {"id": "CKV_K8S_43", "name": "Image should use digest", "shortDescription": {"text": "Image should use digest"}, "fullDescription": {"text": "Image should use digest"}, "help": {"text": "Image should use digest\nResource: Deployment.default.networking-flow-logs"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-39.html"}, {"id": "CKV_K8S_11", "name": "CPU limits should be set", "shortDescription": {"text": "CPU limits should be set"}, "fullDescription": {"text": "CPU limits should be set"}, "help": {"text": "CPU limits should be set\nResource: Deployment.default.networking-flow-logs"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-10.html"}, {"id": "CKV_K8S_8", "name": "Liveness Probe Should be Configured", "shortDescription": {"text": "Liveness Probe Should be Configured"}, "fullDescription": {"text": "Liveness Probe Should be Configured"}, "help": {"text": "Liveness Probe Should be Configured\nResource: Pod.default.networking-flow-logs-test-connection"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-7.html"}, {"id": "CKV_K8S_12", "name": "Memory requests should be set", "shortDescription": {"text": "Memory requests should be set"}, "fullDescription": {"text": "Memory requests should be set"}, "help": {"text": "Memory requests should be set\nResource: Pod.default.networking-flow-logs-test-connection"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-11.html"}, {"id": "CKV_K8S_20", "name": "Containers should not run with allowPrivilegeEscalation", "shortDescription": {"text": "Containers should not run with allowPrivilegeEscalation"}, "fullDescription": {"text": "Containers should not run with allowPrivilegeEscalation"}, "help": {"text": "Containers should not run with allowPrivilegeEscalation\nResource: Pod.default.networking-flow-logs-test-connection"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-19.html"}, {"id": "CKV_K8S_10", "name": "CPU requests should be set", "shortDescription": {"text": "CPU requests should be set"}, "fullDescription": {"text": "CPU requests should be set"}, "help": {"text": "CPU requests should be set\nResource: Pod.default.networking-flow-logs-test-connection"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-9.html"}, {"id": "CKV_K8S_9", "name": "Readiness Probe Should be Configured", "shortDescription": {"text": "Readiness Probe Should be Configured"}, "fullDescription": {"text": "Readiness Probe Should be Configured"}, "help": {"text": "Readiness Probe Should be Configured\nResource: Pod.default.networking-flow-logs-test-connection"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-8.html"}, {"id": "CKV_K8S_29", "name": "Apply security context to your pods and containers", "shortDescription": {"text": "Apply security context to your pods and containers"}, "fullDescription": {"text": "Apply security context to your pods and containers"}, "help": {"text": "Apply security context to your pods and containers\nResource: Pod.default.networking-flow-logs-test-connection"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/ensure-securitycontext-is-applied-to-pods-and-containers.html"}, {"id": "CKV_K8S_30", "name": "Apply security context to your containers", "shortDescription": {"text": "Apply security context to your containers"}, "fullDescription": {"text": "Apply security context to your containers"}, "help": {"text": "Apply security context to your containers\nResource: Pod.default.networking-flow-logs-test-connection"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-28.html"}, {"id": "CKV_K8S_33", "name": "Ensure the Kubernetes dashboard is not deployed", "shortDescription": {"text": "Ensure the Kubernetes dashboard is not deployed"}, "fullDescription": {"text": "Ensure the Kubernetes dashboard is not deployed"}, "help": {"text": "Ensure the Kubernetes dashboard is not deployed\nResource: Job.xxxx.pre-install"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-31.html"}, {"id": "CKV_K8S_158", "name": "Minimize Roles and ClusterRoles that grant permissions to escalate Roles or ClusterRoles", "shortDescription": {"text": "Minimize Roles and ClusterRoles that grant permissions to escalate Roles or ClusterRoles"}, "fullDescription": {"text": "Minimize Roles and ClusterRoles that grant permissions to escalate Roles or ClusterRoles"}, "help": {"text": "Minimize Roles and ClusterRoles that grant permissions to escalate Roles or ClusterRoles\nResource: Role.default.velero-server"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/ensure-roles-and-clusterroles-that-grant-permissions-to-escalate-roles-or-clusterrole-are-minimized.html"}, {"id": "CKV_K8S_157", "name": "Minimize Roles and ClusterRoles that grant permissions to bind RoleBindings or ClusterRoleBindings", "shortDescription": {"text": "Minimize Roles and ClusterRoles that grant permissions to bind RoleBindings or ClusterRoleBindings"}, "fullDescription": {"text": "Minimize Roles and ClusterRoles that grant permissions to bind RoleBindings or ClusterRoleBindings"}, "help": {"text": "Minimize Roles and ClusterRoles that grant permissions to bind RoleBindings or ClusterRoleBindings\nResource: Role.default.velero-server"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/ensure-roles-and-clusterroles-that-grant-permissions-to-bind-rolebindings-or-clusterrolebindings-are-minimized.html"}, {"id": "CKV_K8S_49", "name": "Minimize wildcard use in Roles and ClusterRoles", "shortDescription": {"text": "Minimize wildcard use in Roles and ClusterRoles"}, "fullDescription": {"text": "Minimize wildcard use in Roles and ClusterRoles"}, "help": {"text": "Minimize wildcard use in Roles and ClusterRoles\nResource: Role.default.velero-server"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/ensure-minimized-wildcard-use-in-roles-and-clusterroles.html"}, {"id": "CKV_K8S_155", "name": "Minimize ClusterRoles that grant control over validating or mutating admission webhook configurations", "shortDescription": {"text": "Minimize ClusterRoles that grant control over validating or mutating admission webhook configurations"}, "fullDescription": {"text": "Minimize ClusterRoles that grant control over validating or mutating admission webhook configurations"}, "help": {"text": "Minimize ClusterRoles that grant control over validating or mutating admission webhook configurations\nResource: ClusterRole.default.istio-operator"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/ensure-clusterroles-that-grant-control-over-validating-or-mutating-admission-webhook-configurations-are-minimized.html"}, {"id": "CKV_K8S_6", "name": "Do not admit root containers", "shortDescription": {"text": "Do not admit root containers"}, "fullDescription": {"text": "Do not admit root containers"}, "help": {"text": "Do not admit root containers\nResource: PodSecurityPolicy.default.grafana-test"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-5.html"}, {"id": "CKV_K8S_36", "name": "Minimize the admission of containers with capabilities assigned", "shortDescription": {"text": "Minimize the admission of containers with capabilities assigned"}, "fullDescription": {"text": "Minimize the admission of containers with capabilities assigned"}, "help": {"text": "Minimize the admission of containers with capabilities assigned\nResource: PodSecurityPolicy.default.grafana-test"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/minimize-the-admission-of-containers-with-capabilities-assigned.html"}, {"id": "CKV_K8S_7", "name": "Do not admit containers with the NET_RAW capability", "shortDescription": {"text": "Do not admit containers with the NET_RAW capability"}, "fullDescription": {"text": "Do not admit containers with the NET_RAW capability"}, "help": {"text": "Do not admit containers with the NET_RAW capability\nResource: PodSecurityPolicy.default.grafana-test"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-6.html"}, {"id": "CKV_K8S_32", "name": "Ensure default seccomp profile set to docker/default or runtime/default", "shortDescription": {"text": "Ensure default seccomp profile set to docker/default or runtime/default"}, "fullDescription": {"text": "Ensure default seccomp profile set to docker/default or runtime/default"}, "help": {"text": "Ensure default seccomp profile set to docker/default or runtime/default\nResource: PodSecurityPolicy.default.grafana-test"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-30.html"}, {"id": "CKV_K8S_5", "name": "Containers should not run with allowPrivilegeEscalation", "shortDescription": {"text": "Containers should not run with allowPrivilegeEscalation"}, "fullDescription": {"text": "Containers should not run with allowPrivilegeEscalation"}, "help": {"text": "Containers should not run with allowPrivilegeEscalation\nResource: PodSecurityPolicy.default.grafana-test"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/ensure-containers-do-not-run-with-allowprivilegeescalation.html"}, {"id": "CKV_K8S_19", "name": "Containers should not share the host network namespace", "shortDescription": {"text": "Containers should not share the host network namespace"}, "fullDescription": {"text": "Containers should not share the host network namespace"}, "help": {"text": "Containers should not share the host network namespace\nResource: DaemonSet.default.smarter-device-manager"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-18.html"}, {"id": "CKV_K8S_16", "name": "Container should not be privileged", "shortDescription": {"text": "Container should not be privileged"}, "fullDescription": {"text": "Container should not be privileged"}, "help": {"text": "Container should not be privileged\nResource: Deployment.default.efs-csi-controller"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-15.html"}, {"id": "CKV_K8S_4", "name": "Do not admit containers wishing to share the host network namespace", "shortDescription": {"text": "Do not admit containers wishing to share the host network namespace"}, "fullDescription": {"text": "Do not admit containers wishing to share the host network namespace"}, "help": {"text": "Do not admit containers wishing to share the host network namespace\nResource: PodSecurityPolicy.default.tigera-operator"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-4.html"}, {"id": "CKV_K8S_26", "name": "Do not specify hostPort unless absolutely necessary", "shortDescription": {"text": "Do not specify hostPort unless absolutely necessary"}, "fullDescription": {"text": "Do not specify hostPort unless absolutely necessary"}, "help": {"text": "Do not specify hostPort unless absolutely necessary\nResource: DaemonSet.default.prometheus-node-exporter"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-25.html"}, {"id": "CKV_K8S_17", "name": "Containers should not share the host process ID namespace", "shortDescription": {"text": "Containers should not share the host process ID namespace"}, "fullDescription": {"text": "Containers should not share the host process ID namespace"}, "help": {"text": "Containers should not share the host process ID namespace\nResource: DaemonSet.default.prometheus-node-exporter"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-16.html"}, {"id": "CKV_K8S_156", "name": "Minimize ClusterRoles that grant permissions to approve CertificateSigningRequests", "shortDescription": {"text": "Minimize ClusterRoles that grant permissions to approve CertificateSigningRequests"}, "fullDescription": {"text": "Minimize ClusterRoles that grant permissions to approve CertificateSigningRequests"}, "help": {"text": "Minimize ClusterRoles that grant permissions to approve CertificateSigningRequests\nResource: ClusterRole.default.istiod-istio-system"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/ensure-clusterroles-that-grant-permissions-to-approve-certificatesigningrequests-are-minimized.html"}, {"id": "CKV_K8S_25", "name": "Minimize the admission of containers with added capability", "shortDescription": {"text": "Minimize the admission of containers with added capability"}, "fullDescription": {"text": "Minimize the admission of containers with added capability"}, "help": {"text": "Minimize the admission of containers with added capability\nResource: Deployment.default.ingress-nginx-controller"}, "defaultConfiguration": {"level": "error"}, "helpUri": "https://docs.paloaltonetworks.com/content/techdocs/en_US/prisma/prisma-cloud/prisma-cloud-code-security-policy-reference/kubernetes-policies/kubernetes-policy-index/bc-k8s-24.html"}], "organization": "bridgecrew"}}, "results": [{"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: networking-flow-logs\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: networking-flow-logs\n      app.kubernetes.io/instance: networking-flow-logs\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: networking-flow-logs\n        app.kubernetes.io/instance: networking-flow-logs\n    spec:\n      serviceAccountName: networking-flow-logs\n      securityContext:\n        {}\n      containers:\n        - name: networking-flow-logs\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: networking-flow-logs-configuration\n          volumeMounts:\n          - name: networking-flow-logs-data\n            mountPath: /etc/marker/\n      volumes:\n      - name: networking-flow-logs-data\n        persistentVolumeClaim:\n          claimName: networking-flow-logs\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: networking-flow-logs\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: networking-flow-logs\n      app.kubernetes.io/instance: networking-flow-logs\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: networking-flow-logs\n        app.kubernetes.io/instance: networking-flow-logs\n    spec:\n      serviceAccountName: networking-flow-logs\n      securityContext:\n        {}\n      containers:\n        - name: networking-flow-logs\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: networking-flow-logs-configuration\n          volumeMounts:\n          - name: networking-flow-logs-data\n            mountPath: /etc/marker/\n      volumes:\n      - name: networking-flow-logs-data\n        persistentVolumeClaim:\n          claimName: networking-flow-logs\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: networking-flow-logs\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: networking-flow-logs\n      app.kubernetes.io/instance: networking-flow-logs\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: networking-flow-logs\n        app.kubernetes.io/instance: networking-flow-logs\n    spec:\n      serviceAccountName: networking-flow-logs\n      securityContext:\n        {}\n      containers:\n        - name: networking-flow-logs\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: networking-flow-logs-configuration\n          volumeMounts:\n          - name: networking-flow-logs-data\n            mountPath: /etc/marker/\n      volumes:\n      - name: networking-flow-logs-data\n        persistentVolumeClaim:\n          claimName: networking-flow-logs\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: networking-flow-logs\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: networking-flow-logs\n      app.kubernetes.io/instance: networking-flow-logs\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: networking-flow-logs\n        app.kubernetes.io/instance: networking-flow-logs\n    spec:\n      serviceAccountName: networking-flow-logs\n      securityContext:\n        {}\n      containers:\n        - name: networking-flow-logs\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: networking-flow-logs-configuration\n          volumeMounts:\n          - name: networking-flow-logs-data\n            mountPath: /etc/marker/\n      volumes:\n      - name: networking-flow-logs-data\n        persistentVolumeClaim:\n          claimName: networking-flow-logs\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: networking-flow-logs\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: networking-flow-logs\n      app.kubernetes.io/instance: networking-flow-logs\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: networking-flow-logs\n        app.kubernetes.io/instance: networking-flow-logs\n    spec:\n      serviceAccountName: networking-flow-logs\n      securityContext:\n        {}\n      containers:\n        - name: networking-flow-logs\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: networking-flow-logs-configuration\n          volumeMounts:\n          - name: networking-flow-logs-data\n            mountPath: /etc/marker/\n      volumes:\n      - name: networking-flow-logs-data\n        persistentVolumeClaim:\n          claimName: networking-flow-logs\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: networking-flow-logs\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: networking-flow-logs\n      app.kubernetes.io/instance: networking-flow-logs\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: networking-flow-logs\n        app.kubernetes.io/instance: networking-flow-logs\n    spec:\n      serviceAccountName: networking-flow-logs\n      securityContext:\n        {}\n      containers:\n        - name: networking-flow-logs\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: networking-flow-logs-configuration\n          volumeMounts:\n          - name: networking-flow-logs-data\n            mountPath: /etc/marker/\n      volumes:\n      - name: networking-flow-logs-data\n        persistentVolumeClaim:\n          claimName: networking-flow-logs\n"}}}}]}, {"ruleId": "CKV_K8S_35", "ruleIndex": 6, "level": "error", "attachments": [], "message": {"text": "Prefer using secrets as files over secrets as environment variables"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: networking-flow-logs\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: networking-flow-logs\n      app.kubernetes.io/instance: networking-flow-logs\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: networking-flow-logs\n        app.kubernetes.io/instance: networking-flow-logs\n    spec:\n      serviceAccountName: networking-flow-logs\n      securityContext:\n        {}\n      containers:\n        - name: networking-flow-logs\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: networking-flow-logs-configuration\n          volumeMounts:\n          - name: networking-flow-logs-data\n            mountPath: /etc/marker/\n      volumes:\n      - name: networking-flow-logs-data\n        persistentVolumeClaim:\n          claimName: networking-flow-logs\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: networking-flow-logs\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: networking-flow-logs\n      app.kubernetes.io/instance: networking-flow-logs\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: networking-flow-logs\n        app.kubernetes.io/instance: networking-flow-logs\n    spec:\n      serviceAccountName: networking-flow-logs\n      securityContext:\n        {}\n      containers:\n        - name: networking-flow-logs\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: networking-flow-logs-configuration\n          volumeMounts:\n          - name: networking-flow-logs-data\n            mountPath: /etc/marker/\n      volumes:\n      - name: networking-flow-logs-data\n        persistentVolumeClaim:\n          claimName: networking-flow-logs\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: networking-flow-logs\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: networking-flow-logs\n      app.kubernetes.io/instance: networking-flow-logs\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: networking-flow-logs\n        app.kubernetes.io/instance: networking-flow-logs\n    spec:\n      serviceAccountName: networking-flow-logs\n      securityContext:\n        {}\n      containers:\n        - name: networking-flow-logs\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: networking-flow-logs-configuration\n          volumeMounts:\n          - name: networking-flow-logs-data\n            mountPath: /etc/marker/\n      volumes:\n      - name: networking-flow-logs-data\n        persistentVolumeClaim:\n          claimName: networking-flow-logs\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: networking-flow-logs\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: networking-flow-logs\n      app.kubernetes.io/instance: networking-flow-logs\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: networking-flow-logs\n        app.kubernetes.io/instance: networking-flow-logs\n    spec:\n      serviceAccountName: networking-flow-logs\n      securityContext:\n        {}\n      containers:\n        - name: networking-flow-logs\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: networking-flow-logs-configuration\n          volumeMounts:\n          - name: networking-flow-logs-data\n            mountPath: /etc/marker/\n      volumes:\n      - name: networking-flow-logs-data\n        persistentVolumeClaim:\n          claimName: networking-flow-logs\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: networking-flow-logs\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: networking-flow-logs\n      app.kubernetes.io/instance: networking-flow-logs\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: networking-flow-logs\n        app.kubernetes.io/instance: networking-flow-logs\n    spec:\n      serviceAccountName: networking-flow-logs\n      securityContext:\n        {}\n      containers:\n        - name: networking-flow-logs\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: networking-flow-logs-configuration\n          volumeMounts:\n          - name: networking-flow-logs-data\n            mountPath: /etc/marker/\n      volumes:\n      - name: networking-flow-logs-data\n        persistentVolumeClaim:\n          claimName: networking-flow-logs\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: networking-flow-logs\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: networking-flow-logs\n      app.kubernetes.io/instance: networking-flow-logs\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: networking-flow-logs\n        app.kubernetes.io/instance: networking-flow-logs\n    spec:\n      serviceAccountName: networking-flow-logs\n      securityContext:\n        {}\n      containers:\n        - name: networking-flow-logs\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: networking-flow-logs-configuration\n          volumeMounts:\n          - name: networking-flow-logs-data\n            mountPath: /etc/marker/\n      volumes:\n      - name: networking-flow-logs-data\n        persistentVolumeClaim:\n          claimName: networking-flow-logs\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: networking-flow-logs\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: networking-flow-logs\n      app.kubernetes.io/instance: networking-flow-logs\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: networking-flow-logs\n        app.kubernetes.io/instance: networking-flow-logs\n    spec:\n      serviceAccountName: networking-flow-logs\n      securityContext:\n        {}\n      containers:\n        - name: networking-flow-logs\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: networking-flow-logs-configuration\n          volumeMounts:\n          - name: networking-flow-logs-data\n            mountPath: /etc/marker/\n      volumes:\n      - name: networking-flow-logs-data\n        persistentVolumeClaim:\n          claimName: networking-flow-logs\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: networking-flow-logs\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: networking-flow-logs\n      app.kubernetes.io/instance: networking-flow-logs\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: networking-flow-logs\n        app.kubernetes.io/instance: networking-flow-logs\n    spec:\n      serviceAccountName: networking-flow-logs\n      securityContext:\n        {}\n      containers:\n        - name: networking-flow-logs\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: networking-flow-logs-configuration\n          volumeMounts:\n          - name: networking-flow-logs-data\n            mountPath: /etc/marker/\n      volumes:\n      - name: networking-flow-logs-data\n        persistentVolumeClaim:\n          claimName: networking-flow-logs\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/service.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: networking-flow-logs\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: http\n      protocol: TCP\n      name: http\n  selector:\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 12, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: networking-flow-logs\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/aws-auth.yaml"}, "region": {"startLine": 3, "endLine": 9, "snippet": {"text": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-auth\ndata:\n  accessKeyID: \n  secretAccessKey:\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/networking-flow-logs-configuration.yaml"}, "region": {"startLine": 3, "endLine": 9, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: networking-flow-logs-configuration\ndata:\n  BUCKET: \"\"\n  CLOUD: \"\"\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"networking-flow-logs-test-connection\"\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['networking-flow-logs:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"networking-flow-logs-test-connection\"\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['networking-flow-logs:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"networking-flow-logs-test-connection\"\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['networking-flow-logs:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"networking-flow-logs-test-connection\"\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['networking-flow-logs:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"networking-flow-logs-test-connection\"\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['networking-flow-logs:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"networking-flow-logs-test-connection\"\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['networking-flow-logs:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"networking-flow-logs-test-connection\"\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['networking-flow-logs:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"networking-flow-logs-test-connection\"\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['networking-flow-logs:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"networking-flow-logs-test-connection\"\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['networking-flow-logs:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"networking-flow-logs-test-connection\"\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['networking-flow-logs:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"networking-flow-logs-test-connection\"\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['networking-flow-logs:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"networking-flow-logs-test-connection\"\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['networking-flow-logs:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"networking-flow-logs-test-connection\"\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['networking-flow-logs:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"networking-flow-logs-test-connection\"\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['networking-flow-logs:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"networking-flow-logs-test-connection\"\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['networking-flow-logs:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"networking-flow-logs-test-connection\"\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['networking-flow-logs:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"networking-flow-logs-test-connection\"\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['networking-flow-logs:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"networking-flow-logs-test-connection\"\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['networking-flow-logs:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/networking-flow-logs/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"networking-flow-logs-test-connection\"\n  labels:\n    helm.sh/chart: networking-flow-logs-0.1.0\n    app.kubernetes.io/name: networking-flow-logs\n    app.kubernetes.io/instance: networking-flow-logs\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['networking-flow-logs:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signer\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: logs-signer\n      app.kubernetes.io/instance: logs-signer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: logs-signer\n        app.kubernetes.io/instance: logs-signer\n    spec:\n      serviceAccountName: logs-signer\n      securityContext:\n        {}\n      containers:\n        - name: logs-signer\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: logs-signer-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signer\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signer\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: logs-signer\n      app.kubernetes.io/instance: logs-signer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: logs-signer\n        app.kubernetes.io/instance: logs-signer\n    spec:\n      serviceAccountName: logs-signer\n      securityContext:\n        {}\n      containers:\n        - name: logs-signer\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: logs-signer-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signer\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signer\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: logs-signer\n      app.kubernetes.io/instance: logs-signer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: logs-signer\n        app.kubernetes.io/instance: logs-signer\n    spec:\n      serviceAccountName: logs-signer\n      securityContext:\n        {}\n      containers:\n        - name: logs-signer\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: logs-signer-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signer\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signer\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: logs-signer\n      app.kubernetes.io/instance: logs-signer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: logs-signer\n        app.kubernetes.io/instance: logs-signer\n    spec:\n      serviceAccountName: logs-signer\n      securityContext:\n        {}\n      containers:\n        - name: logs-signer\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: logs-signer-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signer\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signer\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: logs-signer\n      app.kubernetes.io/instance: logs-signer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: logs-signer\n        app.kubernetes.io/instance: logs-signer\n    spec:\n      serviceAccountName: logs-signer\n      securityContext:\n        {}\n      containers:\n        - name: logs-signer\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: logs-signer-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signer\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signer\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: logs-signer\n      app.kubernetes.io/instance: logs-signer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: logs-signer\n        app.kubernetes.io/instance: logs-signer\n    spec:\n      serviceAccountName: logs-signer\n      securityContext:\n        {}\n      containers:\n        - name: logs-signer\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: logs-signer-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signer\n"}}}}]}, {"ruleId": "CKV_K8S_35", "ruleIndex": 6, "level": "error", "attachments": [], "message": {"text": "Prefer using secrets as files over secrets as environment variables"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signer\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: logs-signer\n      app.kubernetes.io/instance: logs-signer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: logs-signer\n        app.kubernetes.io/instance: logs-signer\n    spec:\n      serviceAccountName: logs-signer\n      securityContext:\n        {}\n      containers:\n        - name: logs-signer\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: logs-signer-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signer\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signer\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: logs-signer\n      app.kubernetes.io/instance: logs-signer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: logs-signer\n        app.kubernetes.io/instance: logs-signer\n    spec:\n      serviceAccountName: logs-signer\n      securityContext:\n        {}\n      containers:\n        - name: logs-signer\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: logs-signer-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signer\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signer\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: logs-signer\n      app.kubernetes.io/instance: logs-signer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: logs-signer\n        app.kubernetes.io/instance: logs-signer\n    spec:\n      serviceAccountName: logs-signer\n      securityContext:\n        {}\n      containers:\n        - name: logs-signer\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: logs-signer-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signer\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signer\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: logs-signer\n      app.kubernetes.io/instance: logs-signer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: logs-signer\n        app.kubernetes.io/instance: logs-signer\n    spec:\n      serviceAccountName: logs-signer\n      securityContext:\n        {}\n      containers:\n        - name: logs-signer\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: logs-signer-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signer\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signer\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: logs-signer\n      app.kubernetes.io/instance: logs-signer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: logs-signer\n        app.kubernetes.io/instance: logs-signer\n    spec:\n      serviceAccountName: logs-signer\n      securityContext:\n        {}\n      containers:\n        - name: logs-signer\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: logs-signer-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signer\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signer\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: logs-signer\n      app.kubernetes.io/instance: logs-signer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: logs-signer\n        app.kubernetes.io/instance: logs-signer\n    spec:\n      serviceAccountName: logs-signer\n      securityContext:\n        {}\n      containers:\n        - name: logs-signer\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: logs-signer-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signer\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signer\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: logs-signer\n      app.kubernetes.io/instance: logs-signer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: logs-signer\n        app.kubernetes.io/instance: logs-signer\n    spec:\n      serviceAccountName: logs-signer\n      securityContext:\n        {}\n      containers:\n        - name: logs-signer\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: logs-signer-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signer\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signer\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: logs-signer\n      app.kubernetes.io/instance: logs-signer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: logs-signer\n        app.kubernetes.io/instance: logs-signer\n    spec:\n      serviceAccountName: logs-signer\n      securityContext:\n        {}\n      containers:\n        - name: logs-signer\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: \n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          readinessProbe:\n            exec:\n              command:\n              - cat\n              - /tmp/healthy\n          resources:\n            {}\n          env:\n          - name: \"AWS_ACCESS_KEY_ID\"\n            valueFrom:\n              secretKeyRef:\n                key: accessKeyID\n                name: aws-auth\n          - name: \"AWS_SECRET_ACCESS_KEY\"\n            valueFrom:\n              secretKeyRef:\n                key: secretAccessKey\n                name: aws-auth\n          envFrom:\n          - configMapRef:\n              name: logs-signer-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signer\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 12, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: logs-signer\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/aws-auth.yaml"}, "region": {"startLine": 3, "endLine": 9, "snippet": {"text": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-auth\ndata:\n  accessKeyID: \n  secretAccessKey:\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/logs-signer-configuration.yaml"}, "region": {"startLine": 3, "endLine": 16, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: logs-signer-configuration\ndata:\n  SIGNVERFIPS_ID: \"\"\n  WRSTUDIO_INSTANCE: \n  BUFFER_SIZE: \"\"\n  SIGNVERFIPS_SIGNATURE_ENDPOINT: \n  SIGNVERFIPS_VERIFYSIG_ENDPOINT: \n  SIGNED_DIGEST_KEY: \"\"\n  VALIDATION_KEY: \n  WRSC_ENDPOINT: \n  S3_BUCKET_NAME:\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signer-test-connection\"\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signer:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signer-test-connection\"\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signer:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signer-test-connection\"\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signer:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signer-test-connection\"\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signer:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signer-test-connection\"\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signer:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signer-test-connection\"\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signer:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signer-test-connection\"\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signer:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signer-test-connection\"\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signer:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signer-test-connection\"\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signer:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signer-test-connection\"\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signer:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signer-test-connection\"\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signer:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signer-test-connection\"\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signer:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signer-test-connection\"\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signer:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signer-test-connection\"\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signer:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signer-test-connection\"\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signer:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signer-test-connection\"\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signer:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signer-test-connection\"\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signer:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signer-test-connection\"\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signer:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signer/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signer-test-connection\"\n  labels:\n    helm.sh/chart: logs-signer-0.1.0\n    app.kubernetes.io/name: logs-signer\n    app.kubernetes.io/instance: logs-signer\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signer:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: kube-state-metrics\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: kube-state-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: kube-state-metrics\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: kube-state-metrics\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: kube-state-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: kube-state-metrics\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: kube-state-metrics\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: kube-state-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: kube-state-metrics\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: kube-state-metrics\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: kube-state-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: kube-state-metrics\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: kube-state-metrics\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: kube-state-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: kube-state-metrics\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: kube-state-metrics\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: kube-state-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: kube-state-metrics\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: kube-state-metrics\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: kube-state-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: kube-state-metrics\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: kube-state-metrics\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: kube-state-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: kube-state-metrics\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: kube-state-metrics\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: kube-state-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: kube-state-metrics\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: kube-state-metrics\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: kube-state-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: kube-state-metrics\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: kube-state-metrics\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: kube-state-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: kube-state-metrics\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: kube-state-metrics\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: kube-state-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: kube-state-metrics\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: kube-state-metrics\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: kube-state-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: kube-state-metrics\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: kube-state-metrics\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: kube-state-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: kube-state-metrics\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kube-state-metrics/templates/service.yaml"}, "region": {"startLine": 3, "endLine": 28, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: kube-state-metrics\n    app.kubernetes.io/version: \"2.2.4\"\n  annotations:\n    prometheus.io/scrape: 'true'\nspec:\n  type: \"ClusterIP\"\n  ports:\n  - name: \"http\"\n    protocol: TCP\n    port: 8080\n    targetPort: 8080\n  \n  selector:    \n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: kube-state-metrics\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kube-state-metrics/templates/serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 17, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: kube-state-metrics\n    app.kubernetes.io/version: \"2.2.4\"\n  name: kube-state-metrics\n  namespace: default\nimagePullSecrets:\n  []\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/postgresql/templates/svc.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.9.4\n    release: \"postgresql\"\n    heritage: \"Helm\"\n  annotations:\nspec:\n  type: ClusterIP\n  ports:\n    - name: tcp-postgresql\n      port: 5432\n      targetPort: tcp-postgresql\n  selector:\n    app: postgresql\n    release: \"postgresql\"\n    role: master\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/postgresql/templates/secrets.yaml"}, "region": {"startLine": 3, "endLine": 14, "snippet": {"text": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.9.4\n    release: \"postgresql\"\n    heritage: \"Helm\"\ntype: Opaque\ndata:\n  postgresql-password: \"VUZOR0wwSkd6Rw==\"\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/postgresql/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 112, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.9.4\n    release: \"postgresql\"\n    heritage: \"Helm\"\n  annotations:\nspec:\n  serviceName: postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: \"postgresql\"\n      role: master\n  template:\n    metadata:\n      name: postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.9.4\n        release: \"postgresql\"\n        heritage: \"Helm\"\n        role: master\n    spec:      \n      securityContext:\n        fsGroup: 1001\n      containers:\n        - name: postgresql\n          image: docker.io/bitnami/postgresql:11.7.0-debian-10-r90\n          imagePullPolicy: \"IfNotPresent\"\n          resources:\n            requests:\n              cpu: 250m\n              memory: 256Mi\n          command: [\"/opt/bitnami/scripts/postgresql/entrypoint.sh\"]\n          args: [\"/opt/bitnami/scripts/postgresql/run.sh\"]\n          securityContext:\n            runAsUser: 1001\n          env:\n            - name: BITNAMI_DEBUG\n              value: \"false\"\n            - name: POSTGRESQL_PORT_NUMBER\n              value: \"5432\"\n            - name: POSTGRESQL_VOLUME_DIR\n              value: \"/bitnami/postgresql\"\n            - name: PGDATA\n              value: \"/bitnami/postgresql/data\"\n            - name: POSTGRES_USER\n              value: \"postgres\"\n            - name: POSTGRES_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: postgresql\n                  key: postgresql-password\n            - name: POSTGRESQL_ENABLE_LDAP\n              value: \"no\"\n          ports:\n            - name: tcp-postgresql\n              containerPort: 5432\n          livenessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          readinessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - -e\n                - |\n                  exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          volumeMounts:\n            - name: dshm\n              mountPath: /dev/shm\n            - name: data\n              mountPath: /bitnami/postgresql\n              subPath: \n      volumes:\n        - name: dshm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 1Gi\n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/postgresql/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 112, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.9.4\n    release: \"postgresql\"\n    heritage: \"Helm\"\n  annotations:\nspec:\n  serviceName: postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: \"postgresql\"\n      role: master\n  template:\n    metadata:\n      name: postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.9.4\n        release: \"postgresql\"\n        heritage: \"Helm\"\n        role: master\n    spec:      \n      securityContext:\n        fsGroup: 1001\n      containers:\n        - name: postgresql\n          image: docker.io/bitnami/postgresql:11.7.0-debian-10-r90\n          imagePullPolicy: \"IfNotPresent\"\n          resources:\n            requests:\n              cpu: 250m\n              memory: 256Mi\n          command: [\"/opt/bitnami/scripts/postgresql/entrypoint.sh\"]\n          args: [\"/opt/bitnami/scripts/postgresql/run.sh\"]\n          securityContext:\n            runAsUser: 1001\n          env:\n            - name: BITNAMI_DEBUG\n              value: \"false\"\n            - name: POSTGRESQL_PORT_NUMBER\n              value: \"5432\"\n            - name: POSTGRESQL_VOLUME_DIR\n              value: \"/bitnami/postgresql\"\n            - name: PGDATA\n              value: \"/bitnami/postgresql/data\"\n            - name: POSTGRES_USER\n              value: \"postgres\"\n            - name: POSTGRES_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: postgresql\n                  key: postgresql-password\n            - name: POSTGRESQL_ENABLE_LDAP\n              value: \"no\"\n          ports:\n            - name: tcp-postgresql\n              containerPort: 5432\n          livenessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          readinessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - -e\n                - |\n                  exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          volumeMounts:\n            - name: dshm\n              mountPath: /dev/shm\n            - name: data\n              mountPath: /bitnami/postgresql\n              subPath: \n      volumes:\n        - name: dshm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 1Gi\n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/postgresql/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 112, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.9.4\n    release: \"postgresql\"\n    heritage: \"Helm\"\n  annotations:\nspec:\n  serviceName: postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: \"postgresql\"\n      role: master\n  template:\n    metadata:\n      name: postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.9.4\n        release: \"postgresql\"\n        heritage: \"Helm\"\n        role: master\n    spec:      \n      securityContext:\n        fsGroup: 1001\n      containers:\n        - name: postgresql\n          image: docker.io/bitnami/postgresql:11.7.0-debian-10-r90\n          imagePullPolicy: \"IfNotPresent\"\n          resources:\n            requests:\n              cpu: 250m\n              memory: 256Mi\n          command: [\"/opt/bitnami/scripts/postgresql/entrypoint.sh\"]\n          args: [\"/opt/bitnami/scripts/postgresql/run.sh\"]\n          securityContext:\n            runAsUser: 1001\n          env:\n            - name: BITNAMI_DEBUG\n              value: \"false\"\n            - name: POSTGRESQL_PORT_NUMBER\n              value: \"5432\"\n            - name: POSTGRESQL_VOLUME_DIR\n              value: \"/bitnami/postgresql\"\n            - name: PGDATA\n              value: \"/bitnami/postgresql/data\"\n            - name: POSTGRES_USER\n              value: \"postgres\"\n            - name: POSTGRES_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: postgresql\n                  key: postgresql-password\n            - name: POSTGRESQL_ENABLE_LDAP\n              value: \"no\"\n          ports:\n            - name: tcp-postgresql\n              containerPort: 5432\n          livenessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          readinessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - -e\n                - |\n                  exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          volumeMounts:\n            - name: dshm\n              mountPath: /dev/shm\n            - name: data\n              mountPath: /bitnami/postgresql\n              subPath: \n      volumes:\n        - name: dshm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 1Gi\n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/postgresql/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 112, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.9.4\n    release: \"postgresql\"\n    heritage: \"Helm\"\n  annotations:\nspec:\n  serviceName: postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: \"postgresql\"\n      role: master\n  template:\n    metadata:\n      name: postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.9.4\n        release: \"postgresql\"\n        heritage: \"Helm\"\n        role: master\n    spec:      \n      securityContext:\n        fsGroup: 1001\n      containers:\n        - name: postgresql\n          image: docker.io/bitnami/postgresql:11.7.0-debian-10-r90\n          imagePullPolicy: \"IfNotPresent\"\n          resources:\n            requests:\n              cpu: 250m\n              memory: 256Mi\n          command: [\"/opt/bitnami/scripts/postgresql/entrypoint.sh\"]\n          args: [\"/opt/bitnami/scripts/postgresql/run.sh\"]\n          securityContext:\n            runAsUser: 1001\n          env:\n            - name: BITNAMI_DEBUG\n              value: \"false\"\n            - name: POSTGRESQL_PORT_NUMBER\n              value: \"5432\"\n            - name: POSTGRESQL_VOLUME_DIR\n              value: \"/bitnami/postgresql\"\n            - name: PGDATA\n              value: \"/bitnami/postgresql/data\"\n            - name: POSTGRES_USER\n              value: \"postgres\"\n            - name: POSTGRES_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: postgresql\n                  key: postgresql-password\n            - name: POSTGRESQL_ENABLE_LDAP\n              value: \"no\"\n          ports:\n            - name: tcp-postgresql\n              containerPort: 5432\n          livenessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          readinessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - -e\n                - |\n                  exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          volumeMounts:\n            - name: dshm\n              mountPath: /dev/shm\n            - name: data\n              mountPath: /bitnami/postgresql\n              subPath: \n      volumes:\n        - name: dshm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 1Gi\n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/postgresql/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 112, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.9.4\n    release: \"postgresql\"\n    heritage: \"Helm\"\n  annotations:\nspec:\n  serviceName: postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: \"postgresql\"\n      role: master\n  template:\n    metadata:\n      name: postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.9.4\n        release: \"postgresql\"\n        heritage: \"Helm\"\n        role: master\n    spec:      \n      securityContext:\n        fsGroup: 1001\n      containers:\n        - name: postgresql\n          image: docker.io/bitnami/postgresql:11.7.0-debian-10-r90\n          imagePullPolicy: \"IfNotPresent\"\n          resources:\n            requests:\n              cpu: 250m\n              memory: 256Mi\n          command: [\"/opt/bitnami/scripts/postgresql/entrypoint.sh\"]\n          args: [\"/opt/bitnami/scripts/postgresql/run.sh\"]\n          securityContext:\n            runAsUser: 1001\n          env:\n            - name: BITNAMI_DEBUG\n              value: \"false\"\n            - name: POSTGRESQL_PORT_NUMBER\n              value: \"5432\"\n            - name: POSTGRESQL_VOLUME_DIR\n              value: \"/bitnami/postgresql\"\n            - name: PGDATA\n              value: \"/bitnami/postgresql/data\"\n            - name: POSTGRES_USER\n              value: \"postgres\"\n            - name: POSTGRES_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: postgresql\n                  key: postgresql-password\n            - name: POSTGRESQL_ENABLE_LDAP\n              value: \"no\"\n          ports:\n            - name: tcp-postgresql\n              containerPort: 5432\n          livenessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          readinessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - -e\n                - |\n                  exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          volumeMounts:\n            - name: dshm\n              mountPath: /dev/shm\n            - name: data\n              mountPath: /bitnami/postgresql\n              subPath: \n      volumes:\n        - name: dshm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 1Gi\n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/postgresql/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 112, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.9.4\n    release: \"postgresql\"\n    heritage: \"Helm\"\n  annotations:\nspec:\n  serviceName: postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: \"postgresql\"\n      role: master\n  template:\n    metadata:\n      name: postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.9.4\n        release: \"postgresql\"\n        heritage: \"Helm\"\n        role: master\n    spec:      \n      securityContext:\n        fsGroup: 1001\n      containers:\n        - name: postgresql\n          image: docker.io/bitnami/postgresql:11.7.0-debian-10-r90\n          imagePullPolicy: \"IfNotPresent\"\n          resources:\n            requests:\n              cpu: 250m\n              memory: 256Mi\n          command: [\"/opt/bitnami/scripts/postgresql/entrypoint.sh\"]\n          args: [\"/opt/bitnami/scripts/postgresql/run.sh\"]\n          securityContext:\n            runAsUser: 1001\n          env:\n            - name: BITNAMI_DEBUG\n              value: \"false\"\n            - name: POSTGRESQL_PORT_NUMBER\n              value: \"5432\"\n            - name: POSTGRESQL_VOLUME_DIR\n              value: \"/bitnami/postgresql\"\n            - name: PGDATA\n              value: \"/bitnami/postgresql/data\"\n            - name: POSTGRES_USER\n              value: \"postgres\"\n            - name: POSTGRES_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: postgresql\n                  key: postgresql-password\n            - name: POSTGRESQL_ENABLE_LDAP\n              value: \"no\"\n          ports:\n            - name: tcp-postgresql\n              containerPort: 5432\n          livenessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          readinessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - -e\n                - |\n                  exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          volumeMounts:\n            - name: dshm\n              mountPath: /dev/shm\n            - name: data\n              mountPath: /bitnami/postgresql\n              subPath: \n      volumes:\n        - name: dshm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 1Gi\n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/postgresql/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 112, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.9.4\n    release: \"postgresql\"\n    heritage: \"Helm\"\n  annotations:\nspec:\n  serviceName: postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: \"postgresql\"\n      role: master\n  template:\n    metadata:\n      name: postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.9.4\n        release: \"postgresql\"\n        heritage: \"Helm\"\n        role: master\n    spec:      \n      securityContext:\n        fsGroup: 1001\n      containers:\n        - name: postgresql\n          image: docker.io/bitnami/postgresql:11.7.0-debian-10-r90\n          imagePullPolicy: \"IfNotPresent\"\n          resources:\n            requests:\n              cpu: 250m\n              memory: 256Mi\n          command: [\"/opt/bitnami/scripts/postgresql/entrypoint.sh\"]\n          args: [\"/opt/bitnami/scripts/postgresql/run.sh\"]\n          securityContext:\n            runAsUser: 1001\n          env:\n            - name: BITNAMI_DEBUG\n              value: \"false\"\n            - name: POSTGRESQL_PORT_NUMBER\n              value: \"5432\"\n            - name: POSTGRESQL_VOLUME_DIR\n              value: \"/bitnami/postgresql\"\n            - name: PGDATA\n              value: \"/bitnami/postgresql/data\"\n            - name: POSTGRES_USER\n              value: \"postgres\"\n            - name: POSTGRES_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: postgresql\n                  key: postgresql-password\n            - name: POSTGRESQL_ENABLE_LDAP\n              value: \"no\"\n          ports:\n            - name: tcp-postgresql\n              containerPort: 5432\n          livenessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          readinessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - -e\n                - |\n                  exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          volumeMounts:\n            - name: dshm\n              mountPath: /dev/shm\n            - name: data\n              mountPath: /bitnami/postgresql\n              subPath: \n      volumes:\n        - name: dshm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 1Gi\n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_35", "ruleIndex": 6, "level": "error", "attachments": [], "message": {"text": "Prefer using secrets as files over secrets as environment variables"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/postgresql/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 112, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.9.4\n    release: \"postgresql\"\n    heritage: \"Helm\"\n  annotations:\nspec:\n  serviceName: postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: \"postgresql\"\n      role: master\n  template:\n    metadata:\n      name: postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.9.4\n        release: \"postgresql\"\n        heritage: \"Helm\"\n        role: master\n    spec:      \n      securityContext:\n        fsGroup: 1001\n      containers:\n        - name: postgresql\n          image: docker.io/bitnami/postgresql:11.7.0-debian-10-r90\n          imagePullPolicy: \"IfNotPresent\"\n          resources:\n            requests:\n              cpu: 250m\n              memory: 256Mi\n          command: [\"/opt/bitnami/scripts/postgresql/entrypoint.sh\"]\n          args: [\"/opt/bitnami/scripts/postgresql/run.sh\"]\n          securityContext:\n            runAsUser: 1001\n          env:\n            - name: BITNAMI_DEBUG\n              value: \"false\"\n            - name: POSTGRESQL_PORT_NUMBER\n              value: \"5432\"\n            - name: POSTGRESQL_VOLUME_DIR\n              value: \"/bitnami/postgresql\"\n            - name: PGDATA\n              value: \"/bitnami/postgresql/data\"\n            - name: POSTGRES_USER\n              value: \"postgres\"\n            - name: POSTGRES_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: postgresql\n                  key: postgresql-password\n            - name: POSTGRESQL_ENABLE_LDAP\n              value: \"no\"\n          ports:\n            - name: tcp-postgresql\n              containerPort: 5432\n          livenessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          readinessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - -e\n                - |\n                  exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          volumeMounts:\n            - name: dshm\n              mountPath: /dev/shm\n            - name: data\n              mountPath: /bitnami/postgresql\n              subPath: \n      volumes:\n        - name: dshm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 1Gi\n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/postgresql/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 112, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.9.4\n    release: \"postgresql\"\n    heritage: \"Helm\"\n  annotations:\nspec:\n  serviceName: postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: \"postgresql\"\n      role: master\n  template:\n    metadata:\n      name: postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.9.4\n        release: \"postgresql\"\n        heritage: \"Helm\"\n        role: master\n    spec:      \n      securityContext:\n        fsGroup: 1001\n      containers:\n        - name: postgresql\n          image: docker.io/bitnami/postgresql:11.7.0-debian-10-r90\n          imagePullPolicy: \"IfNotPresent\"\n          resources:\n            requests:\n              cpu: 250m\n              memory: 256Mi\n          command: [\"/opt/bitnami/scripts/postgresql/entrypoint.sh\"]\n          args: [\"/opt/bitnami/scripts/postgresql/run.sh\"]\n          securityContext:\n            runAsUser: 1001\n          env:\n            - name: BITNAMI_DEBUG\n              value: \"false\"\n            - name: POSTGRESQL_PORT_NUMBER\n              value: \"5432\"\n            - name: POSTGRESQL_VOLUME_DIR\n              value: \"/bitnami/postgresql\"\n            - name: PGDATA\n              value: \"/bitnami/postgresql/data\"\n            - name: POSTGRES_USER\n              value: \"postgres\"\n            - name: POSTGRES_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: postgresql\n                  key: postgresql-password\n            - name: POSTGRESQL_ENABLE_LDAP\n              value: \"no\"\n          ports:\n            - name: tcp-postgresql\n              containerPort: 5432\n          livenessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          readinessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - -e\n                - |\n                  exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          volumeMounts:\n            - name: dshm\n              mountPath: /dev/shm\n            - name: data\n              mountPath: /bitnami/postgresql\n              subPath: \n      volumes:\n        - name: dshm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 1Gi\n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/postgresql/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 112, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.9.4\n    release: \"postgresql\"\n    heritage: \"Helm\"\n  annotations:\nspec:\n  serviceName: postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: \"postgresql\"\n      role: master\n  template:\n    metadata:\n      name: postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.9.4\n        release: \"postgresql\"\n        heritage: \"Helm\"\n        role: master\n    spec:      \n      securityContext:\n        fsGroup: 1001\n      containers:\n        - name: postgresql\n          image: docker.io/bitnami/postgresql:11.7.0-debian-10-r90\n          imagePullPolicy: \"IfNotPresent\"\n          resources:\n            requests:\n              cpu: 250m\n              memory: 256Mi\n          command: [\"/opt/bitnami/scripts/postgresql/entrypoint.sh\"]\n          args: [\"/opt/bitnami/scripts/postgresql/run.sh\"]\n          securityContext:\n            runAsUser: 1001\n          env:\n            - name: BITNAMI_DEBUG\n              value: \"false\"\n            - name: POSTGRESQL_PORT_NUMBER\n              value: \"5432\"\n            - name: POSTGRESQL_VOLUME_DIR\n              value: \"/bitnami/postgresql\"\n            - name: PGDATA\n              value: \"/bitnami/postgresql/data\"\n            - name: POSTGRES_USER\n              value: \"postgres\"\n            - name: POSTGRES_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: postgresql\n                  key: postgresql-password\n            - name: POSTGRESQL_ENABLE_LDAP\n              value: \"no\"\n          ports:\n            - name: tcp-postgresql\n              containerPort: 5432\n          livenessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          readinessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - -e\n                - |\n                  exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          volumeMounts:\n            - name: dshm\n              mountPath: /dev/shm\n            - name: data\n              mountPath: /bitnami/postgresql\n              subPath: \n      volumes:\n        - name: dshm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 1Gi\n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/postgresql/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 112, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.9.4\n    release: \"postgresql\"\n    heritage: \"Helm\"\n  annotations:\nspec:\n  serviceName: postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: \"postgresql\"\n      role: master\n  template:\n    metadata:\n      name: postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.9.4\n        release: \"postgresql\"\n        heritage: \"Helm\"\n        role: master\n    spec:      \n      securityContext:\n        fsGroup: 1001\n      containers:\n        - name: postgresql\n          image: docker.io/bitnami/postgresql:11.7.0-debian-10-r90\n          imagePullPolicy: \"IfNotPresent\"\n          resources:\n            requests:\n              cpu: 250m\n              memory: 256Mi\n          command: [\"/opt/bitnami/scripts/postgresql/entrypoint.sh\"]\n          args: [\"/opt/bitnami/scripts/postgresql/run.sh\"]\n          securityContext:\n            runAsUser: 1001\n          env:\n            - name: BITNAMI_DEBUG\n              value: \"false\"\n            - name: POSTGRESQL_PORT_NUMBER\n              value: \"5432\"\n            - name: POSTGRESQL_VOLUME_DIR\n              value: \"/bitnami/postgresql\"\n            - name: PGDATA\n              value: \"/bitnami/postgresql/data\"\n            - name: POSTGRES_USER\n              value: \"postgres\"\n            - name: POSTGRES_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: postgresql\n                  key: postgresql-password\n            - name: POSTGRESQL_ENABLE_LDAP\n              value: \"no\"\n          ports:\n            - name: tcp-postgresql\n              containerPort: 5432\n          livenessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          readinessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - -e\n                - |\n                  exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          volumeMounts:\n            - name: dshm\n              mountPath: /dev/shm\n            - name: data\n              mountPath: /bitnami/postgresql\n              subPath: \n      volumes:\n        - name: dshm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 1Gi\n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/postgresql/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 112, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.9.4\n    release: \"postgresql\"\n    heritage: \"Helm\"\n  annotations:\nspec:\n  serviceName: postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: \"postgresql\"\n      role: master\n  template:\n    metadata:\n      name: postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.9.4\n        release: \"postgresql\"\n        heritage: \"Helm\"\n        role: master\n    spec:      \n      securityContext:\n        fsGroup: 1001\n      containers:\n        - name: postgresql\n          image: docker.io/bitnami/postgresql:11.7.0-debian-10-r90\n          imagePullPolicy: \"IfNotPresent\"\n          resources:\n            requests:\n              cpu: 250m\n              memory: 256Mi\n          command: [\"/opt/bitnami/scripts/postgresql/entrypoint.sh\"]\n          args: [\"/opt/bitnami/scripts/postgresql/run.sh\"]\n          securityContext:\n            runAsUser: 1001\n          env:\n            - name: BITNAMI_DEBUG\n              value: \"false\"\n            - name: POSTGRESQL_PORT_NUMBER\n              value: \"5432\"\n            - name: POSTGRESQL_VOLUME_DIR\n              value: \"/bitnami/postgresql\"\n            - name: PGDATA\n              value: \"/bitnami/postgresql/data\"\n            - name: POSTGRES_USER\n              value: \"postgres\"\n            - name: POSTGRES_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: postgresql\n                  key: postgresql-password\n            - name: POSTGRESQL_ENABLE_LDAP\n              value: \"no\"\n          ports:\n            - name: tcp-postgresql\n              containerPort: 5432\n          livenessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          readinessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - -e\n                - |\n                  exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          volumeMounts:\n            - name: dshm\n              mountPath: /dev/shm\n            - name: data\n              mountPath: /bitnami/postgresql\n              subPath: \n      volumes:\n        - name: dshm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 1Gi\n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/postgresql/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 112, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgresql-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.9.4\n    release: \"postgresql\"\n    heritage: \"Helm\"\n  annotations:\nspec:\n  serviceName: postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: \"postgresql\"\n      role: master\n  template:\n    metadata:\n      name: postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.9.4\n        release: \"postgresql\"\n        heritage: \"Helm\"\n        role: master\n    spec:      \n      securityContext:\n        fsGroup: 1001\n      containers:\n        - name: postgresql\n          image: docker.io/bitnami/postgresql:11.7.0-debian-10-r90\n          imagePullPolicy: \"IfNotPresent\"\n          resources:\n            requests:\n              cpu: 250m\n              memory: 256Mi\n          command: [\"/opt/bitnami/scripts/postgresql/entrypoint.sh\"]\n          args: [\"/opt/bitnami/scripts/postgresql/run.sh\"]\n          securityContext:\n            runAsUser: 1001\n          env:\n            - name: BITNAMI_DEBUG\n              value: \"false\"\n            - name: POSTGRESQL_PORT_NUMBER\n              value: \"5432\"\n            - name: POSTGRESQL_VOLUME_DIR\n              value: \"/bitnami/postgresql\"\n            - name: PGDATA\n              value: \"/bitnami/postgresql/data\"\n            - name: POSTGRES_USER\n              value: \"postgres\"\n            - name: POSTGRES_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: postgresql\n                  key: postgresql-password\n            - name: POSTGRESQL_ENABLE_LDAP\n              value: \"no\"\n          ports:\n            - name: tcp-postgresql\n              containerPort: 5432\n          livenessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          readinessProbe:\n            exec:\n              command:\n                - /bin/sh\n                - -c\n                - -e\n                - |\n                  exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 6\n          volumeMounts:\n            - name: dshm\n              mountPath: /dev/shm\n            - name: data\n              mountPath: /bitnami/postgresql\n              subPath: \n      volumes:\n        - name: dshm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 1Gi\n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/postgresql/templates/svc-headless.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: postgresql-headless\n  labels:\n    app: postgresql\n    chart: postgresql-8.9.4\n    release: \"postgresql\"\n    heritage: \"Helm\"\nspec:\n  type: ClusterIP\n  clusterIP: None\n  ports:\n    - name: tcp-postgresql\n      port: 5432\n      targetPort: tcp-postgresql\n  selector:\n    app: postgresql\n    release: \"postgresql\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/logs-signing-proxy-configuration.yaml"}, "region": {"startLine": 3, "endLine": 11, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: logs-signing-proxy-configuration\ndata:\n  WRSTUDIO_INSTANCE: \n  FILE_LIMIT_SIZE: \"\"\n  LOGDIR: \n  DEFAULT_TIMESTAMP_FORMAT: \"\"\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signing-proxy\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: signing-proxy\n      app.kubernetes.io/instance: logs-signing-proxy\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: signing-proxy\n        app.kubernetes.io/instance: logs-signing-proxy\n    spec:\n      serviceAccountName: logs-signing-proxy\n      securityContext:\n        {}\n      containers:\n        - name: logs-signing-proxy\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 8080\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10          \n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10              \n          resources:\n            {}\n          envFrom:\n          - configMapRef:\n              name: logs-signing-proxy-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n            subPath: remote\n          - name: logs-signing-data\n            mountPath: /var/log/remote/logs\n            subPath: logs\n          - name: logs-signing-data\n            mountPath: /var/log/remote/ready\n            subPath: ready\n          - name: logs-signing-data\n            mountPath: /var/log/remote/sign\n            subPath: sign                      \n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signing-proxy\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signing-proxy\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: signing-proxy\n      app.kubernetes.io/instance: logs-signing-proxy\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: signing-proxy\n        app.kubernetes.io/instance: logs-signing-proxy\n    spec:\n      serviceAccountName: logs-signing-proxy\n      securityContext:\n        {}\n      containers:\n        - name: logs-signing-proxy\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 8080\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10          \n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10              \n          resources:\n            {}\n          envFrom:\n          - configMapRef:\n              name: logs-signing-proxy-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n            subPath: remote\n          - name: logs-signing-data\n            mountPath: /var/log/remote/logs\n            subPath: logs\n          - name: logs-signing-data\n            mountPath: /var/log/remote/ready\n            subPath: ready\n          - name: logs-signing-data\n            mountPath: /var/log/remote/sign\n            subPath: sign                      \n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signing-proxy\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signing-proxy\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: signing-proxy\n      app.kubernetes.io/instance: logs-signing-proxy\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: signing-proxy\n        app.kubernetes.io/instance: logs-signing-proxy\n    spec:\n      serviceAccountName: logs-signing-proxy\n      securityContext:\n        {}\n      containers:\n        - name: logs-signing-proxy\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 8080\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10          \n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10              \n          resources:\n            {}\n          envFrom:\n          - configMapRef:\n              name: logs-signing-proxy-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n            subPath: remote\n          - name: logs-signing-data\n            mountPath: /var/log/remote/logs\n            subPath: logs\n          - name: logs-signing-data\n            mountPath: /var/log/remote/ready\n            subPath: ready\n          - name: logs-signing-data\n            mountPath: /var/log/remote/sign\n            subPath: sign                      \n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signing-proxy\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signing-proxy\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: signing-proxy\n      app.kubernetes.io/instance: logs-signing-proxy\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: signing-proxy\n        app.kubernetes.io/instance: logs-signing-proxy\n    spec:\n      serviceAccountName: logs-signing-proxy\n      securityContext:\n        {}\n      containers:\n        - name: logs-signing-proxy\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 8080\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10          \n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10              \n          resources:\n            {}\n          envFrom:\n          - configMapRef:\n              name: logs-signing-proxy-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n            subPath: remote\n          - name: logs-signing-data\n            mountPath: /var/log/remote/logs\n            subPath: logs\n          - name: logs-signing-data\n            mountPath: /var/log/remote/ready\n            subPath: ready\n          - name: logs-signing-data\n            mountPath: /var/log/remote/sign\n            subPath: sign                      \n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signing-proxy\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signing-proxy\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: signing-proxy\n      app.kubernetes.io/instance: logs-signing-proxy\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: signing-proxy\n        app.kubernetes.io/instance: logs-signing-proxy\n    spec:\n      serviceAccountName: logs-signing-proxy\n      securityContext:\n        {}\n      containers:\n        - name: logs-signing-proxy\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 8080\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10          \n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10              \n          resources:\n            {}\n          envFrom:\n          - configMapRef:\n              name: logs-signing-proxy-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n            subPath: remote\n          - name: logs-signing-data\n            mountPath: /var/log/remote/logs\n            subPath: logs\n          - name: logs-signing-data\n            mountPath: /var/log/remote/ready\n            subPath: ready\n          - name: logs-signing-data\n            mountPath: /var/log/remote/sign\n            subPath: sign                      \n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signing-proxy\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signing-proxy\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: signing-proxy\n      app.kubernetes.io/instance: logs-signing-proxy\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: signing-proxy\n        app.kubernetes.io/instance: logs-signing-proxy\n    spec:\n      serviceAccountName: logs-signing-proxy\n      securityContext:\n        {}\n      containers:\n        - name: logs-signing-proxy\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 8080\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10          \n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10              \n          resources:\n            {}\n          envFrom:\n          - configMapRef:\n              name: logs-signing-proxy-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n            subPath: remote\n          - name: logs-signing-data\n            mountPath: /var/log/remote/logs\n            subPath: logs\n          - name: logs-signing-data\n            mountPath: /var/log/remote/ready\n            subPath: ready\n          - name: logs-signing-data\n            mountPath: /var/log/remote/sign\n            subPath: sign                      \n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signing-proxy\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signing-proxy\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: signing-proxy\n      app.kubernetes.io/instance: logs-signing-proxy\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: signing-proxy\n        app.kubernetes.io/instance: logs-signing-proxy\n    spec:\n      serviceAccountName: logs-signing-proxy\n      securityContext:\n        {}\n      containers:\n        - name: logs-signing-proxy\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 8080\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10          \n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10              \n          resources:\n            {}\n          envFrom:\n          - configMapRef:\n              name: logs-signing-proxy-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n            subPath: remote\n          - name: logs-signing-data\n            mountPath: /var/log/remote/logs\n            subPath: logs\n          - name: logs-signing-data\n            mountPath: /var/log/remote/ready\n            subPath: ready\n          - name: logs-signing-data\n            mountPath: /var/log/remote/sign\n            subPath: sign                      \n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signing-proxy\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signing-proxy\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: signing-proxy\n      app.kubernetes.io/instance: logs-signing-proxy\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: signing-proxy\n        app.kubernetes.io/instance: logs-signing-proxy\n    spec:\n      serviceAccountName: logs-signing-proxy\n      securityContext:\n        {}\n      containers:\n        - name: logs-signing-proxy\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 8080\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10          \n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10              \n          resources:\n            {}\n          envFrom:\n          - configMapRef:\n              name: logs-signing-proxy-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n            subPath: remote\n          - name: logs-signing-data\n            mountPath: /var/log/remote/logs\n            subPath: logs\n          - name: logs-signing-data\n            mountPath: /var/log/remote/ready\n            subPath: ready\n          - name: logs-signing-data\n            mountPath: /var/log/remote/sign\n            subPath: sign                      \n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signing-proxy\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signing-proxy\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: signing-proxy\n      app.kubernetes.io/instance: logs-signing-proxy\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: signing-proxy\n        app.kubernetes.io/instance: logs-signing-proxy\n    spec:\n      serviceAccountName: logs-signing-proxy\n      securityContext:\n        {}\n      containers:\n        - name: logs-signing-proxy\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 8080\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10          \n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10              \n          resources:\n            {}\n          envFrom:\n          - configMapRef:\n              name: logs-signing-proxy-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n            subPath: remote\n          - name: logs-signing-data\n            mountPath: /var/log/remote/logs\n            subPath: logs\n          - name: logs-signing-data\n            mountPath: /var/log/remote/ready\n            subPath: ready\n          - name: logs-signing-data\n            mountPath: /var/log/remote/sign\n            subPath: sign                      \n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signing-proxy\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signing-proxy\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: signing-proxy\n      app.kubernetes.io/instance: logs-signing-proxy\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: signing-proxy\n        app.kubernetes.io/instance: logs-signing-proxy\n    spec:\n      serviceAccountName: logs-signing-proxy\n      securityContext:\n        {}\n      containers:\n        - name: logs-signing-proxy\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 8080\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10          \n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10              \n          resources:\n            {}\n          envFrom:\n          - configMapRef:\n              name: logs-signing-proxy-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n            subPath: remote\n          - name: logs-signing-data\n            mountPath: /var/log/remote/logs\n            subPath: logs\n          - name: logs-signing-data\n            mountPath: /var/log/remote/ready\n            subPath: ready\n          - name: logs-signing-data\n            mountPath: /var/log/remote/sign\n            subPath: sign                      \n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signing-proxy\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signing-proxy\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: signing-proxy\n      app.kubernetes.io/instance: logs-signing-proxy\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: signing-proxy\n        app.kubernetes.io/instance: logs-signing-proxy\n    spec:\n      serviceAccountName: logs-signing-proxy\n      securityContext:\n        {}\n      containers:\n        - name: logs-signing-proxy\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 8080\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10          \n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10              \n          resources:\n            {}\n          envFrom:\n          - configMapRef:\n              name: logs-signing-proxy-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n            subPath: remote\n          - name: logs-signing-data\n            mountPath: /var/log/remote/logs\n            subPath: logs\n          - name: logs-signing-data\n            mountPath: /var/log/remote/ready\n            subPath: ready\n          - name: logs-signing-data\n            mountPath: /var/log/remote/sign\n            subPath: sign                      \n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signing-proxy\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signing-proxy\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: signing-proxy\n      app.kubernetes.io/instance: logs-signing-proxy\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: signing-proxy\n        app.kubernetes.io/instance: logs-signing-proxy\n    spec:\n      serviceAccountName: logs-signing-proxy\n      securityContext:\n        {}\n      containers:\n        - name: logs-signing-proxy\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 8080\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10          \n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10              \n          resources:\n            {}\n          envFrom:\n          - configMapRef:\n              name: logs-signing-proxy-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n            subPath: remote\n          - name: logs-signing-data\n            mountPath: /var/log/remote/logs\n            subPath: logs\n          - name: logs-signing-data\n            mountPath: /var/log/remote/ready\n            subPath: ready\n          - name: logs-signing-data\n            mountPath: /var/log/remote/sign\n            subPath: sign                      \n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signing-proxy\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: logs-signing-proxy\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: signing-proxy\n      app.kubernetes.io/instance: logs-signing-proxy\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: signing-proxy\n        app.kubernetes.io/instance: logs-signing-proxy\n    spec:\n      serviceAccountName: logs-signing-proxy\n      securityContext:\n        {}\n      containers:\n        - name: logs-signing-proxy\n          securityContext:\n            allowPrivilegeEscalation: false\n          image: \":1.16.0\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 8080\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10          \n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            failureThreshold: 30\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10              \n          resources:\n            {}\n          envFrom:\n          - configMapRef:\n              name: logs-signing-proxy-configuration\n          volumeMounts:\n          - name: logs-signing-data\n            mountPath: /var/log/remote\n            subPath: remote\n          - name: logs-signing-data\n            mountPath: /var/log/remote/logs\n            subPath: logs\n          - name: logs-signing-data\n            mountPath: /var/log/remote/ready\n            subPath: ready\n          - name: logs-signing-data\n            mountPath: /var/log/remote/sign\n            subPath: sign                      \n      volumes:\n      - name: logs-signing-data\n        persistentVolumeClaim:\n          claimName: logs-signing-proxy\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/service.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: logs-signing-proxy\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: 8080\n      protocol: TCP\n      name: signing-proxy\n  selector:\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 12, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: logs-signing-proxy\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signing-proxy-test-connection\"\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signing-proxy:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signing-proxy-test-connection\"\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signing-proxy:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signing-proxy-test-connection\"\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signing-proxy:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signing-proxy-test-connection\"\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signing-proxy:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signing-proxy-test-connection\"\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signing-proxy:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signing-proxy-test-connection\"\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signing-proxy:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signing-proxy-test-connection\"\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signing-proxy:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signing-proxy-test-connection\"\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signing-proxy:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signing-proxy-test-connection\"\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signing-proxy:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signing-proxy-test-connection\"\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signing-proxy:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signing-proxy-test-connection\"\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signing-proxy:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signing-proxy-test-connection\"\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signing-proxy:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signing-proxy-test-connection\"\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signing-proxy:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signing-proxy-test-connection\"\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signing-proxy:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signing-proxy-test-connection\"\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signing-proxy:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signing-proxy-test-connection\"\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signing-proxy:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signing-proxy-test-connection\"\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signing-proxy:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signing-proxy-test-connection\"\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signing-proxy:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/logs-signing-proxy/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"logs-signing-proxy-test-connection\"\n  labels:\n    helm.sh/chart: logs-signing-proxy-0.1.0\n    app.kubernetes.io/name: signing-proxy\n    app.kubernetes.io/instance: logs-signing-proxy\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['logs-signing-proxy:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-private-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-private-gateway\n  labels:\n    helm.sh/chart: usp-private-gateway-0.1.0\n    app.kubernetes.io/name: usp-private-gateway\n    app.kubernetes.io/instance: usp-private-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-private-gateway\n      app.kubernetes.io/instance: usp-private-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-private-gateway\n        app.kubernetes.io/instance: usp-private-gateway\n    spec:\n      serviceAccountName: usp-private-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-private-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/private-api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3001\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-private-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-private-gateway\n  labels:\n    helm.sh/chart: usp-private-gateway-0.1.0\n    app.kubernetes.io/name: usp-private-gateway\n    app.kubernetes.io/instance: usp-private-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-private-gateway\n      app.kubernetes.io/instance: usp-private-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-private-gateway\n        app.kubernetes.io/instance: usp-private-gateway\n    spec:\n      serviceAccountName: usp-private-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-private-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/private-api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3001\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-private-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-private-gateway\n  labels:\n    helm.sh/chart: usp-private-gateway-0.1.0\n    app.kubernetes.io/name: usp-private-gateway\n    app.kubernetes.io/instance: usp-private-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-private-gateway\n      app.kubernetes.io/instance: usp-private-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-private-gateway\n        app.kubernetes.io/instance: usp-private-gateway\n    spec:\n      serviceAccountName: usp-private-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-private-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/private-api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3001\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-private-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-private-gateway\n  labels:\n    helm.sh/chart: usp-private-gateway-0.1.0\n    app.kubernetes.io/name: usp-private-gateway\n    app.kubernetes.io/instance: usp-private-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-private-gateway\n      app.kubernetes.io/instance: usp-private-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-private-gateway\n        app.kubernetes.io/instance: usp-private-gateway\n    spec:\n      serviceAccountName: usp-private-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-private-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/private-api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3001\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-private-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-private-gateway\n  labels:\n    helm.sh/chart: usp-private-gateway-0.1.0\n    app.kubernetes.io/name: usp-private-gateway\n    app.kubernetes.io/instance: usp-private-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-private-gateway\n      app.kubernetes.io/instance: usp-private-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-private-gateway\n        app.kubernetes.io/instance: usp-private-gateway\n    spec:\n      serviceAccountName: usp-private-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-private-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/private-api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3001\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-private-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-private-gateway\n  labels:\n    helm.sh/chart: usp-private-gateway-0.1.0\n    app.kubernetes.io/name: usp-private-gateway\n    app.kubernetes.io/instance: usp-private-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-private-gateway\n      app.kubernetes.io/instance: usp-private-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-private-gateway\n        app.kubernetes.io/instance: usp-private-gateway\n    spec:\n      serviceAccountName: usp-private-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-private-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/private-api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3001\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-private-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-private-gateway\n  labels:\n    helm.sh/chart: usp-private-gateway-0.1.0\n    app.kubernetes.io/name: usp-private-gateway\n    app.kubernetes.io/instance: usp-private-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-private-gateway\n      app.kubernetes.io/instance: usp-private-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-private-gateway\n        app.kubernetes.io/instance: usp-private-gateway\n    spec:\n      serviceAccountName: usp-private-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-private-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/private-api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3001\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-private-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-private-gateway\n  labels:\n    helm.sh/chart: usp-private-gateway-0.1.0\n    app.kubernetes.io/name: usp-private-gateway\n    app.kubernetes.io/instance: usp-private-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-private-gateway\n      app.kubernetes.io/instance: usp-private-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-private-gateway\n        app.kubernetes.io/instance: usp-private-gateway\n    spec:\n      serviceAccountName: usp-private-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-private-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/private-api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3001\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-private-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-private-gateway\n  labels:\n    helm.sh/chart: usp-private-gateway-0.1.0\n    app.kubernetes.io/name: usp-private-gateway\n    app.kubernetes.io/instance: usp-private-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-private-gateway\n      app.kubernetes.io/instance: usp-private-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-private-gateway\n        app.kubernetes.io/instance: usp-private-gateway\n    spec:\n      serviceAccountName: usp-private-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-private-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/private-api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3001\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-private-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-private-gateway\n  labels:\n    helm.sh/chart: usp-private-gateway-0.1.0\n    app.kubernetes.io/name: usp-private-gateway\n    app.kubernetes.io/instance: usp-private-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-private-gateway\n      app.kubernetes.io/instance: usp-private-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-private-gateway\n        app.kubernetes.io/instance: usp-private-gateway\n    spec:\n      serviceAccountName: usp-private-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-private-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/private-api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3001\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-private-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-private-gateway\n  labels:\n    helm.sh/chart: usp-private-gateway-0.1.0\n    app.kubernetes.io/name: usp-private-gateway\n    app.kubernetes.io/instance: usp-private-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-private-gateway\n      app.kubernetes.io/instance: usp-private-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-private-gateway\n        app.kubernetes.io/instance: usp-private-gateway\n    spec:\n      serviceAccountName: usp-private-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-private-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/private-api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3001\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-private-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-private-gateway\n  labels:\n    helm.sh/chart: usp-private-gateway-0.1.0\n    app.kubernetes.io/name: usp-private-gateway\n    app.kubernetes.io/instance: usp-private-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-private-gateway\n      app.kubernetes.io/instance: usp-private-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-private-gateway\n        app.kubernetes.io/instance: usp-private-gateway\n    spec:\n      serviceAccountName: usp-private-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-private-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/private-api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3001\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-private-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-private-gateway\n  labels:\n    helm.sh/chart: usp-private-gateway-0.1.0\n    app.kubernetes.io/name: usp-private-gateway\n    app.kubernetes.io/instance: usp-private-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-private-gateway\n      app.kubernetes.io/instance: usp-private-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-private-gateway\n        app.kubernetes.io/instance: usp-private-gateway\n    spec:\n      serviceAccountName: usp-private-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-private-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/private-api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3001\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-private-gateway/templates/service.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: usp-private-gateway\n  labels:\n    helm.sh/chart: usp-private-gateway-0.1.0\n    app.kubernetes.io/name: usp-private-gateway\n    app.kubernetes.io/instance: usp-private-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: NodePort\n  ports:\n    - port: 80\n      targetPort: http\n      protocol: TCP\n      name: http\n  selector:\n    app.kubernetes.io/name: usp-private-gateway\n    app.kubernetes.io/instance: usp-private-gateway\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-private-gateway/templates/serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 12, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: usp-private-gateway\n  labels:\n    helm.sh/chart: usp-private-gateway-0.1.0\n    app.kubernetes.io/name: usp-private-gateway\n    app.kubernetes.io/instance: usp-private-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/ingress/gateway.yaml"}, "region": {"startLine": 3, "endLine": 32, "snippet": {"text": "apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: portal-apigateway\n  labels:\n    io.kompose.service: tls-terminator\n    helm.sh/chart: portal-apigateway-1.0.0\n    app.kubernetes.io/name: portal-apigateway\n    app.kubernetes.io/instance: portal-apigateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-devstar\n    kubernetes.io/ingress.class: nginx\nspec:\n  tls:\n    - hosts:\n        - \"portal-dev1.gateway.dev.wrstudio1.cloud\"\n      secretName: portal-dev1.gateway.dev.wrstudio1.cloud-tls\n  rules:\n    - host: \"portal-dev1.gateway.dev.wrstudio1.cloud\"\n      http:\n        paths:\n          - path: /\n            pathType: ImplementationSpecific\n            backend:\n              service: \n                name: portal-apigateway\n                port:\n                  number: 3000\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/serviceaccount/component-management.yaml"}, "region": {"startLine": 3, "endLine": 8, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: \"portal-component-management\"\n  namespace: default\nautomountServiceAccountToken: true\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/gateway.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-apigateway\n  labels:\n    helm.sh/chart: portal-apigateway-1.0.0\n    app.kubernetes.io/name: portal-apigateway\n    app.kubernetes.io/instance: portal-apigateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-apigateway\n      app.kubernetes.io/instance: portal-apigateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: portal-apigateway\n        app.kubernetes.io/instance: portal-apigateway\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      containers:\n        - name: portal-apigateway\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/api-gateway:0.0.26\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/gateway.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-apigateway\n  labels:\n    helm.sh/chart: portal-apigateway-1.0.0\n    app.kubernetes.io/name: portal-apigateway\n    app.kubernetes.io/instance: portal-apigateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-apigateway\n      app.kubernetes.io/instance: portal-apigateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: portal-apigateway\n        app.kubernetes.io/instance: portal-apigateway\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      containers:\n        - name: portal-apigateway\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/api-gateway:0.0.26\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/gateway.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-apigateway\n  labels:\n    helm.sh/chart: portal-apigateway-1.0.0\n    app.kubernetes.io/name: portal-apigateway\n    app.kubernetes.io/instance: portal-apigateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-apigateway\n      app.kubernetes.io/instance: portal-apigateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: portal-apigateway\n        app.kubernetes.io/instance: portal-apigateway\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      containers:\n        - name: portal-apigateway\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/api-gateway:0.0.26\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/gateway.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-apigateway\n  labels:\n    helm.sh/chart: portal-apigateway-1.0.0\n    app.kubernetes.io/name: portal-apigateway\n    app.kubernetes.io/instance: portal-apigateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-apigateway\n      app.kubernetes.io/instance: portal-apigateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: portal-apigateway\n        app.kubernetes.io/instance: portal-apigateway\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      containers:\n        - name: portal-apigateway\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/api-gateway:0.0.26\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/gateway.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-apigateway\n  labels:\n    helm.sh/chart: portal-apigateway-1.0.0\n    app.kubernetes.io/name: portal-apigateway\n    app.kubernetes.io/instance: portal-apigateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-apigateway\n      app.kubernetes.io/instance: portal-apigateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: portal-apigateway\n        app.kubernetes.io/instance: portal-apigateway\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      containers:\n        - name: portal-apigateway\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/api-gateway:0.0.26\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/gateway.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-apigateway\n  labels:\n    helm.sh/chart: portal-apigateway-1.0.0\n    app.kubernetes.io/name: portal-apigateway\n    app.kubernetes.io/instance: portal-apigateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-apigateway\n      app.kubernetes.io/instance: portal-apigateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: portal-apigateway\n        app.kubernetes.io/instance: portal-apigateway\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      containers:\n        - name: portal-apigateway\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/api-gateway:0.0.26\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/gateway.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-apigateway\n  labels:\n    helm.sh/chart: portal-apigateway-1.0.0\n    app.kubernetes.io/name: portal-apigateway\n    app.kubernetes.io/instance: portal-apigateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-apigateway\n      app.kubernetes.io/instance: portal-apigateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: portal-apigateway\n        app.kubernetes.io/instance: portal-apigateway\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      containers:\n        - name: portal-apigateway\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/api-gateway:0.0.26\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/gateway.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-apigateway\n  labels:\n    helm.sh/chart: portal-apigateway-1.0.0\n    app.kubernetes.io/name: portal-apigateway\n    app.kubernetes.io/instance: portal-apigateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-apigateway\n      app.kubernetes.io/instance: portal-apigateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: portal-apigateway\n        app.kubernetes.io/instance: portal-apigateway\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      containers:\n        - name: portal-apigateway\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/api-gateway:0.0.26\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/gateway.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-apigateway\n  labels:\n    helm.sh/chart: portal-apigateway-1.0.0\n    app.kubernetes.io/name: portal-apigateway\n    app.kubernetes.io/instance: portal-apigateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-apigateway\n      app.kubernetes.io/instance: portal-apigateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: portal-apigateway\n        app.kubernetes.io/instance: portal-apigateway\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      containers:\n        - name: portal-apigateway\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/api-gateway:0.0.26\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/gateway.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-apigateway\n  labels:\n    helm.sh/chart: portal-apigateway-1.0.0\n    app.kubernetes.io/name: portal-apigateway\n    app.kubernetes.io/instance: portal-apigateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-apigateway\n      app.kubernetes.io/instance: portal-apigateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: portal-apigateway\n        app.kubernetes.io/instance: portal-apigateway\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      containers:\n        - name: portal-apigateway\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/api-gateway:0.0.26\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/gateway.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-apigateway\n  labels:\n    helm.sh/chart: portal-apigateway-1.0.0\n    app.kubernetes.io/name: portal-apigateway\n    app.kubernetes.io/instance: portal-apigateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-apigateway\n      app.kubernetes.io/instance: portal-apigateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: portal-apigateway\n        app.kubernetes.io/instance: portal-apigateway\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      containers:\n        - name: portal-apigateway\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/api-gateway:0.0.26\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/gateway.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-apigateway\n  labels:\n    helm.sh/chart: portal-apigateway-1.0.0\n    app.kubernetes.io/name: portal-apigateway\n    app.kubernetes.io/instance: portal-apigateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-apigateway\n      app.kubernetes.io/instance: portal-apigateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: portal-apigateway\n        app.kubernetes.io/instance: portal-apigateway\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      containers:\n        - name: portal-apigateway\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/api-gateway:0.0.26\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n            timeoutSeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            failureThreshold: 3\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/component-management.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-component-management\n  labels:\n    helm.sh/chart: portal-component-management-1.0.0\n    app.kubernetes.io/name: portal-component-management\n    app.kubernetes.io/instance: portal-component-management\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-component-management\n      app.kubernetes.io/instance: portal-component-management\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: portal-component-management\n        app.kubernetes.io/instance: portal-component-management\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      serviceAccountName: portal-component-management\n      containers:\n        - name: portal-component-management\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/component-management:1.0.6\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: VAULT_ENABLE\n              value: \"false\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"um-patroni-test.test-um.svc.cluster.local\\\",\\\"port\\\":5432,\\\"provider\\\":\\\"pg\\\",\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false,\\\"required\\\":true},\\\"user\\\":\\\"postgres\\\"}\"\n            - name: NODE_CFG_PG_SECRETS\n              value: \"{\\\"password\\\":\\\"tea\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/component-management.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-component-management\n  labels:\n    helm.sh/chart: portal-component-management-1.0.0\n    app.kubernetes.io/name: portal-component-management\n    app.kubernetes.io/instance: portal-component-management\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-component-management\n      app.kubernetes.io/instance: portal-component-management\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: portal-component-management\n        app.kubernetes.io/instance: portal-component-management\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      serviceAccountName: portal-component-management\n      containers:\n        - name: portal-component-management\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/component-management:1.0.6\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: VAULT_ENABLE\n              value: \"false\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"um-patroni-test.test-um.svc.cluster.local\\\",\\\"port\\\":5432,\\\"provider\\\":\\\"pg\\\",\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false,\\\"required\\\":true},\\\"user\\\":\\\"postgres\\\"}\"\n            - name: NODE_CFG_PG_SECRETS\n              value: \"{\\\"password\\\":\\\"tea\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/component-management.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-component-management\n  labels:\n    helm.sh/chart: portal-component-management-1.0.0\n    app.kubernetes.io/name: portal-component-management\n    app.kubernetes.io/instance: portal-component-management\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-component-management\n      app.kubernetes.io/instance: portal-component-management\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: portal-component-management\n        app.kubernetes.io/instance: portal-component-management\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      serviceAccountName: portal-component-management\n      containers:\n        - name: portal-component-management\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/component-management:1.0.6\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: VAULT_ENABLE\n              value: \"false\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"um-patroni-test.test-um.svc.cluster.local\\\",\\\"port\\\":5432,\\\"provider\\\":\\\"pg\\\",\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false,\\\"required\\\":true},\\\"user\\\":\\\"postgres\\\"}\"\n            - name: NODE_CFG_PG_SECRETS\n              value: \"{\\\"password\\\":\\\"tea\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/component-management.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-component-management\n  labels:\n    helm.sh/chart: portal-component-management-1.0.0\n    app.kubernetes.io/name: portal-component-management\n    app.kubernetes.io/instance: portal-component-management\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-component-management\n      app.kubernetes.io/instance: portal-component-management\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: portal-component-management\n        app.kubernetes.io/instance: portal-component-management\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      serviceAccountName: portal-component-management\n      containers:\n        - name: portal-component-management\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/component-management:1.0.6\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: VAULT_ENABLE\n              value: \"false\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"um-patroni-test.test-um.svc.cluster.local\\\",\\\"port\\\":5432,\\\"provider\\\":\\\"pg\\\",\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false,\\\"required\\\":true},\\\"user\\\":\\\"postgres\\\"}\"\n            - name: NODE_CFG_PG_SECRETS\n              value: \"{\\\"password\\\":\\\"tea\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/component-management.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-component-management\n  labels:\n    helm.sh/chart: portal-component-management-1.0.0\n    app.kubernetes.io/name: portal-component-management\n    app.kubernetes.io/instance: portal-component-management\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-component-management\n      app.kubernetes.io/instance: portal-component-management\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: portal-component-management\n        app.kubernetes.io/instance: portal-component-management\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      serviceAccountName: portal-component-management\n      containers:\n        - name: portal-component-management\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/component-management:1.0.6\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: VAULT_ENABLE\n              value: \"false\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"um-patroni-test.test-um.svc.cluster.local\\\",\\\"port\\\":5432,\\\"provider\\\":\\\"pg\\\",\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false,\\\"required\\\":true},\\\"user\\\":\\\"postgres\\\"}\"\n            - name: NODE_CFG_PG_SECRETS\n              value: \"{\\\"password\\\":\\\"tea\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/component-management.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-component-management\n  labels:\n    helm.sh/chart: portal-component-management-1.0.0\n    app.kubernetes.io/name: portal-component-management\n    app.kubernetes.io/instance: portal-component-management\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-component-management\n      app.kubernetes.io/instance: portal-component-management\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: portal-component-management\n        app.kubernetes.io/instance: portal-component-management\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      serviceAccountName: portal-component-management\n      containers:\n        - name: portal-component-management\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/component-management:1.0.6\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: VAULT_ENABLE\n              value: \"false\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"um-patroni-test.test-um.svc.cluster.local\\\",\\\"port\\\":5432,\\\"provider\\\":\\\"pg\\\",\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false,\\\"required\\\":true},\\\"user\\\":\\\"postgres\\\"}\"\n            - name: NODE_CFG_PG_SECRETS\n              value: \"{\\\"password\\\":\\\"tea\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/component-management.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-component-management\n  labels:\n    helm.sh/chart: portal-component-management-1.0.0\n    app.kubernetes.io/name: portal-component-management\n    app.kubernetes.io/instance: portal-component-management\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-component-management\n      app.kubernetes.io/instance: portal-component-management\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: portal-component-management\n        app.kubernetes.io/instance: portal-component-management\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      serviceAccountName: portal-component-management\n      containers:\n        - name: portal-component-management\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/component-management:1.0.6\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: VAULT_ENABLE\n              value: \"false\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"um-patroni-test.test-um.svc.cluster.local\\\",\\\"port\\\":5432,\\\"provider\\\":\\\"pg\\\",\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false,\\\"required\\\":true},\\\"user\\\":\\\"postgres\\\"}\"\n            - name: NODE_CFG_PG_SECRETS\n              value: \"{\\\"password\\\":\\\"tea\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/component-management.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-component-management\n  labels:\n    helm.sh/chart: portal-component-management-1.0.0\n    app.kubernetes.io/name: portal-component-management\n    app.kubernetes.io/instance: portal-component-management\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-component-management\n      app.kubernetes.io/instance: portal-component-management\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: portal-component-management\n        app.kubernetes.io/instance: portal-component-management\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      serviceAccountName: portal-component-management\n      containers:\n        - name: portal-component-management\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/component-management:1.0.6\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: VAULT_ENABLE\n              value: \"false\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"um-patroni-test.test-um.svc.cluster.local\\\",\\\"port\\\":5432,\\\"provider\\\":\\\"pg\\\",\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false,\\\"required\\\":true},\\\"user\\\":\\\"postgres\\\"}\"\n            - name: NODE_CFG_PG_SECRETS\n              value: \"{\\\"password\\\":\\\"tea\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/component-management.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-component-management\n  labels:\n    helm.sh/chart: portal-component-management-1.0.0\n    app.kubernetes.io/name: portal-component-management\n    app.kubernetes.io/instance: portal-component-management\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-component-management\n      app.kubernetes.io/instance: portal-component-management\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: portal-component-management\n        app.kubernetes.io/instance: portal-component-management\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      serviceAccountName: portal-component-management\n      containers:\n        - name: portal-component-management\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/component-management:1.0.6\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: VAULT_ENABLE\n              value: \"false\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"um-patroni-test.test-um.svc.cluster.local\\\",\\\"port\\\":5432,\\\"provider\\\":\\\"pg\\\",\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false,\\\"required\\\":true},\\\"user\\\":\\\"postgres\\\"}\"\n            - name: NODE_CFG_PG_SECRETS\n              value: \"{\\\"password\\\":\\\"tea\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/component-management.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-component-management\n  labels:\n    helm.sh/chart: portal-component-management-1.0.0\n    app.kubernetes.io/name: portal-component-management\n    app.kubernetes.io/instance: portal-component-management\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-component-management\n      app.kubernetes.io/instance: portal-component-management\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: portal-component-management\n        app.kubernetes.io/instance: portal-component-management\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      serviceAccountName: portal-component-management\n      containers:\n        - name: portal-component-management\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/component-management:1.0.6\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: VAULT_ENABLE\n              value: \"false\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"um-patroni-test.test-um.svc.cluster.local\\\",\\\"port\\\":5432,\\\"provider\\\":\\\"pg\\\",\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false,\\\"required\\\":true},\\\"user\\\":\\\"postgres\\\"}\"\n            - name: NODE_CFG_PG_SECRETS\n              value: \"{\\\"password\\\":\\\"tea\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/component-management.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-component-management\n  labels:\n    helm.sh/chart: portal-component-management-1.0.0\n    app.kubernetes.io/name: portal-component-management\n    app.kubernetes.io/instance: portal-component-management\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-component-management\n      app.kubernetes.io/instance: portal-component-management\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: portal-component-management\n        app.kubernetes.io/instance: portal-component-management\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      serviceAccountName: portal-component-management\n      containers:\n        - name: portal-component-management\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/component-management:1.0.6\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: VAULT_ENABLE\n              value: \"false\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"um-patroni-test.test-um.svc.cluster.local\\\",\\\"port\\\":5432,\\\"provider\\\":\\\"pg\\\",\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false,\\\"required\\\":true},\\\"user\\\":\\\"postgres\\\"}\"\n            - name: NODE_CFG_PG_SECRETS\n              value: \"{\\\"password\\\":\\\"tea\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/component-management.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-component-management\n  labels:\n    helm.sh/chart: portal-component-management-1.0.0\n    app.kubernetes.io/name: portal-component-management\n    app.kubernetes.io/instance: portal-component-management\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-component-management\n      app.kubernetes.io/instance: portal-component-management\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: portal-component-management\n        app.kubernetes.io/instance: portal-component-management\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      serviceAccountName: portal-component-management\n      containers:\n        - name: portal-component-management\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/component-management:1.0.6\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: VAULT_ENABLE\n              value: \"false\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"um-patroni-test.test-um.svc.cluster.local\\\",\\\"port\\\":5432,\\\"provider\\\":\\\"pg\\\",\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false,\\\"required\\\":true},\\\"user\\\":\\\"postgres\\\"}\"\n            - name: NODE_CFG_PG_SECRETS\n              value: \"{\\\"password\\\":\\\"tea\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/component-management.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-component-management\n  labels:\n    helm.sh/chart: portal-component-management-1.0.0\n    app.kubernetes.io/name: portal-component-management\n    app.kubernetes.io/instance: portal-component-management\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-component-management\n      app.kubernetes.io/instance: portal-component-management\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: portal-component-management\n        app.kubernetes.io/instance: portal-component-management\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      serviceAccountName: portal-component-management\n      containers:\n        - name: portal-component-management\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/component-management:1.0.6\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: VAULT_ENABLE\n              value: \"false\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"um-patroni-test.test-um.svc.cluster.local\\\",\\\"port\\\":5432,\\\"provider\\\":\\\"pg\\\",\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false,\\\"required\\\":true},\\\"user\\\":\\\"postgres\\\"}\"\n            - name: NODE_CFG_PG_SECRETS\n              value: \"{\\\"password\\\":\\\"tea\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/deployment/component-management.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal-component-management\n  labels:\n    helm.sh/chart: portal-component-management-1.0.0\n    app.kubernetes.io/name: portal-component-management\n    app.kubernetes.io/instance: portal-component-management\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: portal-component-management\n      app.kubernetes.io/instance: portal-component-management\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: portal-component-management\n        app.kubernetes.io/instance: portal-component-management\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n      serviceAccountName: portal-component-management\n      containers:\n        - name: portal-component-management\n          securityContext:\n            {}\n          image: \"system.registry.dev.wrstudio1.cloud/usp-litterbox/wrai-usp/portal/portal-be/component-management:1.0.6\"\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n          env:\n            - name: USP_ENV_NAME\n              value: testing\n            - name: VAULT_ENABLE\n              value: \"false\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka.dev3-wrai-usp-datapipeline.svc.cluster.local:9092\\\"]}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"https://um-test1.gateway.dev.wrstudio1.cloud/\\\",\\\"authEndpoint\\\":\\\"https://openapi.usp.aws-training.wrstudio.cloud/api-doc/public/v1/#/Auth/postSignIn\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"level\\\":10,\\\"msName\\\":\\\"Portal\\\",\\\"writeLogLevel\\\":40,\\\"writeLogs\\\":false}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"um-patroni-test.test-um.svc.cluster.local\\\",\\\"port\\\":5432,\\\"provider\\\":\\\"pg\\\",\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false,\\\"required\\\":true},\\\"user\\\":\\\"postgres\\\"}\"\n            - name: NODE_CFG_PG_SECRETS\n              value: \"{\\\"password\\\":\\\"tea\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/portal/templates/service/gateway.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: portal-apigateway\n  labels:\n    helm.sh/chart: portal-apigateway-1.0.0\n    app.kubernetes.io/name: portal-apigateway\n    app.kubernetes.io/instance: portal-apigateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ClusterIP\n  ports:\n    - port: 3000\n      targetPort: 3000\n      protocol: TCP\n      name: https-gateway\n  selector:\n    app.kubernetes.io/name: portal-apigateway\n    app.kubernetes.io/instance: portal-apigateway\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/ingress/public-api.yaml"}, "region": {"startLine": 3, "endLine": 33, "snippet": {"text": "apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-devstar\n    kubernetes.io/ingress.class: nginx\n    kubernetes.io/ingress.provider: nginx\n    nginx.ingress.kubernetes.io/rewrite-target: /$1\n    nginx.ingress.kubernetes.io/proxy-body-size: 512m\n    nginx.ingress.kubernetes.io/proxy-connect-timeout: \"15\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"\n    nginx.ingress.kubernetes.io/proxy-send-timeout: \"600\"\n    nginx.ingress.kubernetes.io/service-upstream: \"true\"\n  name: public-api\n  namespace: default\nspec:\n  rules:\n  - host: \n    http:\n      paths:\n      - backend:\n          service: \n            name: public-api\n            port: \n              number: 3000\n        path: /(.*)\n        pathType: ImplementationSpecific\n  tls:\n  - hosts:\n    - \n    secretName: hive-public-api-domain-https-tls\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/ingress/api-gateway.yaml"}, "region": {"startLine": 3, "endLine": 32, "snippet": {"text": "apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    kubernetes.io/ingress.provider: nginx\n    nginx.ingress.kubernetes.io/rewrite-target: /$1\n    nginx.ingress.kubernetes.io/proxy-body-size: 512m\n    nginx.ingress.kubernetes.io/proxy-connect-timeout: \"15\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"\n    nginx.ingress.kubernetes.io/proxy-send-timeout: \"600\"\n    nginx.ingress.kubernetes.io/service-upstream: \"true\"\n  name: api-gateway\n  namespace: default\nspec:\n  rules:\n  - host: \n    http:\n      paths:\n      - backend:\n          service: \n            name: api-gateway\n            port: \n              number: 3000\n        path: /api/(.*)\n        pathType: ImplementationSpecific\n  tls:\n  - hosts:\n    - \n    secretName: hive-api-domain-https-tls\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/ingress/ui.yaml"}, "region": {"startLine": 3, "endLine": 32, "snippet": {"text": "apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-devstar\n    kubernetes.io/ingress.class: nginx\n    kubernetes.io/ingress.provider: nginx\n    nginx.ingress.kubernetes.io/proxy-body-size: 512m\n    nginx.ingress.kubernetes.io/proxy-connect-timeout: \"15\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"\n    nginx.ingress.kubernetes.io/proxy-send-timeout: \"600\"\n    nginx.ingress.kubernetes.io/service-upstream: \"true\"\n  name: ui\n  namespace: default\nspec:\n  rules:\n  - host: \n    http:\n      paths:\n      - backend:\n          service: \n            name: ui\n            port: \n              number: 80\n        path: /\n        pathType: ImplementationSpecific\n  tls:\n  - hosts:\n    - \n    secretName: hive-domain-https-tls\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/pre-install.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: pre-install\n  namespace: xxxx\n  labels:\n    app: pre-install\n  annotations:\n    \"helm.sh/hook-weight\": \"-4\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: pre-install\n      labels:\n        app: pre-install\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"pre-install\"\n          image: \n          env:\n            - name: USERNAME\n              value: \"xx\"\n            - name: PASSWORD\n              value: \"xxx\"\n            - name: UM_URL\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/pre-install.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: pre-install\n  namespace: xxxx\n  labels:\n    app: pre-install\n  annotations:\n    \"helm.sh/hook-weight\": \"-4\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: pre-install\n      labels:\n        app: pre-install\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"pre-install\"\n          image: \n          env:\n            - name: USERNAME\n              value: \"xx\"\n            - name: PASSWORD\n              value: \"xxx\"\n            - name: UM_URL\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n"}}}}]}, {"ruleId": "CKV_K8S_33", "ruleIndex": 21, "level": "error", "attachments": [], "message": {"text": "Ensure the Kubernetes dashboard is not deployed"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/pre-install.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: pre-install\n  namespace: xxxx\n  labels:\n    app: pre-install\n  annotations:\n    \"helm.sh/hook-weight\": \"-4\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: pre-install\n      labels:\n        app: pre-install\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"pre-install\"\n          image: \n          env:\n            - name: USERNAME\n              value: \"xx\"\n            - name: PASSWORD\n              value: \"xxx\"\n            - name: UM_URL\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/pre-install.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: pre-install\n  namespace: xxxx\n  labels:\n    app: pre-install\n  annotations:\n    \"helm.sh/hook-weight\": \"-4\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: pre-install\n      labels:\n        app: pre-install\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"pre-install\"\n          image: \n          env:\n            - name: USERNAME\n              value: \"xx\"\n            - name: PASSWORD\n              value: \"xxx\"\n            - name: UM_URL\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/pre-install.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: pre-install\n  namespace: xxxx\n  labels:\n    app: pre-install\n  annotations:\n    \"helm.sh/hook-weight\": \"-4\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: pre-install\n      labels:\n        app: pre-install\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"pre-install\"\n          image: \n          env:\n            - name: USERNAME\n              value: \"xx\"\n            - name: PASSWORD\n              value: \"xxx\"\n            - name: UM_URL\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/pre-install.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: pre-install\n  namespace: xxxx\n  labels:\n    app: pre-install\n  annotations:\n    \"helm.sh/hook-weight\": \"-4\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: pre-install\n      labels:\n        app: pre-install\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"pre-install\"\n          image: \n          env:\n            - name: USERNAME\n              value: \"xx\"\n            - name: PASSWORD\n              value: \"xxx\"\n            - name: UM_URL\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/pre-install.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: pre-install\n  namespace: xxxx\n  labels:\n    app: pre-install\n  annotations:\n    \"helm.sh/hook-weight\": \"-4\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: pre-install\n      labels:\n        app: pre-install\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"pre-install\"\n          image: \n          env:\n            - name: USERNAME\n              value: \"xx\"\n            - name: PASSWORD\n              value: \"xxx\"\n            - name: UM_URL\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/pre-install.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: pre-install\n  namespace: xxxx\n  labels:\n    app: pre-install\n  annotations:\n    \"helm.sh/hook-weight\": \"-4\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: pre-install\n      labels:\n        app: pre-install\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"pre-install\"\n          image: \n          env:\n            - name: USERNAME\n              value: \"xx\"\n            - name: PASSWORD\n              value: \"xxx\"\n            - name: UM_URL\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/pre-install.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: pre-install\n  namespace: xxxx\n  labels:\n    app: pre-install\n  annotations:\n    \"helm.sh/hook-weight\": \"-4\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: pre-install\n      labels:\n        app: pre-install\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"pre-install\"\n          image: \n          env:\n            - name: USERNAME\n              value: \"xx\"\n            - name: PASSWORD\n              value: \"xxx\"\n            - name: UM_URL\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/pre-install.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: pre-install\n  namespace: xxxx\n  labels:\n    app: pre-install\n  annotations:\n    \"helm.sh/hook-weight\": \"-4\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: pre-install\n      labels:\n        app: pre-install\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"pre-install\"\n          image: \n          env:\n            - name: USERNAME\n              value: \"xx\"\n            - name: PASSWORD\n              value: \"xxx\"\n            - name: UM_URL\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/pre-install.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: pre-install\n  namespace: xxxx\n  labels:\n    app: pre-install\n  annotations:\n    \"helm.sh/hook-weight\": \"-4\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: pre-install\n      labels:\n        app: pre-install\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"pre-install\"\n          image: \n          env:\n            - name: USERNAME\n              value: \"xx\"\n            - name: PASSWORD\n              value: \"xxx\"\n            - name: UM_URL\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/pre-install.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: pre-install\n  namespace: xxxx\n  labels:\n    app: pre-install\n  annotations:\n    \"helm.sh/hook-weight\": \"-4\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: pre-install\n      labels:\n        app: pre-install\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"pre-install\"\n          image: \n          env:\n            - name: USERNAME\n              value: \"xx\"\n            - name: PASSWORD\n              value: \"xxx\"\n            - name: UM_URL\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/pre-install.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: pre-install\n  namespace: xxxx\n  labels:\n    app: pre-install\n  annotations:\n    \"helm.sh/hook-weight\": \"-4\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: pre-install\n      labels:\n        app: pre-install\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"pre-install\"\n          image: \n          env:\n            - name: USERNAME\n              value: \"xx\"\n            - name: PASSWORD\n              value: \"xxx\"\n            - name: UM_URL\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/pre-install.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: pre-install\n  namespace: xxxx\n  labels:\n    app: pre-install\n  annotations:\n    \"helm.sh/hook-weight\": \"-4\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: pre-install\n      labels:\n        app: pre-install\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"pre-install\"\n          image: \n          env:\n            - name: USERNAME\n              value: \"xx\"\n            - name: PASSWORD\n              value: \"xxx\"\n            - name: UM_URL\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/pre-install.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: pre-install\n  namespace: xxxx\n  labels:\n    app: pre-install\n  annotations:\n    \"helm.sh/hook-weight\": \"-4\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: pre-install\n      labels:\n        app: pre-install\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"pre-install\"\n          image: \n          env:\n            - name: USERNAME\n              value: \"xx\"\n            - name: PASSWORD\n              value: \"xxx\"\n            - name: UM_URL\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/pre-install.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: pre-install\n  namespace: xxxx\n  labels:\n    app: pre-install\n  annotations:\n    \"helm.sh/hook-weight\": \"-4\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: pre-install\n      labels:\n        app: pre-install\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"pre-install\"\n          image: \n          env:\n            - name: USERNAME\n              value: \"xx\"\n            - name: PASSWORD\n              value: \"xxx\"\n            - name: UM_URL\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/pre-install.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: pre-install\n  namespace: xxxx\n  labels:\n    app: pre-install\n  annotations:\n    \"helm.sh/hook-weight\": \"-4\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: pre-install\n      labels:\n        app: pre-install\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"pre-install\"\n          image: \n          env:\n            - name: USERNAME\n              value: \"xx\"\n            - name: PASSWORD\n              value: \"xxx\"\n            - name: UM_URL\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/pre-install.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: pre-install\n  namespace: xxxx\n  labels:\n    app: pre-install\n  annotations:\n    \"helm.sh/hook-weight\": \"-4\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: pre-install\n      labels:\n        app: pre-install\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"pre-install\"\n          image: \n          env:\n            - name: USERNAME\n              value: \"xx\"\n            - name: PASSWORD\n              value: \"xxx\"\n            - name: UM_URL\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/db-init.yaml"}, "region": {"startLine": 16, "endLine": 53, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: db-init\n  namespace: xxxx\n  labels:\n    app: db-init\n  annotations:\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: db-init\n      labels:\n        app: db-init\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"db-init\"\n          image: \":xxxx\"\n          env:\n            - name: DB_NAME\n              value: \"\"\n            - name: DB_HOST\n              value: \"\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"\"\n            - name: DB_PASSWORD\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: STAGE\n              value: \"install\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/db-init.yaml"}, "region": {"startLine": 16, "endLine": 53, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: db-init\n  namespace: xxxx\n  labels:\n    app: db-init\n  annotations:\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: db-init\n      labels:\n        app: db-init\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"db-init\"\n          image: \":xxxx\"\n          env:\n            - name: DB_NAME\n              value: \"\"\n            - name: DB_HOST\n              value: \"\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"\"\n            - name: DB_PASSWORD\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: STAGE\n              value: \"install\"\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/db-init.yaml"}, "region": {"startLine": 16, "endLine": 53, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: db-init\n  namespace: xxxx\n  labels:\n    app: db-init\n  annotations:\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: db-init\n      labels:\n        app: db-init\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"db-init\"\n          image: \":xxxx\"\n          env:\n            - name: DB_NAME\n              value: \"\"\n            - name: DB_HOST\n              value: \"\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"\"\n            - name: DB_PASSWORD\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: STAGE\n              value: \"install\"\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/db-init.yaml"}, "region": {"startLine": 16, "endLine": 53, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: db-init\n  namespace: xxxx\n  labels:\n    app: db-init\n  annotations:\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: db-init\n      labels:\n        app: db-init\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"db-init\"\n          image: \":xxxx\"\n          env:\n            - name: DB_NAME\n              value: \"\"\n            - name: DB_HOST\n              value: \"\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"\"\n            - name: DB_PASSWORD\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: STAGE\n              value: \"install\"\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/db-init.yaml"}, "region": {"startLine": 16, "endLine": 53, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: db-init\n  namespace: xxxx\n  labels:\n    app: db-init\n  annotations:\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: db-init\n      labels:\n        app: db-init\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"db-init\"\n          image: \":xxxx\"\n          env:\n            - name: DB_NAME\n              value: \"\"\n            - name: DB_HOST\n              value: \"\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"\"\n            - name: DB_PASSWORD\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: STAGE\n              value: \"install\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/db-init.yaml"}, "region": {"startLine": 16, "endLine": 53, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: db-init\n  namespace: xxxx\n  labels:\n    app: db-init\n  annotations:\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: db-init\n      labels:\n        app: db-init\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"db-init\"\n          image: \":xxxx\"\n          env:\n            - name: DB_NAME\n              value: \"\"\n            - name: DB_HOST\n              value: \"\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"\"\n            - name: DB_PASSWORD\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: STAGE\n              value: \"install\"\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/db-init.yaml"}, "region": {"startLine": 16, "endLine": 53, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: db-init\n  namespace: xxxx\n  labels:\n    app: db-init\n  annotations:\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: db-init\n      labels:\n        app: db-init\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"db-init\"\n          image: \":xxxx\"\n          env:\n            - name: DB_NAME\n              value: \"\"\n            - name: DB_HOST\n              value: \"\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"\"\n            - name: DB_PASSWORD\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: STAGE\n              value: \"install\"\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/db-init.yaml"}, "region": {"startLine": 16, "endLine": 53, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: db-init\n  namespace: xxxx\n  labels:\n    app: db-init\n  annotations:\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: db-init\n      labels:\n        app: db-init\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"db-init\"\n          image: \":xxxx\"\n          env:\n            - name: DB_NAME\n              value: \"\"\n            - name: DB_HOST\n              value: \"\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"\"\n            - name: DB_PASSWORD\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: STAGE\n              value: \"install\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/db-init.yaml"}, "region": {"startLine": 16, "endLine": 53, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: db-init\n  namespace: xxxx\n  labels:\n    app: db-init\n  annotations:\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: db-init\n      labels:\n        app: db-init\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"db-init\"\n          image: \":xxxx\"\n          env:\n            - name: DB_NAME\n              value: \"\"\n            - name: DB_HOST\n              value: \"\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"\"\n            - name: DB_PASSWORD\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: STAGE\n              value: \"install\"\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/db-init.yaml"}, "region": {"startLine": 16, "endLine": 53, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: db-init\n  namespace: xxxx\n  labels:\n    app: db-init\n  annotations:\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: db-init\n      labels:\n        app: db-init\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"db-init\"\n          image: \":xxxx\"\n          env:\n            - name: DB_NAME\n              value: \"\"\n            - name: DB_HOST\n              value: \"\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"\"\n            - name: DB_PASSWORD\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: STAGE\n              value: \"install\"\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/db-init.yaml"}, "region": {"startLine": 16, "endLine": 53, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: db-init\n  namespace: xxxx\n  labels:\n    app: db-init\n  annotations:\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: db-init\n      labels:\n        app: db-init\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"db-init\"\n          image: \":xxxx\"\n          env:\n            - name: DB_NAME\n              value: \"\"\n            - name: DB_HOST\n              value: \"\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"\"\n            - name: DB_PASSWORD\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: STAGE\n              value: \"install\"\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/db-init.yaml"}, "region": {"startLine": 16, "endLine": 53, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: db-init\n  namespace: xxxx\n  labels:\n    app: db-init\n  annotations:\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: db-init\n      labels:\n        app: db-init\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"db-init\"\n          image: \":xxxx\"\n          env:\n            - name: DB_NAME\n              value: \"\"\n            - name: DB_HOST\n              value: \"\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"\"\n            - name: DB_PASSWORD\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: STAGE\n              value: \"install\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/db-init.yaml"}, "region": {"startLine": 16, "endLine": 53, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: db-init\n  namespace: xxxx\n  labels:\n    app: db-init\n  annotations:\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: db-init\n      labels:\n        app: db-init\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"db-init\"\n          image: \":xxxx\"\n          env:\n            - name: DB_NAME\n              value: \"\"\n            - name: DB_HOST\n              value: \"\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"\"\n            - name: DB_PASSWORD\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: STAGE\n              value: \"install\"\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/db-init.yaml"}, "region": {"startLine": 16, "endLine": 53, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: db-init\n  namespace: xxxx\n  labels:\n    app: db-init\n  annotations:\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: db-init\n      labels:\n        app: db-init\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"db-init\"\n          image: \":xxxx\"\n          env:\n            - name: DB_NAME\n              value: \"\"\n            - name: DB_HOST\n              value: \"\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"\"\n            - name: DB_PASSWORD\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: STAGE\n              value: \"install\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/db-init.yaml"}, "region": {"startLine": 16, "endLine": 53, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: db-init\n  namespace: xxxx\n  labels:\n    app: db-init\n  annotations:\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: db-init\n      labels:\n        app: db-init\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"db-init\"\n          image: \":xxxx\"\n          env:\n            - name: DB_NAME\n              value: \"\"\n            - name: DB_HOST\n              value: \"\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"\"\n            - name: DB_PASSWORD\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: STAGE\n              value: \"install\"\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/db-init.yaml"}, "region": {"startLine": 16, "endLine": 53, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: db-init\n  namespace: xxxx\n  labels:\n    app: db-init\n  annotations:\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    metadata:\n      name: db-init\n      labels:\n        app: db-init\n    spec:\n      restartPolicy: \"Never\"\n      containers:\n        - name: \"db-init\"\n          image: \":xxxx\"\n          env:\n            - name: DB_NAME\n              value: \"\"\n            - name: DB_HOST\n              value: \"\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"\"\n            - name: DB_PASSWORD\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: STAGE\n              value: \"install\"\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/install.yaml"}, "region": {"startLine": 16, "endLine": 68, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: install\n  namespace: xxxx\n  labels:\n    app: install\nspec:\n  ttlSecondsAfterFinished: 600\n  template:\n    metadata:\n      name: install\n      labels:\n        app: install\n      annotations: \n        \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n\n\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      restartPolicy: \"Never\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: \"install\"\n          image: \n          #image: 853502460716.dkr.ecr.us-east-1.amazonaws.com/hive/dev/install:check\n          env:\n            - name: INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSH_PORTAL_SERVICE\n              value: \"ssh-portal\"\n            - name: SSH_PORTAL_PORT\n              value: \"2222\"\n            - name: HIVE_VT\n              value : \"xxxx\"\n            - name: HIVE_NS\n              value : \"xxxx\"\n            - name : MINIO_ACCESS_KEY\n              value : \"\"\n            - name : MINIO_SECRET_KEY\n              value : \"\"\n            - name : MINIO_HOST_ALIAS\n              value : \"\"\n            - name : MINIO_HOST_URL\n              value : \"\"\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/install/\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/install.yaml"}, "region": {"startLine": 16, "endLine": 68, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: install\n  namespace: xxxx\n  labels:\n    app: install\nspec:\n  ttlSecondsAfterFinished: 600\n  template:\n    metadata:\n      name: install\n      labels:\n        app: install\n      annotations: \n        \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n\n\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      restartPolicy: \"Never\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: \"install\"\n          image: \n          #image: 853502460716.dkr.ecr.us-east-1.amazonaws.com/hive/dev/install:check\n          env:\n            - name: INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSH_PORTAL_SERVICE\n              value: \"ssh-portal\"\n            - name: SSH_PORTAL_PORT\n              value: \"2222\"\n            - name: HIVE_VT\n              value : \"xxxx\"\n            - name: HIVE_NS\n              value : \"xxxx\"\n            - name : MINIO_ACCESS_KEY\n              value : \"\"\n            - name : MINIO_SECRET_KEY\n              value : \"\"\n            - name : MINIO_HOST_ALIAS\n              value : \"\"\n            - name : MINIO_HOST_URL\n              value : \"\"\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/install/\"\n"}}}}]}, {"ruleId": "CKV_K8S_33", "ruleIndex": 21, "level": "error", "attachments": [], "message": {"text": "Ensure the Kubernetes dashboard is not deployed"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/install.yaml"}, "region": {"startLine": 16, "endLine": 68, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: install\n  namespace: xxxx\n  labels:\n    app: install\nspec:\n  ttlSecondsAfterFinished: 600\n  template:\n    metadata:\n      name: install\n      labels:\n        app: install\n      annotations: \n        \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n\n\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      restartPolicy: \"Never\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: \"install\"\n          image: \n          #image: 853502460716.dkr.ecr.us-east-1.amazonaws.com/hive/dev/install:check\n          env:\n            - name: INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSH_PORTAL_SERVICE\n              value: \"ssh-portal\"\n            - name: SSH_PORTAL_PORT\n              value: \"2222\"\n            - name: HIVE_VT\n              value : \"xxxx\"\n            - name: HIVE_NS\n              value : \"xxxx\"\n            - name : MINIO_ACCESS_KEY\n              value : \"\"\n            - name : MINIO_SECRET_KEY\n              value : \"\"\n            - name : MINIO_HOST_ALIAS\n              value : \"\"\n            - name : MINIO_HOST_URL\n              value : \"\"\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/install/\"\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/install.yaml"}, "region": {"startLine": 16, "endLine": 68, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: install\n  namespace: xxxx\n  labels:\n    app: install\nspec:\n  ttlSecondsAfterFinished: 600\n  template:\n    metadata:\n      name: install\n      labels:\n        app: install\n      annotations: \n        \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n\n\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      restartPolicy: \"Never\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: \"install\"\n          image: \n          #image: 853502460716.dkr.ecr.us-east-1.amazonaws.com/hive/dev/install:check\n          env:\n            - name: INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSH_PORTAL_SERVICE\n              value: \"ssh-portal\"\n            - name: SSH_PORTAL_PORT\n              value: \"2222\"\n            - name: HIVE_VT\n              value : \"xxxx\"\n            - name: HIVE_NS\n              value : \"xxxx\"\n            - name : MINIO_ACCESS_KEY\n              value : \"\"\n            - name : MINIO_SECRET_KEY\n              value : \"\"\n            - name : MINIO_HOST_ALIAS\n              value : \"\"\n            - name : MINIO_HOST_URL\n              value : \"\"\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/install/\"\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/install.yaml"}, "region": {"startLine": 16, "endLine": 68, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: install\n  namespace: xxxx\n  labels:\n    app: install\nspec:\n  ttlSecondsAfterFinished: 600\n  template:\n    metadata:\n      name: install\n      labels:\n        app: install\n      annotations: \n        \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n\n\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      restartPolicy: \"Never\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: \"install\"\n          image: \n          #image: 853502460716.dkr.ecr.us-east-1.amazonaws.com/hive/dev/install:check\n          env:\n            - name: INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSH_PORTAL_SERVICE\n              value: \"ssh-portal\"\n            - name: SSH_PORTAL_PORT\n              value: \"2222\"\n            - name: HIVE_VT\n              value : \"xxxx\"\n            - name: HIVE_NS\n              value : \"xxxx\"\n            - name : MINIO_ACCESS_KEY\n              value : \"\"\n            - name : MINIO_SECRET_KEY\n              value : \"\"\n            - name : MINIO_HOST_ALIAS\n              value : \"\"\n            - name : MINIO_HOST_URL\n              value : \"\"\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/install/\"\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/install.yaml"}, "region": {"startLine": 16, "endLine": 68, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: install\n  namespace: xxxx\n  labels:\n    app: install\nspec:\n  ttlSecondsAfterFinished: 600\n  template:\n    metadata:\n      name: install\n      labels:\n        app: install\n      annotations: \n        \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n\n\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      restartPolicy: \"Never\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: \"install\"\n          image: \n          #image: 853502460716.dkr.ecr.us-east-1.amazonaws.com/hive/dev/install:check\n          env:\n            - name: INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSH_PORTAL_SERVICE\n              value: \"ssh-portal\"\n            - name: SSH_PORTAL_PORT\n              value: \"2222\"\n            - name: HIVE_VT\n              value : \"xxxx\"\n            - name: HIVE_NS\n              value : \"xxxx\"\n            - name : MINIO_ACCESS_KEY\n              value : \"\"\n            - name : MINIO_SECRET_KEY\n              value : \"\"\n            - name : MINIO_HOST_ALIAS\n              value : \"\"\n            - name : MINIO_HOST_URL\n              value : \"\"\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/install/\"\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/install.yaml"}, "region": {"startLine": 16, "endLine": 68, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: install\n  namespace: xxxx\n  labels:\n    app: install\nspec:\n  ttlSecondsAfterFinished: 600\n  template:\n    metadata:\n      name: install\n      labels:\n        app: install\n      annotations: \n        \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n\n\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      restartPolicy: \"Never\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: \"install\"\n          image: \n          #image: 853502460716.dkr.ecr.us-east-1.amazonaws.com/hive/dev/install:check\n          env:\n            - name: INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSH_PORTAL_SERVICE\n              value: \"ssh-portal\"\n            - name: SSH_PORTAL_PORT\n              value: \"2222\"\n            - name: HIVE_VT\n              value : \"xxxx\"\n            - name: HIVE_NS\n              value : \"xxxx\"\n            - name : MINIO_ACCESS_KEY\n              value : \"\"\n            - name : MINIO_SECRET_KEY\n              value : \"\"\n            - name : MINIO_HOST_ALIAS\n              value : \"\"\n            - name : MINIO_HOST_URL\n              value : \"\"\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/install/\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/install.yaml"}, "region": {"startLine": 16, "endLine": 68, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: install\n  namespace: xxxx\n  labels:\n    app: install\nspec:\n  ttlSecondsAfterFinished: 600\n  template:\n    metadata:\n      name: install\n      labels:\n        app: install\n      annotations: \n        \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n\n\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      restartPolicy: \"Never\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: \"install\"\n          image: \n          #image: 853502460716.dkr.ecr.us-east-1.amazonaws.com/hive/dev/install:check\n          env:\n            - name: INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSH_PORTAL_SERVICE\n              value: \"ssh-portal\"\n            - name: SSH_PORTAL_PORT\n              value: \"2222\"\n            - name: HIVE_VT\n              value : \"xxxx\"\n            - name: HIVE_NS\n              value : \"xxxx\"\n            - name : MINIO_ACCESS_KEY\n              value : \"\"\n            - name : MINIO_SECRET_KEY\n              value : \"\"\n            - name : MINIO_HOST_ALIAS\n              value : \"\"\n            - name : MINIO_HOST_URL\n              value : \"\"\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/install/\"\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/install.yaml"}, "region": {"startLine": 16, "endLine": 68, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: install\n  namespace: xxxx\n  labels:\n    app: install\nspec:\n  ttlSecondsAfterFinished: 600\n  template:\n    metadata:\n      name: install\n      labels:\n        app: install\n      annotations: \n        \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n\n\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      restartPolicy: \"Never\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: \"install\"\n          image: \n          #image: 853502460716.dkr.ecr.us-east-1.amazonaws.com/hive/dev/install:check\n          env:\n            - name: INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSH_PORTAL_SERVICE\n              value: \"ssh-portal\"\n            - name: SSH_PORTAL_PORT\n              value: \"2222\"\n            - name: HIVE_VT\n              value : \"xxxx\"\n            - name: HIVE_NS\n              value : \"xxxx\"\n            - name : MINIO_ACCESS_KEY\n              value : \"\"\n            - name : MINIO_SECRET_KEY\n              value : \"\"\n            - name : MINIO_HOST_ALIAS\n              value : \"\"\n            - name : MINIO_HOST_URL\n              value : \"\"\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/install/\"\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/install.yaml"}, "region": {"startLine": 16, "endLine": 68, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: install\n  namespace: xxxx\n  labels:\n    app: install\nspec:\n  ttlSecondsAfterFinished: 600\n  template:\n    metadata:\n      name: install\n      labels:\n        app: install\n      annotations: \n        \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n\n\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      restartPolicy: \"Never\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: \"install\"\n          image: \n          #image: 853502460716.dkr.ecr.us-east-1.amazonaws.com/hive/dev/install:check\n          env:\n            - name: INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSH_PORTAL_SERVICE\n              value: \"ssh-portal\"\n            - name: SSH_PORTAL_PORT\n              value: \"2222\"\n            - name: HIVE_VT\n              value : \"xxxx\"\n            - name: HIVE_NS\n              value : \"xxxx\"\n            - name : MINIO_ACCESS_KEY\n              value : \"\"\n            - name : MINIO_SECRET_KEY\n              value : \"\"\n            - name : MINIO_HOST_ALIAS\n              value : \"\"\n            - name : MINIO_HOST_URL\n              value : \"\"\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/install/\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/install.yaml"}, "region": {"startLine": 16, "endLine": 68, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: install\n  namespace: xxxx\n  labels:\n    app: install\nspec:\n  ttlSecondsAfterFinished: 600\n  template:\n    metadata:\n      name: install\n      labels:\n        app: install\n      annotations: \n        \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n\n\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      restartPolicy: \"Never\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: \"install\"\n          image: \n          #image: 853502460716.dkr.ecr.us-east-1.amazonaws.com/hive/dev/install:check\n          env:\n            - name: INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSH_PORTAL_SERVICE\n              value: \"ssh-portal\"\n            - name: SSH_PORTAL_PORT\n              value: \"2222\"\n            - name: HIVE_VT\n              value : \"xxxx\"\n            - name: HIVE_NS\n              value : \"xxxx\"\n            - name : MINIO_ACCESS_KEY\n              value : \"\"\n            - name : MINIO_SECRET_KEY\n              value : \"\"\n            - name : MINIO_HOST_ALIAS\n              value : \"\"\n            - name : MINIO_HOST_URL\n              value : \"\"\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/install/\"\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/install.yaml"}, "region": {"startLine": 16, "endLine": 68, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: install\n  namespace: xxxx\n  labels:\n    app: install\nspec:\n  ttlSecondsAfterFinished: 600\n  template:\n    metadata:\n      name: install\n      labels:\n        app: install\n      annotations: \n        \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n\n\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      restartPolicy: \"Never\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: \"install\"\n          image: \n          #image: 853502460716.dkr.ecr.us-east-1.amazonaws.com/hive/dev/install:check\n          env:\n            - name: INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSH_PORTAL_SERVICE\n              value: \"ssh-portal\"\n            - name: SSH_PORTAL_PORT\n              value: \"2222\"\n            - name: HIVE_VT\n              value : \"xxxx\"\n            - name: HIVE_NS\n              value : \"xxxx\"\n            - name : MINIO_ACCESS_KEY\n              value : \"\"\n            - name : MINIO_SECRET_KEY\n              value : \"\"\n            - name : MINIO_HOST_ALIAS\n              value : \"\"\n            - name : MINIO_HOST_URL\n              value : \"\"\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/install/\"\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/install.yaml"}, "region": {"startLine": 16, "endLine": 68, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: install\n  namespace: xxxx\n  labels:\n    app: install\nspec:\n  ttlSecondsAfterFinished: 600\n  template:\n    metadata:\n      name: install\n      labels:\n        app: install\n      annotations: \n        \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n\n\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      restartPolicy: \"Never\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: \"install\"\n          image: \n          #image: 853502460716.dkr.ecr.us-east-1.amazonaws.com/hive/dev/install:check\n          env:\n            - name: INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSH_PORTAL_SERVICE\n              value: \"ssh-portal\"\n            - name: SSH_PORTAL_PORT\n              value: \"2222\"\n            - name: HIVE_VT\n              value : \"xxxx\"\n            - name: HIVE_NS\n              value : \"xxxx\"\n            - name : MINIO_ACCESS_KEY\n              value : \"\"\n            - name : MINIO_SECRET_KEY\n              value : \"\"\n            - name : MINIO_HOST_ALIAS\n              value : \"\"\n            - name : MINIO_HOST_URL\n              value : \"\"\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/install/\"\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/install.yaml"}, "region": {"startLine": 16, "endLine": 68, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: install\n  namespace: xxxx\n  labels:\n    app: install\nspec:\n  ttlSecondsAfterFinished: 600\n  template:\n    metadata:\n      name: install\n      labels:\n        app: install\n      annotations: \n        \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n\n\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      restartPolicy: \"Never\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: \"install\"\n          image: \n          #image: 853502460716.dkr.ecr.us-east-1.amazonaws.com/hive/dev/install:check\n          env:\n            - name: INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSH_PORTAL_SERVICE\n              value: \"ssh-portal\"\n            - name: SSH_PORTAL_PORT\n              value: \"2222\"\n            - name: HIVE_VT\n              value : \"xxxx\"\n            - name: HIVE_NS\n              value : \"xxxx\"\n            - name : MINIO_ACCESS_KEY\n              value : \"\"\n            - name : MINIO_SECRET_KEY\n              value : \"\"\n            - name : MINIO_HOST_ALIAS\n              value : \"\"\n            - name : MINIO_HOST_URL\n              value : \"\"\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/install/\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/install.yaml"}, "region": {"startLine": 16, "endLine": 68, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: install\n  namespace: xxxx\n  labels:\n    app: install\nspec:\n  ttlSecondsAfterFinished: 600\n  template:\n    metadata:\n      name: install\n      labels:\n        app: install\n      annotations: \n        \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n\n\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      restartPolicy: \"Never\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: \"install\"\n          image: \n          #image: 853502460716.dkr.ecr.us-east-1.amazonaws.com/hive/dev/install:check\n          env:\n            - name: INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSH_PORTAL_SERVICE\n              value: \"ssh-portal\"\n            - name: SSH_PORTAL_PORT\n              value: \"2222\"\n            - name: HIVE_VT\n              value : \"xxxx\"\n            - name: HIVE_NS\n              value : \"xxxx\"\n            - name : MINIO_ACCESS_KEY\n              value : \"\"\n            - name : MINIO_SECRET_KEY\n              value : \"\"\n            - name : MINIO_HOST_ALIAS\n              value : \"\"\n            - name : MINIO_HOST_URL\n              value : \"\"\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/install/\"\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/install.yaml"}, "region": {"startLine": 16, "endLine": 68, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: install\n  namespace: xxxx\n  labels:\n    app: install\nspec:\n  ttlSecondsAfterFinished: 600\n  template:\n    metadata:\n      name: install\n      labels:\n        app: install\n      annotations: \n        \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n\n\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      restartPolicy: \"Never\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: \"install\"\n          image: \n          #image: 853502460716.dkr.ecr.us-east-1.amazonaws.com/hive/dev/install:check\n          env:\n            - name: INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSH_PORTAL_SERVICE\n              value: \"ssh-portal\"\n            - name: SSH_PORTAL_PORT\n              value: \"2222\"\n            - name: HIVE_VT\n              value : \"xxxx\"\n            - name: HIVE_NS\n              value : \"xxxx\"\n            - name : MINIO_ACCESS_KEY\n              value : \"\"\n            - name : MINIO_SECRET_KEY\n              value : \"\"\n            - name : MINIO_HOST_ALIAS\n              value : \"\"\n            - name : MINIO_HOST_URL\n              value : \"\"\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/install/\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/install.yaml"}, "region": {"startLine": 16, "endLine": 68, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: install\n  namespace: xxxx\n  labels:\n    app: install\nspec:\n  ttlSecondsAfterFinished: 600\n  template:\n    metadata:\n      name: install\n      labels:\n        app: install\n      annotations: \n        \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n\n\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      restartPolicy: \"Never\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: \"install\"\n          image: \n          #image: 853502460716.dkr.ecr.us-east-1.amazonaws.com/hive/dev/install:check\n          env:\n            - name: INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSH_PORTAL_SERVICE\n              value: \"ssh-portal\"\n            - name: SSH_PORTAL_PORT\n              value: \"2222\"\n            - name: HIVE_VT\n              value : \"xxxx\"\n            - name: HIVE_NS\n              value : \"xxxx\"\n            - name : MINIO_ACCESS_KEY\n              value : \"\"\n            - name : MINIO_SECRET_KEY\n              value : \"\"\n            - name : MINIO_HOST_ALIAS\n              value : \"\"\n            - name : MINIO_HOST_URL\n              value : \"\"\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/install/\"\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/jobs/install.yaml"}, "region": {"startLine": 16, "endLine": 68, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: install\n  namespace: xxxx\n  labels:\n    app: install\nspec:\n  ttlSecondsAfterFinished: 600\n  template:\n    metadata:\n      name: install\n      labels:\n        app: install\n      annotations: \n        \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n\n\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      restartPolicy: \"Never\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: \"install\"\n          image: \n          #image: 853502460716.dkr.ecr.us-east-1.amazonaws.com/hive/dev/install:check\n          env:\n            - name: INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSH_PORTAL_SERVICE\n              value: \"ssh-portal\"\n            - name: SSH_PORTAL_PORT\n              value: \"2222\"\n            - name: HIVE_VT\n              value : \"xxxx\"\n            - name: HIVE_NS\n              value : \"xxxx\"\n            - name : MINIO_ACCESS_KEY\n              value : \"\"\n            - name : MINIO_SECRET_KEY\n              value : \"\"\n            - name : MINIO_HOST_ALIAS\n              value : \"\"\n            - name : MINIO_HOST_URL\n              value : \"\"\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/install/\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/services/public-api.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "kind: Service\napiVersion: v1\nmetadata:\n  name: public-api\n  namespace: default\n  labels:\n    app: public-api\n  annotations:\nspec:\n  ports:\n    - name: http\n      port: 3000\n      targetPort: 3000\n    - name: https\n      port: 3001\n      targetPort: 3001\n  selector:\n    app: public-api\n  type: ClusterIP\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/services/api-gateway.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "kind: Service\napiVersion: v1\nmetadata:\n  name: api-gateway\n  namespace: default\n  labels:\n    app: api-gateway\n  annotations:\nspec:\n  ports:\n    - name: http-gateway\n      port: 3000\n      targetPort: 3000\n    - name: https-gateway\n      port: 3001\n      targetPort: 3001\n  selector:\n    app: api-gateway\n  type: ClusterIP\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/services/postgres-service.yaml"}, "region": {"startLine": 3, "endLine": 18, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  creationTimestamp: null\n  labels:\n    service: postgres\n  name: postgres\nspec:\n  ports:\n    - name: tcp-5432\n      port: 5432\n      targetPort: 5432\n  selector:\n    service: postgres\nstatus:\n  loadBalancer: {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/services/ui.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "kind: Service\napiVersion: v1\nmetadata:\n  name: ui\n  namespace: default\n  labels:\n    app: ui\n  annotations:\nspec:\n  ports:\n    - name: http-ui\n      port: 80\n      targetPort: 8080\n    - name: https-ui\n      port: 443\n      targetPort: 443\n  selector:\n    app: ui\n  type: ClusterIP\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/services/ssh-portal.yaml"}, "region": {"startLine": 3, "endLine": 18, "snippet": {"text": "kind: Service\napiVersion: v1\nmetadata:\n  name: ssh-portal\n  namespace: default\n  labels:\n    app: ssh-portal\n  annotations:\nspec:\n  ports:\n    - name: tcp-ssh\n      port: 2222\n      targetPort: 2222\n  selector:\n    app: ssh-portal\n  type: ClusterIP\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/public-api.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: public-api\n  labels:\n    app: public-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: public-api\n  template:\n    metadata:\n      name: public-api\n      labels:\n        app: public-api\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: public-api\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/public-api/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/public-api.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: public-api\n  labels:\n    app: public-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: public-api\n  template:\n    metadata:\n      name: public-api\n      labels:\n        app: public-api\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: public-api\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/public-api/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/public-api.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: public-api\n  labels:\n    app: public-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: public-api\n  template:\n    metadata:\n      name: public-api\n      labels:\n        app: public-api\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: public-api\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/public-api/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_33", "ruleIndex": 21, "level": "error", "attachments": [], "message": {"text": "Ensure the Kubernetes dashboard is not deployed"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/public-api.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: public-api\n  labels:\n    app: public-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: public-api\n  template:\n    metadata:\n      name: public-api\n      labels:\n        app: public-api\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: public-api\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/public-api/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/public-api.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: public-api\n  labels:\n    app: public-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: public-api\n  template:\n    metadata:\n      name: public-api\n      labels:\n        app: public-api\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: public-api\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/public-api/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/public-api.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: public-api\n  labels:\n    app: public-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: public-api\n  template:\n    metadata:\n      name: public-api\n      labels:\n        app: public-api\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: public-api\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/public-api/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/public-api.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: public-api\n  labels:\n    app: public-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: public-api\n  template:\n    metadata:\n      name: public-api\n      labels:\n        app: public-api\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: public-api\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/public-api/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/public-api.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: public-api\n  labels:\n    app: public-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: public-api\n  template:\n    metadata:\n      name: public-api\n      labels:\n        app: public-api\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: public-api\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/public-api/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/public-api.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: public-api\n  labels:\n    app: public-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: public-api\n  template:\n    metadata:\n      name: public-api\n      labels:\n        app: public-api\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: public-api\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/public-api/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/public-api.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: public-api\n  labels:\n    app: public-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: public-api\n  template:\n    metadata:\n      name: public-api\n      labels:\n        app: public-api\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: public-api\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/public-api/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/public-api.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: public-api\n  labels:\n    app: public-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: public-api\n  template:\n    metadata:\n      name: public-api\n      labels:\n        app: public-api\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: public-api\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/public-api/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/public-api.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: public-api\n  labels:\n    app: public-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: public-api\n  template:\n    metadata:\n      name: public-api\n      labels:\n        app: public-api\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: public-api\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/public-api/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/public-api.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: public-api\n  labels:\n    app: public-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: public-api\n  template:\n    metadata:\n      name: public-api\n      labels:\n        app: public-api\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: public-api\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/public-api/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/public-api.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: public-api\n  labels:\n    app: public-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: public-api\n  template:\n    metadata:\n      name: public-api\n      labels:\n        app: public-api\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: public-api\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/public-api/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/public-api.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: public-api\n  labels:\n    app: public-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: public-api\n  template:\n    metadata:\n      name: public-api\n      labels:\n        app: public-api\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: public-api\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/public-api/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/public-api.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: public-api\n  labels:\n    app: public-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: public-api\n  template:\n    metadata:\n      name: public-api\n      labels:\n        app: public-api\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: public-api\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/public-api/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/public-api.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: public-api\n  labels:\n    app: public-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: public-api\n  template:\n    metadata:\n      name: public-api\n      labels:\n        app: public-api\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: public-api\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/public-api/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/public-api.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: public-api\n  labels:\n    app: public-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: public-api\n  template:\n    metadata:\n      name: public-api\n      labels:\n        app: public-api\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: public-api\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/public-api/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/public-api.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: public-api\n  labels:\n    app: public-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: public-api\n  template:\n    metadata:\n      name: public-api\n      labels:\n        app: public-api\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: public-api\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/public-api/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/public-api.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: public-api\n  labels:\n    app: public-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: public-api\n  template:\n    metadata:\n      name: public-api\n      labels:\n        app: public-api\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: public-api\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/public-api/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/public-api.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: public-api\n  labels:\n    app: public-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: public-api\n  template:\n    metadata:\n      name: public-api\n      labels:\n        app: public-api\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: public-api\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/public-api/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-manager.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-manager\n  labels:\n    app: target-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-manager\n  template:\n    metadata:\n      name: target-manager\n      labels:\n        app: target-manager\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-manager\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-manager/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-manager.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-manager\n  labels:\n    app: target-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-manager\n  template:\n    metadata:\n      name: target-manager\n      labels:\n        app: target-manager\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-manager\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-manager/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-manager.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-manager\n  labels:\n    app: target-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-manager\n  template:\n    metadata:\n      name: target-manager\n      labels:\n        app: target-manager\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-manager\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-manager/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_33", "ruleIndex": 21, "level": "error", "attachments": [], "message": {"text": "Ensure the Kubernetes dashboard is not deployed"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-manager.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-manager\n  labels:\n    app: target-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-manager\n  template:\n    metadata:\n      name: target-manager\n      labels:\n        app: target-manager\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-manager\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-manager/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-manager.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-manager\n  labels:\n    app: target-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-manager\n  template:\n    metadata:\n      name: target-manager\n      labels:\n        app: target-manager\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-manager\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-manager/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-manager.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-manager\n  labels:\n    app: target-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-manager\n  template:\n    metadata:\n      name: target-manager\n      labels:\n        app: target-manager\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-manager\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-manager/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-manager.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-manager\n  labels:\n    app: target-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-manager\n  template:\n    metadata:\n      name: target-manager\n      labels:\n        app: target-manager\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-manager\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-manager/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-manager.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-manager\n  labels:\n    app: target-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-manager\n  template:\n    metadata:\n      name: target-manager\n      labels:\n        app: target-manager\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-manager\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-manager/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-manager.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-manager\n  labels:\n    app: target-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-manager\n  template:\n    metadata:\n      name: target-manager\n      labels:\n        app: target-manager\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-manager\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-manager/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-manager.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-manager\n  labels:\n    app: target-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-manager\n  template:\n    metadata:\n      name: target-manager\n      labels:\n        app: target-manager\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-manager\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-manager/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-manager.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-manager\n  labels:\n    app: target-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-manager\n  template:\n    metadata:\n      name: target-manager\n      labels:\n        app: target-manager\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-manager\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-manager/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-manager.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-manager\n  labels:\n    app: target-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-manager\n  template:\n    metadata:\n      name: target-manager\n      labels:\n        app: target-manager\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-manager\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-manager/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-manager.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-manager\n  labels:\n    app: target-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-manager\n  template:\n    metadata:\n      name: target-manager\n      labels:\n        app: target-manager\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-manager\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-manager/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-manager.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-manager\n  labels:\n    app: target-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-manager\n  template:\n    metadata:\n      name: target-manager\n      labels:\n        app: target-manager\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-manager\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-manager/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-manager.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-manager\n  labels:\n    app: target-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-manager\n  template:\n    metadata:\n      name: target-manager\n      labels:\n        app: target-manager\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-manager\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-manager/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-manager.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-manager\n  labels:\n    app: target-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-manager\n  template:\n    metadata:\n      name: target-manager\n      labels:\n        app: target-manager\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-manager\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-manager/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-manager.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-manager\n  labels:\n    app: target-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-manager\n  template:\n    metadata:\n      name: target-manager\n      labels:\n        app: target-manager\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-manager\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-manager/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-manager.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-manager\n  labels:\n    app: target-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-manager\n  template:\n    metadata:\n      name: target-manager\n      labels:\n        app: target-manager\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-manager\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-manager/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-manager.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-manager\n  labels:\n    app: target-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-manager\n  template:\n    metadata:\n      name: target-manager\n      labels:\n        app: target-manager\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-manager\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-manager/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-manager.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-manager\n  labels:\n    app: target-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-manager\n  template:\n    metadata:\n      name: target-manager\n      labels:\n        app: target-manager\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-manager\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-manager/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-manager.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-manager\n  labels:\n    app: target-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-manager\n  template:\n    metadata:\n      name: target-manager\n      labels:\n        app: target-manager\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-manager\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-manager/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/api-gateway.yaml"}, "region": {"startLine": 4, "endLine": 50, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: api-gateway\n  namespace: default\n  labels:\n    app: api-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      name: api-gateway\n      labels:\n        app: api-gateway\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: api-gateway\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/api-gateway/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/api-gateway.yaml"}, "region": {"startLine": 4, "endLine": 50, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: api-gateway\n  namespace: default\n  labels:\n    app: api-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      name: api-gateway\n      labels:\n        app: api-gateway\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: api-gateway\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/api-gateway/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/api-gateway.yaml"}, "region": {"startLine": 4, "endLine": 50, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: api-gateway\n  namespace: default\n  labels:\n    app: api-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      name: api-gateway\n      labels:\n        app: api-gateway\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: api-gateway\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/api-gateway/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_33", "ruleIndex": 21, "level": "error", "attachments": [], "message": {"text": "Ensure the Kubernetes dashboard is not deployed"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/api-gateway.yaml"}, "region": {"startLine": 4, "endLine": 50, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: api-gateway\n  namespace: default\n  labels:\n    app: api-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      name: api-gateway\n      labels:\n        app: api-gateway\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: api-gateway\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/api-gateway/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/api-gateway.yaml"}, "region": {"startLine": 4, "endLine": 50, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: api-gateway\n  namespace: default\n  labels:\n    app: api-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      name: api-gateway\n      labels:\n        app: api-gateway\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: api-gateway\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/api-gateway/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/api-gateway.yaml"}, "region": {"startLine": 4, "endLine": 50, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: api-gateway\n  namespace: default\n  labels:\n    app: api-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      name: api-gateway\n      labels:\n        app: api-gateway\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: api-gateway\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/api-gateway/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/api-gateway.yaml"}, "region": {"startLine": 4, "endLine": 50, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: api-gateway\n  namespace: default\n  labels:\n    app: api-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      name: api-gateway\n      labels:\n        app: api-gateway\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: api-gateway\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/api-gateway/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/api-gateway.yaml"}, "region": {"startLine": 4, "endLine": 50, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: api-gateway\n  namespace: default\n  labels:\n    app: api-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      name: api-gateway\n      labels:\n        app: api-gateway\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: api-gateway\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/api-gateway/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/api-gateway.yaml"}, "region": {"startLine": 4, "endLine": 50, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: api-gateway\n  namespace: default\n  labels:\n    app: api-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      name: api-gateway\n      labels:\n        app: api-gateway\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: api-gateway\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/api-gateway/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/api-gateway.yaml"}, "region": {"startLine": 4, "endLine": 50, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: api-gateway\n  namespace: default\n  labels:\n    app: api-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      name: api-gateway\n      labels:\n        app: api-gateway\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: api-gateway\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/api-gateway/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/api-gateway.yaml"}, "region": {"startLine": 4, "endLine": 50, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: api-gateway\n  namespace: default\n  labels:\n    app: api-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      name: api-gateway\n      labels:\n        app: api-gateway\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: api-gateway\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/api-gateway/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/api-gateway.yaml"}, "region": {"startLine": 4, "endLine": 50, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: api-gateway\n  namespace: default\n  labels:\n    app: api-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      name: api-gateway\n      labels:\n        app: api-gateway\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: api-gateway\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/api-gateway/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/api-gateway.yaml"}, "region": {"startLine": 4, "endLine": 50, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: api-gateway\n  namespace: default\n  labels:\n    app: api-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      name: api-gateway\n      labels:\n        app: api-gateway\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: api-gateway\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/api-gateway/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/api-gateway.yaml"}, "region": {"startLine": 4, "endLine": 50, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: api-gateway\n  namespace: default\n  labels:\n    app: api-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      name: api-gateway\n      labels:\n        app: api-gateway\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: api-gateway\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/api-gateway/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/api-gateway.yaml"}, "region": {"startLine": 4, "endLine": 50, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: api-gateway\n  namespace: default\n  labels:\n    app: api-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      name: api-gateway\n      labels:\n        app: api-gateway\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: api-gateway\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/api-gateway/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/api-gateway.yaml"}, "region": {"startLine": 4, "endLine": 50, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: api-gateway\n  namespace: default\n  labels:\n    app: api-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      name: api-gateway\n      labels:\n        app: api-gateway\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: api-gateway\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/api-gateway/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/api-gateway.yaml"}, "region": {"startLine": 4, "endLine": 50, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: api-gateway\n  namespace: default\n  labels:\n    app: api-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      name: api-gateway\n      labels:\n        app: api-gateway\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: api-gateway\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/api-gateway/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/api-gateway.yaml"}, "region": {"startLine": 4, "endLine": 50, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: api-gateway\n  namespace: default\n  labels:\n    app: api-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      name: api-gateway\n      labels:\n        app: api-gateway\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: api-gateway\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/api-gateway/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/api-gateway.yaml"}, "region": {"startLine": 4, "endLine": 50, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: api-gateway\n  namespace: default\n  labels:\n    app: api-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      name: api-gateway\n      labels:\n        app: api-gateway\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: api-gateway\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/api-gateway/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/api-gateway.yaml"}, "region": {"startLine": 4, "endLine": 50, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: api-gateway\n  namespace: default\n  labels:\n    app: api-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      name: api-gateway\n      labels:\n        app: api-gateway\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: api-gateway\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/api-gateway/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/api-gateway.yaml"}, "region": {"startLine": 4, "endLine": 50, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: api-gateway\n  namespace: default\n  labels:\n    app: api-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: api-gateway\n  template:\n    metadata:\n      name: api-gateway\n      labels:\n        app: api-gateway\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: api-gateway\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value :  dockerEnvironment\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/api-gateway/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-control.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-control\n  labels:\n    app: target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-control\n  template:\n    metadata:\n      name: target-control\n      labels:\n        app: target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-control/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-control.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-control\n  labels:\n    app: target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-control\n  template:\n    metadata:\n      name: target-control\n      labels:\n        app: target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-control/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-control.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-control\n  labels:\n    app: target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-control\n  template:\n    metadata:\n      name: target-control\n      labels:\n        app: target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-control/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_33", "ruleIndex": 21, "level": "error", "attachments": [], "message": {"text": "Ensure the Kubernetes dashboard is not deployed"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-control.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-control\n  labels:\n    app: target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-control\n  template:\n    metadata:\n      name: target-control\n      labels:\n        app: target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-control/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-control.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-control\n  labels:\n    app: target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-control\n  template:\n    metadata:\n      name: target-control\n      labels:\n        app: target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-control/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-control.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-control\n  labels:\n    app: target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-control\n  template:\n    metadata:\n      name: target-control\n      labels:\n        app: target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-control/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-control.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-control\n  labels:\n    app: target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-control\n  template:\n    metadata:\n      name: target-control\n      labels:\n        app: target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-control/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-control.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-control\n  labels:\n    app: target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-control\n  template:\n    metadata:\n      name: target-control\n      labels:\n        app: target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-control/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-control.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-control\n  labels:\n    app: target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-control\n  template:\n    metadata:\n      name: target-control\n      labels:\n        app: target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-control/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-control.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-control\n  labels:\n    app: target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-control\n  template:\n    metadata:\n      name: target-control\n      labels:\n        app: target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-control/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-control.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-control\n  labels:\n    app: target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-control\n  template:\n    metadata:\n      name: target-control\n      labels:\n        app: target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-control/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-control.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-control\n  labels:\n    app: target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-control\n  template:\n    metadata:\n      name: target-control\n      labels:\n        app: target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-control/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-control.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-control\n  labels:\n    app: target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-control\n  template:\n    metadata:\n      name: target-control\n      labels:\n        app: target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-control/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-control.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-control\n  labels:\n    app: target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-control\n  template:\n    metadata:\n      name: target-control\n      labels:\n        app: target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-control/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-control.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-control\n  labels:\n    app: target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-control\n  template:\n    metadata:\n      name: target-control\n      labels:\n        app: target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-control/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-control.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-control\n  labels:\n    app: target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-control\n  template:\n    metadata:\n      name: target-control\n      labels:\n        app: target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-control/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-control.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-control\n  labels:\n    app: target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-control\n  template:\n    metadata:\n      name: target-control\n      labels:\n        app: target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-control/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-control.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-control\n  labels:\n    app: target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-control\n  template:\n    metadata:\n      name: target-control\n      labels:\n        app: target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-control/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-control.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-control\n  labels:\n    app: target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-control\n  template:\n    metadata:\n      name: target-control\n      labels:\n        app: target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-control/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-control.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-control\n  labels:\n    app: target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-control\n  template:\n    metadata:\n      name: target-control\n      labels:\n        app: target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-control/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/target-control.yaml"}, "region": {"startLine": 3, "endLine": 44, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: target-control\n  labels:\n    app: target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: target-control\n  template:\n    metadata:\n      name: target-control\n      labels:\n        app: target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/target-control/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/hive-cli-tool.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: hive-cli-tool\n  labels:\n    app: hive-cli-tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hive-cli-tool\n  template:\n    metadata:\n      name: hive-cli-tool\n      labels:\n        app: hive-cli-tool\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: hive-cli-tool\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/hive-cli-tool/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/hive-cli-tool.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: hive-cli-tool\n  labels:\n    app: hive-cli-tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hive-cli-tool\n  template:\n    metadata:\n      name: hive-cli-tool\n      labels:\n        app: hive-cli-tool\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: hive-cli-tool\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/hive-cli-tool/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/hive-cli-tool.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: hive-cli-tool\n  labels:\n    app: hive-cli-tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hive-cli-tool\n  template:\n    metadata:\n      name: hive-cli-tool\n      labels:\n        app: hive-cli-tool\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: hive-cli-tool\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/hive-cli-tool/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_33", "ruleIndex": 21, "level": "error", "attachments": [], "message": {"text": "Ensure the Kubernetes dashboard is not deployed"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/hive-cli-tool.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: hive-cli-tool\n  labels:\n    app: hive-cli-tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hive-cli-tool\n  template:\n    metadata:\n      name: hive-cli-tool\n      labels:\n        app: hive-cli-tool\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: hive-cli-tool\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/hive-cli-tool/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/hive-cli-tool.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: hive-cli-tool\n  labels:\n    app: hive-cli-tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hive-cli-tool\n  template:\n    metadata:\n      name: hive-cli-tool\n      labels:\n        app: hive-cli-tool\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: hive-cli-tool\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/hive-cli-tool/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/hive-cli-tool.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: hive-cli-tool\n  labels:\n    app: hive-cli-tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hive-cli-tool\n  template:\n    metadata:\n      name: hive-cli-tool\n      labels:\n        app: hive-cli-tool\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: hive-cli-tool\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/hive-cli-tool/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/hive-cli-tool.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: hive-cli-tool\n  labels:\n    app: hive-cli-tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hive-cli-tool\n  template:\n    metadata:\n      name: hive-cli-tool\n      labels:\n        app: hive-cli-tool\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: hive-cli-tool\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/hive-cli-tool/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/hive-cli-tool.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: hive-cli-tool\n  labels:\n    app: hive-cli-tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hive-cli-tool\n  template:\n    metadata:\n      name: hive-cli-tool\n      labels:\n        app: hive-cli-tool\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: hive-cli-tool\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/hive-cli-tool/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/hive-cli-tool.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: hive-cli-tool\n  labels:\n    app: hive-cli-tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hive-cli-tool\n  template:\n    metadata:\n      name: hive-cli-tool\n      labels:\n        app: hive-cli-tool\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: hive-cli-tool\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/hive-cli-tool/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/hive-cli-tool.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: hive-cli-tool\n  labels:\n    app: hive-cli-tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hive-cli-tool\n  template:\n    metadata:\n      name: hive-cli-tool\n      labels:\n        app: hive-cli-tool\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: hive-cli-tool\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/hive-cli-tool/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/hive-cli-tool.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: hive-cli-tool\n  labels:\n    app: hive-cli-tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hive-cli-tool\n  template:\n    metadata:\n      name: hive-cli-tool\n      labels:\n        app: hive-cli-tool\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: hive-cli-tool\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/hive-cli-tool/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/hive-cli-tool.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: hive-cli-tool\n  labels:\n    app: hive-cli-tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hive-cli-tool\n  template:\n    metadata:\n      name: hive-cli-tool\n      labels:\n        app: hive-cli-tool\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: hive-cli-tool\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/hive-cli-tool/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/hive-cli-tool.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: hive-cli-tool\n  labels:\n    app: hive-cli-tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hive-cli-tool\n  template:\n    metadata:\n      name: hive-cli-tool\n      labels:\n        app: hive-cli-tool\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: hive-cli-tool\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/hive-cli-tool/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/hive-cli-tool.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: hive-cli-tool\n  labels:\n    app: hive-cli-tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hive-cli-tool\n  template:\n    metadata:\n      name: hive-cli-tool\n      labels:\n        app: hive-cli-tool\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: hive-cli-tool\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/hive-cli-tool/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/hive-cli-tool.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: hive-cli-tool\n  labels:\n    app: hive-cli-tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hive-cli-tool\n  template:\n    metadata:\n      name: hive-cli-tool\n      labels:\n        app: hive-cli-tool\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: hive-cli-tool\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/hive-cli-tool/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/hive-cli-tool.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: hive-cli-tool\n  labels:\n    app: hive-cli-tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hive-cli-tool\n  template:\n    metadata:\n      name: hive-cli-tool\n      labels:\n        app: hive-cli-tool\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: hive-cli-tool\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/hive-cli-tool/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/hive-cli-tool.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: hive-cli-tool\n  labels:\n    app: hive-cli-tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hive-cli-tool\n  template:\n    metadata:\n      name: hive-cli-tool\n      labels:\n        app: hive-cli-tool\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: hive-cli-tool\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/hive-cli-tool/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/hive-cli-tool.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: hive-cli-tool\n  labels:\n    app: hive-cli-tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hive-cli-tool\n  template:\n    metadata:\n      name: hive-cli-tool\n      labels:\n        app: hive-cli-tool\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: hive-cli-tool\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/hive-cli-tool/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/hive-cli-tool.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: hive-cli-tool\n  labels:\n    app: hive-cli-tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hive-cli-tool\n  template:\n    metadata:\n      name: hive-cli-tool\n      labels:\n        app: hive-cli-tool\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: hive-cli-tool\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/hive-cli-tool/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/hive-cli-tool.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: hive-cli-tool\n  labels:\n    app: hive-cli-tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hive-cli-tool\n  template:\n    metadata:\n      name: hive-cli-tool\n      labels:\n        app: hive-cli-tool\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: hive-cli-tool\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/hive-cli-tool/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/hive-cli-tool.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: hive-cli-tool\n  labels:\n    app: hive-cli-tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hive-cli-tool\n  template:\n    metadata:\n      name: hive-cli-tool\n      labels:\n        app: hive-cli-tool\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: hive-cli-tool\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/hive-cli-tool/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      containers:\n        - name: ui\n          image: xxx\n          env:\n            - name: DOCKER_IMAGE\n              value: xxx\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: TOZID_REALM_NAME\n              value :  wrai\n            - name: TOZID_CLIENT_ID\n              value :  wr-studio-sso\n            - name: TOZID_HOST_NAME\n              value :                 \n          resources:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      containers:\n        - name: ui\n          image: xxx\n          env:\n            - name: DOCKER_IMAGE\n              value: xxx\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: TOZID_REALM_NAME\n              value :  wrai\n            - name: TOZID_CLIENT_ID\n              value :  wr-studio-sso\n            - name: TOZID_HOST_NAME\n              value :                 \n          resources:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      containers:\n        - name: ui\n          image: xxx\n          env:\n            - name: DOCKER_IMAGE\n              value: xxx\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: TOZID_REALM_NAME\n              value :  wrai\n            - name: TOZID_CLIENT_ID\n              value :  wr-studio-sso\n            - name: TOZID_HOST_NAME\n              value :                 \n          resources:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      containers:\n        - name: ui\n          image: xxx\n          env:\n            - name: DOCKER_IMAGE\n              value: xxx\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: TOZID_REALM_NAME\n              value :  wrai\n            - name: TOZID_CLIENT_ID\n              value :  wr-studio-sso\n            - name: TOZID_HOST_NAME\n              value :                 \n          resources:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      containers:\n        - name: ui\n          image: xxx\n          env:\n            - name: DOCKER_IMAGE\n              value: xxx\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: TOZID_REALM_NAME\n              value :  wrai\n            - name: TOZID_CLIENT_ID\n              value :  wr-studio-sso\n            - name: TOZID_HOST_NAME\n              value :                 \n          resources:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      containers:\n        - name: ui\n          image: xxx\n          env:\n            - name: DOCKER_IMAGE\n              value: xxx\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: TOZID_REALM_NAME\n              value :  wrai\n            - name: TOZID_CLIENT_ID\n              value :  wr-studio-sso\n            - name: TOZID_HOST_NAME\n              value :                 \n          resources:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      containers:\n        - name: ui\n          image: xxx\n          env:\n            - name: DOCKER_IMAGE\n              value: xxx\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: TOZID_REALM_NAME\n              value :  wrai\n            - name: TOZID_CLIENT_ID\n              value :  wr-studio-sso\n            - name: TOZID_HOST_NAME\n              value :                 \n          resources:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      containers:\n        - name: ui\n          image: xxx\n          env:\n            - name: DOCKER_IMAGE\n              value: xxx\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: TOZID_REALM_NAME\n              value :  wrai\n            - name: TOZID_CLIENT_ID\n              value :  wr-studio-sso\n            - name: TOZID_HOST_NAME\n              value :                 \n          resources:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      containers:\n        - name: ui\n          image: xxx\n          env:\n            - name: DOCKER_IMAGE\n              value: xxx\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: TOZID_REALM_NAME\n              value :  wrai\n            - name: TOZID_CLIENT_ID\n              value :  wr-studio-sso\n            - name: TOZID_HOST_NAME\n              value :                 \n          resources:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      containers:\n        - name: ui\n          image: xxx\n          env:\n            - name: DOCKER_IMAGE\n              value: xxx\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: TOZID_REALM_NAME\n              value :  wrai\n            - name: TOZID_CLIENT_ID\n              value :  wr-studio-sso\n            - name: TOZID_HOST_NAME\n              value :                 \n          resources:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      containers:\n        - name: ui\n          image: xxx\n          env:\n            - name: DOCKER_IMAGE\n              value: xxx\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: TOZID_REALM_NAME\n              value :  wrai\n            - name: TOZID_CLIENT_ID\n              value :  wr-studio-sso\n            - name: TOZID_HOST_NAME\n              value :                 \n          resources:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      containers:\n        - name: ui\n          image: xxx\n          env:\n            - name: DOCKER_IMAGE\n              value: xxx\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: TOZID_REALM_NAME\n              value :  wrai\n            - name: TOZID_CLIENT_ID\n              value :  wr-studio-sso\n            - name: TOZID_HOST_NAME\n              value :                 \n          resources:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      containers:\n        - name: ui\n          image: xxx\n          env:\n            - name: DOCKER_IMAGE\n              value: xxx\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: TOZID_REALM_NAME\n              value :  wrai\n            - name: TOZID_CLIENT_ID\n              value :  wr-studio-sso\n            - name: TOZID_HOST_NAME\n              value :                 \n          resources:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      containers:\n        - name: ui\n          image: xxx\n          env:\n            - name: DOCKER_IMAGE\n              value: xxx\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: TOZID_REALM_NAME\n              value :  wrai\n            - name: TOZID_CLIENT_ID\n              value :  wr-studio-sso\n            - name: TOZID_HOST_NAME\n              value :                 \n          resources:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      containers:\n        - name: ui\n          image: xxx\n          env:\n            - name: DOCKER_IMAGE\n              value: xxx\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: TOZID_REALM_NAME\n              value :  wrai\n            - name: TOZID_CLIENT_ID\n              value :  wr-studio-sso\n            - name: TOZID_HOST_NAME\n              value :                 \n          resources:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      containers:\n        - name: ui\n          image: xxx\n          env:\n            - name: DOCKER_IMAGE\n              value: xxx\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: TOZID_REALM_NAME\n              value :  wrai\n            - name: TOZID_CLIENT_ID\n              value :  wr-studio-sso\n            - name: TOZID_HOST_NAME\n              value :                 \n          resources:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      containers:\n        - name: ui\n          image: xxx\n          env:\n            - name: DOCKER_IMAGE\n              value: xxx\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: TOZID_REALM_NAME\n              value :  wrai\n            - name: TOZID_CLIENT_ID\n              value :  wr-studio-sso\n            - name: TOZID_HOST_NAME\n              value :                 \n          resources:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      containers:\n        - name: ui\n          image: xxx\n          env:\n            - name: DOCKER_IMAGE\n              value: xxx\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: TOZID_REALM_NAME\n              value :  wrai\n            - name: TOZID_CLIENT_ID\n              value :  wr-studio-sso\n            - name: TOZID_HOST_NAME\n              value :                 \n          resources:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      containers:\n        - name: ui\n          image: xxx\n          env:\n            - name: DOCKER_IMAGE\n              value: xxx\n            - name: MCP_AUTH_URL\n              value :  xxxxx\n            - name: TOZID_REALM_NAME\n              value :  wrai\n            - name: TOZID_CLIENT_ID\n              value :  wr-studio-sso\n            - name: TOZID_HOST_NAME\n              value :                 \n          resources:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-handler.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-handler\n  labels:\n    app: ssh-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-handler\n  template:\n    metadata:\n      name: ssh-handler\n      labels:\n        app: ssh-handler\n    spec:\n      volumes:\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-handler\n          image:  \n          imagePullPolicy: Always\n          env:\n            - name : SSH_PORTAL_CONNECTION_URL\n              value : ssh-portal\n            - name : sshKeyPath\n              value : /var/secrets/id_rsa  \n            - name : SSH_PORTAL_CONNECTION_PORT\n              value : \"2222\"\n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          volumeMounts:\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-handler.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-handler\n  labels:\n    app: ssh-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-handler\n  template:\n    metadata:\n      name: ssh-handler\n      labels:\n        app: ssh-handler\n    spec:\n      volumes:\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-handler\n          image:  \n          imagePullPolicy: Always\n          env:\n            - name : SSH_PORTAL_CONNECTION_URL\n              value : ssh-portal\n            - name : sshKeyPath\n              value : /var/secrets/id_rsa  \n            - name : SSH_PORTAL_CONNECTION_PORT\n              value : \"2222\"\n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          volumeMounts:\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-handler.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-handler\n  labels:\n    app: ssh-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-handler\n  template:\n    metadata:\n      name: ssh-handler\n      labels:\n        app: ssh-handler\n    spec:\n      volumes:\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-handler\n          image:  \n          imagePullPolicy: Always\n          env:\n            - name : SSH_PORTAL_CONNECTION_URL\n              value : ssh-portal\n            - name : sshKeyPath\n              value : /var/secrets/id_rsa  \n            - name : SSH_PORTAL_CONNECTION_PORT\n              value : \"2222\"\n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          volumeMounts:\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_33", "ruleIndex": 21, "level": "error", "attachments": [], "message": {"text": "Ensure the Kubernetes dashboard is not deployed"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-handler.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-handler\n  labels:\n    app: ssh-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-handler\n  template:\n    metadata:\n      name: ssh-handler\n      labels:\n        app: ssh-handler\n    spec:\n      volumes:\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-handler\n          image:  \n          imagePullPolicy: Always\n          env:\n            - name : SSH_PORTAL_CONNECTION_URL\n              value : ssh-portal\n            - name : sshKeyPath\n              value : /var/secrets/id_rsa  \n            - name : SSH_PORTAL_CONNECTION_PORT\n              value : \"2222\"\n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          volumeMounts:\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-handler.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-handler\n  labels:\n    app: ssh-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-handler\n  template:\n    metadata:\n      name: ssh-handler\n      labels:\n        app: ssh-handler\n    spec:\n      volumes:\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-handler\n          image:  \n          imagePullPolicy: Always\n          env:\n            - name : SSH_PORTAL_CONNECTION_URL\n              value : ssh-portal\n            - name : sshKeyPath\n              value : /var/secrets/id_rsa  \n            - name : SSH_PORTAL_CONNECTION_PORT\n              value : \"2222\"\n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          volumeMounts:\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-handler.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-handler\n  labels:\n    app: ssh-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-handler\n  template:\n    metadata:\n      name: ssh-handler\n      labels:\n        app: ssh-handler\n    spec:\n      volumes:\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-handler\n          image:  \n          imagePullPolicy: Always\n          env:\n            - name : SSH_PORTAL_CONNECTION_URL\n              value : ssh-portal\n            - name : sshKeyPath\n              value : /var/secrets/id_rsa  \n            - name : SSH_PORTAL_CONNECTION_PORT\n              value : \"2222\"\n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          volumeMounts:\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-handler.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-handler\n  labels:\n    app: ssh-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-handler\n  template:\n    metadata:\n      name: ssh-handler\n      labels:\n        app: ssh-handler\n    spec:\n      volumes:\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-handler\n          image:  \n          imagePullPolicy: Always\n          env:\n            - name : SSH_PORTAL_CONNECTION_URL\n              value : ssh-portal\n            - name : sshKeyPath\n              value : /var/secrets/id_rsa  \n            - name : SSH_PORTAL_CONNECTION_PORT\n              value : \"2222\"\n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          volumeMounts:\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-handler.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-handler\n  labels:\n    app: ssh-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-handler\n  template:\n    metadata:\n      name: ssh-handler\n      labels:\n        app: ssh-handler\n    spec:\n      volumes:\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-handler\n          image:  \n          imagePullPolicy: Always\n          env:\n            - name : SSH_PORTAL_CONNECTION_URL\n              value : ssh-portal\n            - name : sshKeyPath\n              value : /var/secrets/id_rsa  \n            - name : SSH_PORTAL_CONNECTION_PORT\n              value : \"2222\"\n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          volumeMounts:\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-handler.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-handler\n  labels:\n    app: ssh-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-handler\n  template:\n    metadata:\n      name: ssh-handler\n      labels:\n        app: ssh-handler\n    spec:\n      volumes:\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-handler\n          image:  \n          imagePullPolicy: Always\n          env:\n            - name : SSH_PORTAL_CONNECTION_URL\n              value : ssh-portal\n            - name : sshKeyPath\n              value : /var/secrets/id_rsa  \n            - name : SSH_PORTAL_CONNECTION_PORT\n              value : \"2222\"\n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          volumeMounts:\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-handler.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-handler\n  labels:\n    app: ssh-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-handler\n  template:\n    metadata:\n      name: ssh-handler\n      labels:\n        app: ssh-handler\n    spec:\n      volumes:\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-handler\n          image:  \n          imagePullPolicy: Always\n          env:\n            - name : SSH_PORTAL_CONNECTION_URL\n              value : ssh-portal\n            - name : sshKeyPath\n              value : /var/secrets/id_rsa  \n            - name : SSH_PORTAL_CONNECTION_PORT\n              value : \"2222\"\n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          volumeMounts:\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-handler.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-handler\n  labels:\n    app: ssh-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-handler\n  template:\n    metadata:\n      name: ssh-handler\n      labels:\n        app: ssh-handler\n    spec:\n      volumes:\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-handler\n          image:  \n          imagePullPolicy: Always\n          env:\n            - name : SSH_PORTAL_CONNECTION_URL\n              value : ssh-portal\n            - name : sshKeyPath\n              value : /var/secrets/id_rsa  \n            - name : SSH_PORTAL_CONNECTION_PORT\n              value : \"2222\"\n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          volumeMounts:\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-handler.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-handler\n  labels:\n    app: ssh-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-handler\n  template:\n    metadata:\n      name: ssh-handler\n      labels:\n        app: ssh-handler\n    spec:\n      volumes:\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-handler\n          image:  \n          imagePullPolicy: Always\n          env:\n            - name : SSH_PORTAL_CONNECTION_URL\n              value : ssh-portal\n            - name : sshKeyPath\n              value : /var/secrets/id_rsa  \n            - name : SSH_PORTAL_CONNECTION_PORT\n              value : \"2222\"\n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          volumeMounts:\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-handler.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-handler\n  labels:\n    app: ssh-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-handler\n  template:\n    metadata:\n      name: ssh-handler\n      labels:\n        app: ssh-handler\n    spec:\n      volumes:\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-handler\n          image:  \n          imagePullPolicy: Always\n          env:\n            - name : SSH_PORTAL_CONNECTION_URL\n              value : ssh-portal\n            - name : sshKeyPath\n              value : /var/secrets/id_rsa  \n            - name : SSH_PORTAL_CONNECTION_PORT\n              value : \"2222\"\n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          volumeMounts:\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-handler.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-handler\n  labels:\n    app: ssh-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-handler\n  template:\n    metadata:\n      name: ssh-handler\n      labels:\n        app: ssh-handler\n    spec:\n      volumes:\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-handler\n          image:  \n          imagePullPolicy: Always\n          env:\n            - name : SSH_PORTAL_CONNECTION_URL\n              value : ssh-portal\n            - name : sshKeyPath\n              value : /var/secrets/id_rsa  \n            - name : SSH_PORTAL_CONNECTION_PORT\n              value : \"2222\"\n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          volumeMounts:\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-handler.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-handler\n  labels:\n    app: ssh-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-handler\n  template:\n    metadata:\n      name: ssh-handler\n      labels:\n        app: ssh-handler\n    spec:\n      volumes:\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-handler\n          image:  \n          imagePullPolicy: Always\n          env:\n            - name : SSH_PORTAL_CONNECTION_URL\n              value : ssh-portal\n            - name : sshKeyPath\n              value : /var/secrets/id_rsa  \n            - name : SSH_PORTAL_CONNECTION_PORT\n              value : \"2222\"\n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          volumeMounts:\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-handler.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-handler\n  labels:\n    app: ssh-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-handler\n  template:\n    metadata:\n      name: ssh-handler\n      labels:\n        app: ssh-handler\n    spec:\n      volumes:\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-handler\n          image:  \n          imagePullPolicy: Always\n          env:\n            - name : SSH_PORTAL_CONNECTION_URL\n              value : ssh-portal\n            - name : sshKeyPath\n              value : /var/secrets/id_rsa  \n            - name : SSH_PORTAL_CONNECTION_PORT\n              value : \"2222\"\n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          volumeMounts:\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-handler.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-handler\n  labels:\n    app: ssh-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-handler\n  template:\n    metadata:\n      name: ssh-handler\n      labels:\n        app: ssh-handler\n    spec:\n      volumes:\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-handler\n          image:  \n          imagePullPolicy: Always\n          env:\n            - name : SSH_PORTAL_CONNECTION_URL\n              value : ssh-portal\n            - name : sshKeyPath\n              value : /var/secrets/id_rsa  \n            - name : SSH_PORTAL_CONNECTION_PORT\n              value : \"2222\"\n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          volumeMounts:\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-handler.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-handler\n  labels:\n    app: ssh-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-handler\n  template:\n    metadata:\n      name: ssh-handler\n      labels:\n        app: ssh-handler\n    spec:\n      volumes:\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-handler\n          image:  \n          imagePullPolicy: Always\n          env:\n            - name : SSH_PORTAL_CONNECTION_URL\n              value : ssh-portal\n            - name : sshKeyPath\n              value : /var/secrets/id_rsa  \n            - name : SSH_PORTAL_CONNECTION_PORT\n              value : \"2222\"\n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          volumeMounts:\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-handler.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-handler\n  labels:\n    app: ssh-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-handler\n  template:\n    metadata:\n      name: ssh-handler\n      labels:\n        app: ssh-handler\n    spec:\n      volumes:\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-handler\n          image:  \n          imagePullPolicy: Always\n          env:\n            - name : SSH_PORTAL_CONNECTION_URL\n              value : ssh-portal\n            - name : sshKeyPath\n              value : /var/secrets/id_rsa  \n            - name : SSH_PORTAL_CONNECTION_PORT\n              value : \"2222\"\n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          volumeMounts:\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-handler.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-handler\n  labels:\n    app: ssh-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-handler\n  template:\n    metadata:\n      name: ssh-handler\n      labels:\n        app: ssh-handler\n    spec:\n      volumes:\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-handler\n          image:  \n          imagePullPolicy: Always\n          env:\n            - name : SSH_PORTAL_CONNECTION_URL\n              value : ssh-portal\n            - name : sshKeyPath\n              value : /var/secrets/id_rsa  \n            - name : SSH_PORTAL_CONNECTION_PORT\n              value : \"2222\"\n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          volumeMounts:\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-handler.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-handler\n  labels:\n    app: ssh-handler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-handler\n  template:\n    metadata:\n      name: ssh-handler\n      labels:\n        app: ssh-handler\n    spec:\n      volumes:\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-handler\n          image:  \n          imagePullPolicy: Always\n          env:\n            - name : SSH_PORTAL_CONNECTION_URL\n              value : ssh-portal\n            - name : sshKeyPath\n              value : /var/secrets/id_rsa  \n            - name : SSH_PORTAL_CONNECTION_PORT\n              value : \"2222\"\n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          volumeMounts:\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/virtual-target.yaml"}, "region": {"startLine": 16, "endLine": 91, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: virtual-target-control\n  namespace: default\n  labels:\n    app: virtual-target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: virtual-target-control\n  template:\n    metadata:\n      name: virtual-target-control\n      labels:\n        app: virtual-target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: virtual-target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: VIRTUAL_DNS\n              value : \n            - name : VT_LAUNCHER_IMAGE\n              value : \n            - name : GITLAB_URL\n              value : \n            - name : REPO_URL\n              value : \n            - name : SSH_URL\n              value : \n            - name : SSH_KEY_PATH\n              value : \n            - name : SSH_SECRET_NAME\n              value : \n            - name : SSH_CONNECTION_PORT\n              value : \"2222\"\n            - name: UM_URL\n              value :  xxxx\n            - name: DT_SVC_ACCOUNT\n              value :  vt-service\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/virtual-target-control/\"\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n          args:\n            - -c\n            - while [ 1 ] ; do ./run.sh ; done\n          command:\n            - bash\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/virtual-target.yaml"}, "region": {"startLine": 16, "endLine": 91, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: virtual-target-control\n  namespace: default\n  labels:\n    app: virtual-target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: virtual-target-control\n  template:\n    metadata:\n      name: virtual-target-control\n      labels:\n        app: virtual-target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: virtual-target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: VIRTUAL_DNS\n              value : \n            - name : VT_LAUNCHER_IMAGE\n              value : \n            - name : GITLAB_URL\n              value : \n            - name : REPO_URL\n              value : \n            - name : SSH_URL\n              value : \n            - name : SSH_KEY_PATH\n              value : \n            - name : SSH_SECRET_NAME\n              value : \n            - name : SSH_CONNECTION_PORT\n              value : \"2222\"\n            - name: UM_URL\n              value :  xxxx\n            - name: DT_SVC_ACCOUNT\n              value :  vt-service\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/virtual-target-control/\"\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n          args:\n            - -c\n            - while [ 1 ] ; do ./run.sh ; done\n          command:\n            - bash\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/virtual-target.yaml"}, "region": {"startLine": 16, "endLine": 91, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: virtual-target-control\n  namespace: default\n  labels:\n    app: virtual-target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: virtual-target-control\n  template:\n    metadata:\n      name: virtual-target-control\n      labels:\n        app: virtual-target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: virtual-target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: VIRTUAL_DNS\n              value : \n            - name : VT_LAUNCHER_IMAGE\n              value : \n            - name : GITLAB_URL\n              value : \n            - name : REPO_URL\n              value : \n            - name : SSH_URL\n              value : \n            - name : SSH_KEY_PATH\n              value : \n            - name : SSH_SECRET_NAME\n              value : \n            - name : SSH_CONNECTION_PORT\n              value : \"2222\"\n            - name: UM_URL\n              value :  xxxx\n            - name: DT_SVC_ACCOUNT\n              value :  vt-service\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/virtual-target-control/\"\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n          args:\n            - -c\n            - while [ 1 ] ; do ./run.sh ; done\n          command:\n            - bash\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_33", "ruleIndex": 21, "level": "error", "attachments": [], "message": {"text": "Ensure the Kubernetes dashboard is not deployed"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/virtual-target.yaml"}, "region": {"startLine": 16, "endLine": 91, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: virtual-target-control\n  namespace: default\n  labels:\n    app: virtual-target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: virtual-target-control\n  template:\n    metadata:\n      name: virtual-target-control\n      labels:\n        app: virtual-target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: virtual-target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: VIRTUAL_DNS\n              value : \n            - name : VT_LAUNCHER_IMAGE\n              value : \n            - name : GITLAB_URL\n              value : \n            - name : REPO_URL\n              value : \n            - name : SSH_URL\n              value : \n            - name : SSH_KEY_PATH\n              value : \n            - name : SSH_SECRET_NAME\n              value : \n            - name : SSH_CONNECTION_PORT\n              value : \"2222\"\n            - name: UM_URL\n              value :  xxxx\n            - name: DT_SVC_ACCOUNT\n              value :  vt-service\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/virtual-target-control/\"\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n          args:\n            - -c\n            - while [ 1 ] ; do ./run.sh ; done\n          command:\n            - bash\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/virtual-target.yaml"}, "region": {"startLine": 16, "endLine": 91, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: virtual-target-control\n  namespace: default\n  labels:\n    app: virtual-target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: virtual-target-control\n  template:\n    metadata:\n      name: virtual-target-control\n      labels:\n        app: virtual-target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: virtual-target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: VIRTUAL_DNS\n              value : \n            - name : VT_LAUNCHER_IMAGE\n              value : \n            - name : GITLAB_URL\n              value : \n            - name : REPO_URL\n              value : \n            - name : SSH_URL\n              value : \n            - name : SSH_KEY_PATH\n              value : \n            - name : SSH_SECRET_NAME\n              value : \n            - name : SSH_CONNECTION_PORT\n              value : \"2222\"\n            - name: UM_URL\n              value :  xxxx\n            - name: DT_SVC_ACCOUNT\n              value :  vt-service\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/virtual-target-control/\"\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n          args:\n            - -c\n            - while [ 1 ] ; do ./run.sh ; done\n          command:\n            - bash\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/virtual-target.yaml"}, "region": {"startLine": 16, "endLine": 91, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: virtual-target-control\n  namespace: default\n  labels:\n    app: virtual-target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: virtual-target-control\n  template:\n    metadata:\n      name: virtual-target-control\n      labels:\n        app: virtual-target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: virtual-target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: VIRTUAL_DNS\n              value : \n            - name : VT_LAUNCHER_IMAGE\n              value : \n            - name : GITLAB_URL\n              value : \n            - name : REPO_URL\n              value : \n            - name : SSH_URL\n              value : \n            - name : SSH_KEY_PATH\n              value : \n            - name : SSH_SECRET_NAME\n              value : \n            - name : SSH_CONNECTION_PORT\n              value : \"2222\"\n            - name: UM_URL\n              value :  xxxx\n            - name: DT_SVC_ACCOUNT\n              value :  vt-service\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/virtual-target-control/\"\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n          args:\n            - -c\n            - while [ 1 ] ; do ./run.sh ; done\n          command:\n            - bash\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/virtual-target.yaml"}, "region": {"startLine": 16, "endLine": 91, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: virtual-target-control\n  namespace: default\n  labels:\n    app: virtual-target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: virtual-target-control\n  template:\n    metadata:\n      name: virtual-target-control\n      labels:\n        app: virtual-target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: virtual-target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: VIRTUAL_DNS\n              value : \n            - name : VT_LAUNCHER_IMAGE\n              value : \n            - name : GITLAB_URL\n              value : \n            - name : REPO_URL\n              value : \n            - name : SSH_URL\n              value : \n            - name : SSH_KEY_PATH\n              value : \n            - name : SSH_SECRET_NAME\n              value : \n            - name : SSH_CONNECTION_PORT\n              value : \"2222\"\n            - name: UM_URL\n              value :  xxxx\n            - name: DT_SVC_ACCOUNT\n              value :  vt-service\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/virtual-target-control/\"\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n          args:\n            - -c\n            - while [ 1 ] ; do ./run.sh ; done\n          command:\n            - bash\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/virtual-target.yaml"}, "region": {"startLine": 16, "endLine": 91, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: virtual-target-control\n  namespace: default\n  labels:\n    app: virtual-target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: virtual-target-control\n  template:\n    metadata:\n      name: virtual-target-control\n      labels:\n        app: virtual-target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: virtual-target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: VIRTUAL_DNS\n              value : \n            - name : VT_LAUNCHER_IMAGE\n              value : \n            - name : GITLAB_URL\n              value : \n            - name : REPO_URL\n              value : \n            - name : SSH_URL\n              value : \n            - name : SSH_KEY_PATH\n              value : \n            - name : SSH_SECRET_NAME\n              value : \n            - name : SSH_CONNECTION_PORT\n              value : \"2222\"\n            - name: UM_URL\n              value :  xxxx\n            - name: DT_SVC_ACCOUNT\n              value :  vt-service\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/virtual-target-control/\"\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n          args:\n            - -c\n            - while [ 1 ] ; do ./run.sh ; done\n          command:\n            - bash\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/virtual-target.yaml"}, "region": {"startLine": 16, "endLine": 91, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: virtual-target-control\n  namespace: default\n  labels:\n    app: virtual-target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: virtual-target-control\n  template:\n    metadata:\n      name: virtual-target-control\n      labels:\n        app: virtual-target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: virtual-target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: VIRTUAL_DNS\n              value : \n            - name : VT_LAUNCHER_IMAGE\n              value : \n            - name : GITLAB_URL\n              value : \n            - name : REPO_URL\n              value : \n            - name : SSH_URL\n              value : \n            - name : SSH_KEY_PATH\n              value : \n            - name : SSH_SECRET_NAME\n              value : \n            - name : SSH_CONNECTION_PORT\n              value : \"2222\"\n            - name: UM_URL\n              value :  xxxx\n            - name: DT_SVC_ACCOUNT\n              value :  vt-service\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/virtual-target-control/\"\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n          args:\n            - -c\n            - while [ 1 ] ; do ./run.sh ; done\n          command:\n            - bash\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/virtual-target.yaml"}, "region": {"startLine": 16, "endLine": 91, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: virtual-target-control\n  namespace: default\n  labels:\n    app: virtual-target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: virtual-target-control\n  template:\n    metadata:\n      name: virtual-target-control\n      labels:\n        app: virtual-target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: virtual-target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: VIRTUAL_DNS\n              value : \n            - name : VT_LAUNCHER_IMAGE\n              value : \n            - name : GITLAB_URL\n              value : \n            - name : REPO_URL\n              value : \n            - name : SSH_URL\n              value : \n            - name : SSH_KEY_PATH\n              value : \n            - name : SSH_SECRET_NAME\n              value : \n            - name : SSH_CONNECTION_PORT\n              value : \"2222\"\n            - name: UM_URL\n              value :  xxxx\n            - name: DT_SVC_ACCOUNT\n              value :  vt-service\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/virtual-target-control/\"\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n          args:\n            - -c\n            - while [ 1 ] ; do ./run.sh ; done\n          command:\n            - bash\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/virtual-target.yaml"}, "region": {"startLine": 16, "endLine": 91, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: virtual-target-control\n  namespace: default\n  labels:\n    app: virtual-target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: virtual-target-control\n  template:\n    metadata:\n      name: virtual-target-control\n      labels:\n        app: virtual-target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: virtual-target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: VIRTUAL_DNS\n              value : \n            - name : VT_LAUNCHER_IMAGE\n              value : \n            - name : GITLAB_URL\n              value : \n            - name : REPO_URL\n              value : \n            - name : SSH_URL\n              value : \n            - name : SSH_KEY_PATH\n              value : \n            - name : SSH_SECRET_NAME\n              value : \n            - name : SSH_CONNECTION_PORT\n              value : \"2222\"\n            - name: UM_URL\n              value :  xxxx\n            - name: DT_SVC_ACCOUNT\n              value :  vt-service\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/virtual-target-control/\"\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n          args:\n            - -c\n            - while [ 1 ] ; do ./run.sh ; done\n          command:\n            - bash\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/virtual-target.yaml"}, "region": {"startLine": 16, "endLine": 91, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: virtual-target-control\n  namespace: default\n  labels:\n    app: virtual-target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: virtual-target-control\n  template:\n    metadata:\n      name: virtual-target-control\n      labels:\n        app: virtual-target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: virtual-target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: VIRTUAL_DNS\n              value : \n            - name : VT_LAUNCHER_IMAGE\n              value : \n            - name : GITLAB_URL\n              value : \n            - name : REPO_URL\n              value : \n            - name : SSH_URL\n              value : \n            - name : SSH_KEY_PATH\n              value : \n            - name : SSH_SECRET_NAME\n              value : \n            - name : SSH_CONNECTION_PORT\n              value : \"2222\"\n            - name: UM_URL\n              value :  xxxx\n            - name: DT_SVC_ACCOUNT\n              value :  vt-service\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/virtual-target-control/\"\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n          args:\n            - -c\n            - while [ 1 ] ; do ./run.sh ; done\n          command:\n            - bash\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/virtual-target.yaml"}, "region": {"startLine": 16, "endLine": 91, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: virtual-target-control\n  namespace: default\n  labels:\n    app: virtual-target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: virtual-target-control\n  template:\n    metadata:\n      name: virtual-target-control\n      labels:\n        app: virtual-target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: virtual-target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: VIRTUAL_DNS\n              value : \n            - name : VT_LAUNCHER_IMAGE\n              value : \n            - name : GITLAB_URL\n              value : \n            - name : REPO_URL\n              value : \n            - name : SSH_URL\n              value : \n            - name : SSH_KEY_PATH\n              value : \n            - name : SSH_SECRET_NAME\n              value : \n            - name : SSH_CONNECTION_PORT\n              value : \"2222\"\n            - name: UM_URL\n              value :  xxxx\n            - name: DT_SVC_ACCOUNT\n              value :  vt-service\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/virtual-target-control/\"\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n          args:\n            - -c\n            - while [ 1 ] ; do ./run.sh ; done\n          command:\n            - bash\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/virtual-target.yaml"}, "region": {"startLine": 16, "endLine": 91, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: virtual-target-control\n  namespace: default\n  labels:\n    app: virtual-target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: virtual-target-control\n  template:\n    metadata:\n      name: virtual-target-control\n      labels:\n        app: virtual-target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: virtual-target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: VIRTUAL_DNS\n              value : \n            - name : VT_LAUNCHER_IMAGE\n              value : \n            - name : GITLAB_URL\n              value : \n            - name : REPO_URL\n              value : \n            - name : SSH_URL\n              value : \n            - name : SSH_KEY_PATH\n              value : \n            - name : SSH_SECRET_NAME\n              value : \n            - name : SSH_CONNECTION_PORT\n              value : \"2222\"\n            - name: UM_URL\n              value :  xxxx\n            - name: DT_SVC_ACCOUNT\n              value :  vt-service\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/virtual-target-control/\"\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n          args:\n            - -c\n            - while [ 1 ] ; do ./run.sh ; done\n          command:\n            - bash\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/virtual-target.yaml"}, "region": {"startLine": 16, "endLine": 91, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: virtual-target-control\n  namespace: default\n  labels:\n    app: virtual-target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: virtual-target-control\n  template:\n    metadata:\n      name: virtual-target-control\n      labels:\n        app: virtual-target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: virtual-target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: VIRTUAL_DNS\n              value : \n            - name : VT_LAUNCHER_IMAGE\n              value : \n            - name : GITLAB_URL\n              value : \n            - name : REPO_URL\n              value : \n            - name : SSH_URL\n              value : \n            - name : SSH_KEY_PATH\n              value : \n            - name : SSH_SECRET_NAME\n              value : \n            - name : SSH_CONNECTION_PORT\n              value : \"2222\"\n            - name: UM_URL\n              value :  xxxx\n            - name: DT_SVC_ACCOUNT\n              value :  vt-service\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/virtual-target-control/\"\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n          args:\n            - -c\n            - while [ 1 ] ; do ./run.sh ; done\n          command:\n            - bash\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/virtual-target.yaml"}, "region": {"startLine": 16, "endLine": 91, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: virtual-target-control\n  namespace: default\n  labels:\n    app: virtual-target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: virtual-target-control\n  template:\n    metadata:\n      name: virtual-target-control\n      labels:\n        app: virtual-target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: virtual-target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: VIRTUAL_DNS\n              value : \n            - name : VT_LAUNCHER_IMAGE\n              value : \n            - name : GITLAB_URL\n              value : \n            - name : REPO_URL\n              value : \n            - name : SSH_URL\n              value : \n            - name : SSH_KEY_PATH\n              value : \n            - name : SSH_SECRET_NAME\n              value : \n            - name : SSH_CONNECTION_PORT\n              value : \"2222\"\n            - name: UM_URL\n              value :  xxxx\n            - name: DT_SVC_ACCOUNT\n              value :  vt-service\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/virtual-target-control/\"\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n          args:\n            - -c\n            - while [ 1 ] ; do ./run.sh ; done\n          command:\n            - bash\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/virtual-target.yaml"}, "region": {"startLine": 16, "endLine": 91, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: virtual-target-control\n  namespace: default\n  labels:\n    app: virtual-target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: virtual-target-control\n  template:\n    metadata:\n      name: virtual-target-control\n      labels:\n        app: virtual-target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: virtual-target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: VIRTUAL_DNS\n              value : \n            - name : VT_LAUNCHER_IMAGE\n              value : \n            - name : GITLAB_URL\n              value : \n            - name : REPO_URL\n              value : \n            - name : SSH_URL\n              value : \n            - name : SSH_KEY_PATH\n              value : \n            - name : SSH_SECRET_NAME\n              value : \n            - name : SSH_CONNECTION_PORT\n              value : \"2222\"\n            - name: UM_URL\n              value :  xxxx\n            - name: DT_SVC_ACCOUNT\n              value :  vt-service\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/virtual-target-control/\"\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n          args:\n            - -c\n            - while [ 1 ] ; do ./run.sh ; done\n          command:\n            - bash\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/virtual-target.yaml"}, "region": {"startLine": 16, "endLine": 91, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: virtual-target-control\n  namespace: default\n  labels:\n    app: virtual-target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: virtual-target-control\n  template:\n    metadata:\n      name: virtual-target-control\n      labels:\n        app: virtual-target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: virtual-target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: VIRTUAL_DNS\n              value : \n            - name : VT_LAUNCHER_IMAGE\n              value : \n            - name : GITLAB_URL\n              value : \n            - name : REPO_URL\n              value : \n            - name : SSH_URL\n              value : \n            - name : SSH_KEY_PATH\n              value : \n            - name : SSH_SECRET_NAME\n              value : \n            - name : SSH_CONNECTION_PORT\n              value : \"2222\"\n            - name: UM_URL\n              value :  xxxx\n            - name: DT_SVC_ACCOUNT\n              value :  vt-service\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/virtual-target-control/\"\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n          args:\n            - -c\n            - while [ 1 ] ; do ./run.sh ; done\n          command:\n            - bash\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/virtual-target.yaml"}, "region": {"startLine": 16, "endLine": 91, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: virtual-target-control\n  namespace: default\n  labels:\n    app: virtual-target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: virtual-target-control\n  template:\n    metadata:\n      name: virtual-target-control\n      labels:\n        app: virtual-target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: virtual-target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: VIRTUAL_DNS\n              value : \n            - name : VT_LAUNCHER_IMAGE\n              value : \n            - name : GITLAB_URL\n              value : \n            - name : REPO_URL\n              value : \n            - name : SSH_URL\n              value : \n            - name : SSH_KEY_PATH\n              value : \n            - name : SSH_SECRET_NAME\n              value : \n            - name : SSH_CONNECTION_PORT\n              value : \"2222\"\n            - name: UM_URL\n              value :  xxxx\n            - name: DT_SVC_ACCOUNT\n              value :  vt-service\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/virtual-target-control/\"\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n          args:\n            - -c\n            - while [ 1 ] ; do ./run.sh ; done\n          command:\n            - bash\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/virtual-target.yaml"}, "region": {"startLine": 16, "endLine": 91, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: virtual-target-control\n  namespace: default\n  labels:\n    app: virtual-target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: virtual-target-control\n  template:\n    metadata:\n      name: virtual-target-control\n      labels:\n        app: virtual-target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: virtual-target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: VIRTUAL_DNS\n              value : \n            - name : VT_LAUNCHER_IMAGE\n              value : \n            - name : GITLAB_URL\n              value : \n            - name : REPO_URL\n              value : \n            - name : SSH_URL\n              value : \n            - name : SSH_KEY_PATH\n              value : \n            - name : SSH_SECRET_NAME\n              value : \n            - name : SSH_CONNECTION_PORT\n              value : \"2222\"\n            - name: UM_URL\n              value :  xxxx\n            - name: DT_SVC_ACCOUNT\n              value :  vt-service\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/virtual-target-control/\"\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n          args:\n            - -c\n            - while [ 1 ] ; do ./run.sh ; done\n          command:\n            - bash\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/virtual-target.yaml"}, "region": {"startLine": 16, "endLine": 91, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: virtual-target-control\n  namespace: default\n  labels:\n    app: virtual-target-control\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: virtual-target-control\n  template:\n    metadata:\n      name: virtual-target-control\n      labels:\n        app: virtual-target-control\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n        - name: ssh-private-volume\n          secret:\n            secretName: ssh-portal-admin\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: virtual-target-control\n          image: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: VIRTUAL_DNS\n              value : \n            - name : VT_LAUNCHER_IMAGE\n              value : \n            - name : GITLAB_URL\n              value : \n            - name : REPO_URL\n              value : \n            - name : SSH_URL\n              value : \n            - name : SSH_KEY_PATH\n              value : \n            - name : SSH_SECRET_NAME\n              value : \n            - name : SSH_CONNECTION_PORT\n              value : \"2222\"\n            - name: UM_URL\n              value :  xxxx\n            - name: DT_SVC_ACCOUNT\n              value :  vt-service\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/virtual-target-control/\"\n            - name: ssh-private-volume\n              readOnly: true\n              mountPath: \"/var/secrets/\"\n          args:\n            - -c\n            - while [ 1 ] ; do ./run.sh ; done\n          command:\n            - bash\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-portal.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-portal\n  namespace: default\n  labels:\n    app: ssh-portal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-portal\n  template:\n    metadata:\n      name: ssh-portal\n      labels:\n        app: ssh-portal\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-portal\n          image: xxxx\n          imagePullPolicy: \n          env:\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: SSHPORTAL_DATABASE_URL\n              value: postgres://xxx:xxx@:/xxx\n            - name: SSHPORTAL_DB_DRIVER\n              value: postgres\n            - name: SSHPORTAL_DEFAULT_ADMIN_INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSHPORTAL_DEBUG\n              value: \"1\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/ssh-portal/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-portal.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-portal\n  namespace: default\n  labels:\n    app: ssh-portal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-portal\n  template:\n    metadata:\n      name: ssh-portal\n      labels:\n        app: ssh-portal\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-portal\n          image: xxxx\n          imagePullPolicy: \n          env:\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: SSHPORTAL_DATABASE_URL\n              value: postgres://xxx:xxx@:/xxx\n            - name: SSHPORTAL_DB_DRIVER\n              value: postgres\n            - name: SSHPORTAL_DEFAULT_ADMIN_INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSHPORTAL_DEBUG\n              value: \"1\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/ssh-portal/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-portal.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-portal\n  namespace: default\n  labels:\n    app: ssh-portal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-portal\n  template:\n    metadata:\n      name: ssh-portal\n      labels:\n        app: ssh-portal\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-portal\n          image: xxxx\n          imagePullPolicy: \n          env:\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: SSHPORTAL_DATABASE_URL\n              value: postgres://xxx:xxx@:/xxx\n            - name: SSHPORTAL_DB_DRIVER\n              value: postgres\n            - name: SSHPORTAL_DEFAULT_ADMIN_INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSHPORTAL_DEBUG\n              value: \"1\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/ssh-portal/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-portal.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-portal\n  namespace: default\n  labels:\n    app: ssh-portal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-portal\n  template:\n    metadata:\n      name: ssh-portal\n      labels:\n        app: ssh-portal\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-portal\n          image: xxxx\n          imagePullPolicy: \n          env:\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: SSHPORTAL_DATABASE_URL\n              value: postgres://xxx:xxx@:/xxx\n            - name: SSHPORTAL_DB_DRIVER\n              value: postgres\n            - name: SSHPORTAL_DEFAULT_ADMIN_INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSHPORTAL_DEBUG\n              value: \"1\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/ssh-portal/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-portal.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-portal\n  namespace: default\n  labels:\n    app: ssh-portal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-portal\n  template:\n    metadata:\n      name: ssh-portal\n      labels:\n        app: ssh-portal\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-portal\n          image: xxxx\n          imagePullPolicy: \n          env:\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: SSHPORTAL_DATABASE_URL\n              value: postgres://xxx:xxx@:/xxx\n            - name: SSHPORTAL_DB_DRIVER\n              value: postgres\n            - name: SSHPORTAL_DEFAULT_ADMIN_INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSHPORTAL_DEBUG\n              value: \"1\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/ssh-portal/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-portal.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-portal\n  namespace: default\n  labels:\n    app: ssh-portal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-portal\n  template:\n    metadata:\n      name: ssh-portal\n      labels:\n        app: ssh-portal\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-portal\n          image: xxxx\n          imagePullPolicy: \n          env:\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: SSHPORTAL_DATABASE_URL\n              value: postgres://xxx:xxx@:/xxx\n            - name: SSHPORTAL_DB_DRIVER\n              value: postgres\n            - name: SSHPORTAL_DEFAULT_ADMIN_INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSHPORTAL_DEBUG\n              value: \"1\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/ssh-portal/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-portal.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-portal\n  namespace: default\n  labels:\n    app: ssh-portal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-portal\n  template:\n    metadata:\n      name: ssh-portal\n      labels:\n        app: ssh-portal\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-portal\n          image: xxxx\n          imagePullPolicy: \n          env:\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: SSHPORTAL_DATABASE_URL\n              value: postgres://xxx:xxx@:/xxx\n            - name: SSHPORTAL_DB_DRIVER\n              value: postgres\n            - name: SSHPORTAL_DEFAULT_ADMIN_INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSHPORTAL_DEBUG\n              value: \"1\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/ssh-portal/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-portal.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-portal\n  namespace: default\n  labels:\n    app: ssh-portal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-portal\n  template:\n    metadata:\n      name: ssh-portal\n      labels:\n        app: ssh-portal\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-portal\n          image: xxxx\n          imagePullPolicy: \n          env:\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: SSHPORTAL_DATABASE_URL\n              value: postgres://xxx:xxx@:/xxx\n            - name: SSHPORTAL_DB_DRIVER\n              value: postgres\n            - name: SSHPORTAL_DEFAULT_ADMIN_INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSHPORTAL_DEBUG\n              value: \"1\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/ssh-portal/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-portal.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-portal\n  namespace: default\n  labels:\n    app: ssh-portal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-portal\n  template:\n    metadata:\n      name: ssh-portal\n      labels:\n        app: ssh-portal\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-portal\n          image: xxxx\n          imagePullPolicy: \n          env:\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: SSHPORTAL_DATABASE_URL\n              value: postgres://xxx:xxx@:/xxx\n            - name: SSHPORTAL_DB_DRIVER\n              value: postgres\n            - name: SSHPORTAL_DEFAULT_ADMIN_INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSHPORTAL_DEBUG\n              value: \"1\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/ssh-portal/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-portal.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-portal\n  namespace: default\n  labels:\n    app: ssh-portal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-portal\n  template:\n    metadata:\n      name: ssh-portal\n      labels:\n        app: ssh-portal\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-portal\n          image: xxxx\n          imagePullPolicy: \n          env:\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: SSHPORTAL_DATABASE_URL\n              value: postgres://xxx:xxx@:/xxx\n            - name: SSHPORTAL_DB_DRIVER\n              value: postgres\n            - name: SSHPORTAL_DEFAULT_ADMIN_INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSHPORTAL_DEBUG\n              value: \"1\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/ssh-portal/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-portal.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-portal\n  namespace: default\n  labels:\n    app: ssh-portal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-portal\n  template:\n    metadata:\n      name: ssh-portal\n      labels:\n        app: ssh-portal\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-portal\n          image: xxxx\n          imagePullPolicy: \n          env:\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: SSHPORTAL_DATABASE_URL\n              value: postgres://xxx:xxx@:/xxx\n            - name: SSHPORTAL_DB_DRIVER\n              value: postgres\n            - name: SSHPORTAL_DEFAULT_ADMIN_INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSHPORTAL_DEBUG\n              value: \"1\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/ssh-portal/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-portal.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-portal\n  namespace: default\n  labels:\n    app: ssh-portal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-portal\n  template:\n    metadata:\n      name: ssh-portal\n      labels:\n        app: ssh-portal\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-portal\n          image: xxxx\n          imagePullPolicy: \n          env:\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: SSHPORTAL_DATABASE_URL\n              value: postgres://xxx:xxx@:/xxx\n            - name: SSHPORTAL_DB_DRIVER\n              value: postgres\n            - name: SSHPORTAL_DEFAULT_ADMIN_INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSHPORTAL_DEBUG\n              value: \"1\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/ssh-portal/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-portal.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-portal\n  namespace: default\n  labels:\n    app: ssh-portal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-portal\n  template:\n    metadata:\n      name: ssh-portal\n      labels:\n        app: ssh-portal\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-portal\n          image: xxxx\n          imagePullPolicy: \n          env:\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: SSHPORTAL_DATABASE_URL\n              value: postgres://xxx:xxx@:/xxx\n            - name: SSHPORTAL_DB_DRIVER\n              value: postgres\n            - name: SSHPORTAL_DEFAULT_ADMIN_INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSHPORTAL_DEBUG\n              value: \"1\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/ssh-portal/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-portal.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-portal\n  namespace: default\n  labels:\n    app: ssh-portal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-portal\n  template:\n    metadata:\n      name: ssh-portal\n      labels:\n        app: ssh-portal\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-portal\n          image: xxxx\n          imagePullPolicy: \n          env:\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: SSHPORTAL_DATABASE_URL\n              value: postgres://xxx:xxx@:/xxx\n            - name: SSHPORTAL_DB_DRIVER\n              value: postgres\n            - name: SSHPORTAL_DEFAULT_ADMIN_INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSHPORTAL_DEBUG\n              value: \"1\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/ssh-portal/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-portal.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-portal\n  namespace: default\n  labels:\n    app: ssh-portal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-portal\n  template:\n    metadata:\n      name: ssh-portal\n      labels:\n        app: ssh-portal\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-portal\n          image: xxxx\n          imagePullPolicy: \n          env:\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: SSHPORTAL_DATABASE_URL\n              value: postgres://xxx:xxx@:/xxx\n            - name: SSHPORTAL_DB_DRIVER\n              value: postgres\n            - name: SSHPORTAL_DEFAULT_ADMIN_INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSHPORTAL_DEBUG\n              value: \"1\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/ssh-portal/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-portal.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-portal\n  namespace: default\n  labels:\n    app: ssh-portal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-portal\n  template:\n    metadata:\n      name: ssh-portal\n      labels:\n        app: ssh-portal\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-portal\n          image: xxxx\n          imagePullPolicy: \n          env:\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: SSHPORTAL_DATABASE_URL\n              value: postgres://xxx:xxx@:/xxx\n            - name: SSHPORTAL_DB_DRIVER\n              value: postgres\n            - name: SSHPORTAL_DEFAULT_ADMIN_INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSHPORTAL_DEBUG\n              value: \"1\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/ssh-portal/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-portal.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-portal\n  namespace: default\n  labels:\n    app: ssh-portal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-portal\n  template:\n    metadata:\n      name: ssh-portal\n      labels:\n        app: ssh-portal\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-portal\n          image: xxxx\n          imagePullPolicy: \n          env:\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: SSHPORTAL_DATABASE_URL\n              value: postgres://xxx:xxx@:/xxx\n            - name: SSHPORTAL_DB_DRIVER\n              value: postgres\n            - name: SSHPORTAL_DEFAULT_ADMIN_INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSHPORTAL_DEBUG\n              value: \"1\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/ssh-portal/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-portal.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-portal\n  namespace: default\n  labels:\n    app: ssh-portal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-portal\n  template:\n    metadata:\n      name: ssh-portal\n      labels:\n        app: ssh-portal\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-portal\n          image: xxxx\n          imagePullPolicy: \n          env:\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: SSHPORTAL_DATABASE_URL\n              value: postgres://xxx:xxx@:/xxx\n            - name: SSHPORTAL_DB_DRIVER\n              value: postgres\n            - name: SSHPORTAL_DEFAULT_ADMIN_INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSHPORTAL_DEBUG\n              value: \"1\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/ssh-portal/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-portal.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-portal\n  namespace: default\n  labels:\n    app: ssh-portal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-portal\n  template:\n    metadata:\n      name: ssh-portal\n      labels:\n        app: ssh-portal\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-portal\n          image: xxxx\n          imagePullPolicy: \n          env:\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: SSHPORTAL_DATABASE_URL\n              value: postgres://xxx:xxx@:/xxx\n            - name: SSHPORTAL_DB_DRIVER\n              value: postgres\n            - name: SSHPORTAL_DEFAULT_ADMIN_INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSHPORTAL_DEBUG\n              value: \"1\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/ssh-portal/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/ssh-portal.yaml"}, "region": {"startLine": 16, "endLine": 64, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ssh-portal\n  namespace: default\n  labels:\n    app: ssh-portal\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ssh-portal\n  template:\n    metadata:\n      name: ssh-portal\n      labels:\n        app: ssh-portal\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      serviceAccount: hive-vtarget\n      serviceAccountName: hive-vtarget\n      containers:\n        - name: ssh-portal\n          image: xxxx\n          imagePullPolicy: \n          env:\n            - name: VIRTUAL_NAMESPACE\n              value : prod-wrai-usp-hive-virtual-target\n            - name: SSHPORTAL_DATABASE_URL\n              value: postgres://xxx:xxx@:/xxx\n            - name: SSHPORTAL_DB_DRIVER\n              value: postgres\n            - name: SSHPORTAL_DEFAULT_ADMIN_INVITE_TOKEN\n              value: \"123456789\"\n            - name: SSHPORTAL_DEBUG\n              value: \"1\"\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/ssh-portal/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/reservation.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: reservation\n  namespace: default\n  labels:\n    app: reservation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reservation\n  template:\n    metadata:\n      name: reservation\n      labels:\n        app: reservation\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: reservation\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/reservation/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/reservation.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: reservation\n  namespace: default\n  labels:\n    app: reservation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reservation\n  template:\n    metadata:\n      name: reservation\n      labels:\n        app: reservation\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: reservation\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/reservation/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/reservation.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: reservation\n  namespace: default\n  labels:\n    app: reservation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reservation\n  template:\n    metadata:\n      name: reservation\n      labels:\n        app: reservation\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: reservation\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/reservation/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_33", "ruleIndex": 21, "level": "error", "attachments": [], "message": {"text": "Ensure the Kubernetes dashboard is not deployed"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/reservation.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: reservation\n  namespace: default\n  labels:\n    app: reservation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reservation\n  template:\n    metadata:\n      name: reservation\n      labels:\n        app: reservation\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: reservation\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/reservation/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/reservation.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: reservation\n  namespace: default\n  labels:\n    app: reservation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reservation\n  template:\n    metadata:\n      name: reservation\n      labels:\n        app: reservation\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: reservation\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/reservation/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/reservation.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: reservation\n  namespace: default\n  labels:\n    app: reservation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reservation\n  template:\n    metadata:\n      name: reservation\n      labels:\n        app: reservation\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: reservation\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/reservation/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/reservation.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: reservation\n  namespace: default\n  labels:\n    app: reservation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reservation\n  template:\n    metadata:\n      name: reservation\n      labels:\n        app: reservation\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: reservation\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/reservation/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/reservation.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: reservation\n  namespace: default\n  labels:\n    app: reservation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reservation\n  template:\n    metadata:\n      name: reservation\n      labels:\n        app: reservation\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: reservation\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/reservation/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/reservation.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: reservation\n  namespace: default\n  labels:\n    app: reservation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reservation\n  template:\n    metadata:\n      name: reservation\n      labels:\n        app: reservation\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: reservation\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/reservation/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/reservation.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: reservation\n  namespace: default\n  labels:\n    app: reservation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reservation\n  template:\n    metadata:\n      name: reservation\n      labels:\n        app: reservation\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: reservation\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/reservation/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/reservation.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: reservation\n  namespace: default\n  labels:\n    app: reservation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reservation\n  template:\n    metadata:\n      name: reservation\n      labels:\n        app: reservation\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: reservation\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/reservation/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/reservation.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: reservation\n  namespace: default\n  labels:\n    app: reservation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reservation\n  template:\n    metadata:\n      name: reservation\n      labels:\n        app: reservation\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: reservation\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/reservation/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/reservation.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: reservation\n  namespace: default\n  labels:\n    app: reservation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reservation\n  template:\n    metadata:\n      name: reservation\n      labels:\n        app: reservation\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: reservation\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/reservation/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/reservation.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: reservation\n  namespace: default\n  labels:\n    app: reservation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reservation\n  template:\n    metadata:\n      name: reservation\n      labels:\n        app: reservation\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: reservation\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/reservation/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/reservation.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: reservation\n  namespace: default\n  labels:\n    app: reservation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reservation\n  template:\n    metadata:\n      name: reservation\n      labels:\n        app: reservation\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: reservation\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/reservation/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/reservation.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: reservation\n  namespace: default\n  labels:\n    app: reservation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reservation\n  template:\n    metadata:\n      name: reservation\n      labels:\n        app: reservation\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: reservation\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/reservation/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/reservation.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: reservation\n  namespace: default\n  labels:\n    app: reservation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reservation\n  template:\n    metadata:\n      name: reservation\n      labels:\n        app: reservation\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: reservation\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/reservation/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/reservation.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: reservation\n  namespace: default\n  labels:\n    app: reservation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reservation\n  template:\n    metadata:\n      name: reservation\n      labels:\n        app: reservation\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: reservation\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/reservation/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/reservation.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: reservation\n  namespace: default\n  labels:\n    app: reservation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reservation\n  template:\n    metadata:\n      name: reservation\n      labels:\n        app: reservation\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: reservation\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/reservation/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/reservation.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: reservation\n  namespace: default\n  labels:\n    app: reservation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reservation\n  template:\n    metadata:\n      name: reservation\n      labels:\n        app: reservation\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: reservation\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/reservation/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/hive-app/templates/deployments/reservation.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: reservation\n  namespace: default\n  labels:\n    app: reservation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reservation\n  template:\n    metadata:\n      name: reservation\n      labels:\n        app: reservation\n      annotations:\n    spec:\n      volumes:\n        - name: volume-logs\n          persistentVolumeClaim:\n            claimName: \"logs-data\"\n      containers:\n        - name: reservation\n          image: \n          imagePullPolicy: \n          env:\n            - name: DOCKER_IMAGE\n              value: \n            - name: NODE_ENV\n              value : dockerEnvironment\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"kafka\\\":{\\\"clientId\\\":\\\"xxxx\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.training-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"postgre\\\":{\\\"database\\\":\\\"xxxx\\\",\\\"host\\\":\\\"xxxx\\\",\\\"password\\\":\\\"xxx\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"xxx\\\"},\\\"regex\\\":{\\\"value\\\":\\\"wrai\\\"},\\\"tozny\\\":{\\\"apiUrl\\\":\\\"https://xxxxx\\\",\\\"idPortalUrl\\\":\\\"https://xxxxxx\\\",\\\"realm\\\":\\\"xxxxx\\\",\\\"registrationToken\\\":\\\"xxxx\\\"}}\"\n            - name: UM_URL\n              value :  xxxx\n          resources:\n          volumeMounts:\n            - name: volume-logs\n              mountPath: \"/var/log/reservation/\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-gateway\n  labels:\n    helm.sh/chart: usp-gateway-0.1.0\n    app.kubernetes.io/name: usp-gateway\n    app.kubernetes.io/instance: usp-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-gateway\n      app.kubernetes.io/instance: usp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-gateway\n        app.kubernetes.io/instance: usp-gateway\n    spec:\n      serviceAccountName: usp-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          # livenessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          # readinessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-gateway\n  labels:\n    helm.sh/chart: usp-gateway-0.1.0\n    app.kubernetes.io/name: usp-gateway\n    app.kubernetes.io/instance: usp-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-gateway\n      app.kubernetes.io/instance: usp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-gateway\n        app.kubernetes.io/instance: usp-gateway\n    spec:\n      serviceAccountName: usp-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          # livenessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          # readinessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-gateway\n  labels:\n    helm.sh/chart: usp-gateway-0.1.0\n    app.kubernetes.io/name: usp-gateway\n    app.kubernetes.io/instance: usp-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-gateway\n      app.kubernetes.io/instance: usp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-gateway\n        app.kubernetes.io/instance: usp-gateway\n    spec:\n      serviceAccountName: usp-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          # livenessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          # readinessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-gateway\n  labels:\n    helm.sh/chart: usp-gateway-0.1.0\n    app.kubernetes.io/name: usp-gateway\n    app.kubernetes.io/instance: usp-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-gateway\n      app.kubernetes.io/instance: usp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-gateway\n        app.kubernetes.io/instance: usp-gateway\n    spec:\n      serviceAccountName: usp-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          # livenessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          # readinessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-gateway\n  labels:\n    helm.sh/chart: usp-gateway-0.1.0\n    app.kubernetes.io/name: usp-gateway\n    app.kubernetes.io/instance: usp-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-gateway\n      app.kubernetes.io/instance: usp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-gateway\n        app.kubernetes.io/instance: usp-gateway\n    spec:\n      serviceAccountName: usp-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          # livenessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          # readinessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-gateway\n  labels:\n    helm.sh/chart: usp-gateway-0.1.0\n    app.kubernetes.io/name: usp-gateway\n    app.kubernetes.io/instance: usp-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-gateway\n      app.kubernetes.io/instance: usp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-gateway\n        app.kubernetes.io/instance: usp-gateway\n    spec:\n      serviceAccountName: usp-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          # livenessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          # readinessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-gateway\n  labels:\n    helm.sh/chart: usp-gateway-0.1.0\n    app.kubernetes.io/name: usp-gateway\n    app.kubernetes.io/instance: usp-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-gateway\n      app.kubernetes.io/instance: usp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-gateway\n        app.kubernetes.io/instance: usp-gateway\n    spec:\n      serviceAccountName: usp-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          # livenessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          # readinessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-gateway\n  labels:\n    helm.sh/chart: usp-gateway-0.1.0\n    app.kubernetes.io/name: usp-gateway\n    app.kubernetes.io/instance: usp-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-gateway\n      app.kubernetes.io/instance: usp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-gateway\n        app.kubernetes.io/instance: usp-gateway\n    spec:\n      serviceAccountName: usp-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          # livenessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          # readinessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-gateway\n  labels:\n    helm.sh/chart: usp-gateway-0.1.0\n    app.kubernetes.io/name: usp-gateway\n    app.kubernetes.io/instance: usp-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-gateway\n      app.kubernetes.io/instance: usp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-gateway\n        app.kubernetes.io/instance: usp-gateway\n    spec:\n      serviceAccountName: usp-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          # livenessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          # readinessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-gateway\n  labels:\n    helm.sh/chart: usp-gateway-0.1.0\n    app.kubernetes.io/name: usp-gateway\n    app.kubernetes.io/instance: usp-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-gateway\n      app.kubernetes.io/instance: usp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-gateway\n        app.kubernetes.io/instance: usp-gateway\n    spec:\n      serviceAccountName: usp-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          # livenessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          # readinessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-gateway\n  labels:\n    helm.sh/chart: usp-gateway-0.1.0\n    app.kubernetes.io/name: usp-gateway\n    app.kubernetes.io/instance: usp-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-gateway\n      app.kubernetes.io/instance: usp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-gateway\n        app.kubernetes.io/instance: usp-gateway\n    spec:\n      serviceAccountName: usp-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          # livenessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          # readinessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-gateway\n  labels:\n    helm.sh/chart: usp-gateway-0.1.0\n    app.kubernetes.io/name: usp-gateway\n    app.kubernetes.io/instance: usp-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-gateway\n      app.kubernetes.io/instance: usp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-gateway\n        app.kubernetes.io/instance: usp-gateway\n    spec:\n      serviceAccountName: usp-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          # livenessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          # readinessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-gateway\n  labels:\n    helm.sh/chart: usp-gateway-0.1.0\n    app.kubernetes.io/name: usp-gateway\n    app.kubernetes.io/instance: usp-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-gateway\n      app.kubernetes.io/instance: usp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-gateway\n        app.kubernetes.io/instance: usp-gateway\n    spec:\n      serviceAccountName: usp-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          # livenessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          # readinessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-gateway\n  labels:\n    helm.sh/chart: usp-gateway-0.1.0\n    app.kubernetes.io/name: usp-gateway\n    app.kubernetes.io/instance: usp-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-gateway\n      app.kubernetes.io/instance: usp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-gateway\n        app.kubernetes.io/instance: usp-gateway\n    spec:\n      serviceAccountName: usp-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          # livenessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          # readinessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-gateway/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-gateway\n  labels:\n    helm.sh/chart: usp-gateway-0.1.0\n    app.kubernetes.io/name: usp-gateway\n    app.kubernetes.io/instance: usp-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-gateway\n      app.kubernetes.io/instance: usp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-gateway\n        app.kubernetes.io/instance: usp-gateway\n    spec:\n      serviceAccountName: usp-gateway\n      securityContext:\n        {}\n      containers:\n        - name: usp-gateway\n          securityContext:\n            {}\n          image: \"artifactory.wrs.com/docker-devstar/usp/api-gateway:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          # livenessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          # readinessProbe:\n          #   httpGet:\n          #     path: /\n          #     port: http\n          resources:\n            {}\n          command: [\"npm\", \"run\",start:testing]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{}\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-gateway/templates/service.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: usp-gateway\n  labels:\n    helm.sh/chart: usp-gateway-0.1.0\n    app.kubernetes.io/name: usp-gateway\n    app.kubernetes.io/instance: usp-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: NodePort\n  ports:\n    - port: 80\n      targetPort: http\n      protocol: TCP\n      name: http\n  selector:\n    app.kubernetes.io/name: usp-gateway\n    app.kubernetes.io/instance: usp-gateway\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-gateway/templates/serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 12, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: usp-gateway\n  labels:\n    helm.sh/chart: usp-gateway-0.1.0\n    app.kubernetes.io/name: usp-gateway\n    app.kubernetes.io/instance: usp-gateway\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/configmaps/vxconfig-releases.yaml"}, "region": {"startLine": 3, "endLine": 10, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vxconfig-available-releases\ndata:\n  AVAILABLE_RELEASES: \"[\n    \\\"2303\\\"\n  ]\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/ingress/gateway.yaml"}, "region": {"startLine": 3, "endLine": 32, "snippet": {"text": "apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: vxbs-gateway\n  labels:\n    io.kompose.service: tls-terminator\n  annotations:\n    cert-manager.io/cluster-issuer: null\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/proxy-connect-timeout: \"360\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: \"360\"\n    nginx.ingress.kubernetes.io/proxy-send-timeout: \"360\"\n    nginx.ingress.kubernetes.io/service-upstream: \"\"\n    nginx.ingress.kubernetes.io/upstream-vhost: \"\"\nspec:\n  rules:\n    - host: \n      http:\n        paths:\n        - backend:\n            service: \n              name: vxbs-gateway\n              port: \n                number: 80\n          pathType: ImplementationSpecific\n\n  tls:\n    - hosts:\n      - \n      secretName:\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/ingress/ui.yaml"}, "region": {"startLine": 3, "endLine": 28, "snippet": {"text": "apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  labels:\n    io.kompose.service: tls-terminator\n  annotations:\n    cert-manager.io/cluster-issuer: null\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/service-upstream: \"\"\n    nginx.ingress.kubernetes.io/upstream-vhost: \"\"\n  name: vxbs-ui\nspec:\n  rules:\n    - host: \n      http:\n        paths:\n        - backend:\n            service: \n              name: vxbs-ui\n              port: \n                number: 80\n          pathType: ImplementationSpecific\n  tls:\n    - hosts:\n      - \n      secretName:\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-gateway-test-connection\"\n  labels:\n     app: \"vxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-gateway-test-connection\"\n  labels:\n     app: \"vxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-gateway-test-connection\"\n  labels:\n     app: \"vxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-gateway-test-connection\"\n  labels:\n     app: \"vxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-gateway-test-connection\"\n  labels:\n     app: \"vxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-gateway-test-connection\"\n  labels:\n     app: \"vxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-gateway-test-connection\"\n  labels:\n     app: \"vxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-gateway-test-connection\"\n  labels:\n     app: \"vxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-gateway-test-connection\"\n  labels:\n     app: \"vxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-gateway-test-connection\"\n  labels:\n     app: \"vxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-gateway-test-connection\"\n  labels:\n     app: \"vxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-gateway-test-connection\"\n  labels:\n     app: \"vxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-gateway-test-connection\"\n  labels:\n     app: \"vxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-gateway-test-connection\"\n  labels:\n     app: \"vxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-gateway-test-connection\"\n  labels:\n     app: \"vxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-gateway-test-connection\"\n  labels:\n     app: \"vxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-gateway-test-connection\"\n  labels:\n     app: \"vxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-gateway-test-connection\"\n  labels:\n     app: \"vxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-gateway-test-connection\"\n  labels:\n     app: \"vxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-ui-test-connection\"\n  labels:\n     app: \"vxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-ui-test-connection\"\n  labels:\n     app: \"vxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-ui-test-connection\"\n  labels:\n     app: \"vxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-ui-test-connection\"\n  labels:\n     app: \"vxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-ui-test-connection\"\n  labels:\n     app: \"vxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-ui-test-connection\"\n  labels:\n     app: \"vxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-ui-test-connection\"\n  labels:\n     app: \"vxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-ui-test-connection\"\n  labels:\n     app: \"vxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-ui-test-connection\"\n  labels:\n     app: \"vxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-ui-test-connection\"\n  labels:\n     app: \"vxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-ui-test-connection\"\n  labels:\n     app: \"vxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-ui-test-connection\"\n  labels:\n     app: \"vxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-ui-test-connection\"\n  labels:\n     app: \"vxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-ui-test-connection\"\n  labels:\n     app: \"vxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-ui-test-connection\"\n  labels:\n     app: \"vxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-ui-test-connection\"\n  labels:\n     app: \"vxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-ui-test-connection\"\n  labels:\n     app: \"vxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-ui-test-connection\"\n  labels:\n     app: \"vxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vxbs-ui-test-connection\"\n  labels:\n     app: \"vxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['vxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: vxbs-init-db\n  namespace: default\n  labels:\n    app: vxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: vxbs-init-db\n      labels:\n        app: vxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"vxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: vxbs-init-db\n  namespace: default\n  labels:\n    app: vxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: vxbs-init-db\n      labels:\n        app: vxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"vxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: vxbs-init-db\n  namespace: default\n  labels:\n    app: vxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: vxbs-init-db\n      labels:\n        app: vxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"vxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: vxbs-init-db\n  namespace: default\n  labels:\n    app: vxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: vxbs-init-db\n      labels:\n        app: vxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"vxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: vxbs-init-db\n  namespace: default\n  labels:\n    app: vxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: vxbs-init-db\n      labels:\n        app: vxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"vxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: vxbs-init-db\n  namespace: default\n  labels:\n    app: vxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: vxbs-init-db\n      labels:\n        app: vxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"vxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: vxbs-init-db\n  namespace: default\n  labels:\n    app: vxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: vxbs-init-db\n      labels:\n        app: vxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"vxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: vxbs-init-db\n  namespace: default\n  labels:\n    app: vxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: vxbs-init-db\n      labels:\n        app: vxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"vxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: vxbs-init-db\n  namespace: default\n  labels:\n    app: vxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: vxbs-init-db\n      labels:\n        app: vxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"vxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: vxbs-init-db\n  namespace: default\n  labels:\n    app: vxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: vxbs-init-db\n      labels:\n        app: vxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"vxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: vxbs-init-db\n  namespace: default\n  labels:\n    app: vxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: vxbs-init-db\n      labels:\n        app: vxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"vxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: vxbs-init-db\n  namespace: default\n  labels:\n    app: vxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: vxbs-init-db\n      labels:\n        app: vxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"vxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: vxbs-init-db\n  namespace: default\n  labels:\n    app: vxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: vxbs-init-db\n      labels:\n        app: vxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"vxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: vxbs-init-db\n  namespace: default\n  labels:\n    app: vxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: vxbs-init-db\n      labels:\n        app: vxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"vxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: vxbs-init-db\n  namespace: default\n  labels:\n    app: vxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: vxbs-init-db\n      labels:\n        app: vxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"vxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: vxbs-init-db\n  namespace: default\n  labels:\n    app: vxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: vxbs-init-db\n      labels:\n        app: vxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"vxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: vxbs-init-db\n  namespace: default\n  labels:\n    app: vxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: vxbs-init-db\n      labels:\n        app: vxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"vxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: vxbs-init-db\n  namespace: default\n  labels:\n    app: vxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: vxbs-init-db\n      labels:\n        app: vxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"vxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/services/gateway.yaml"}, "region": {"startLine": 3, "endLine": 17, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: vxbs-gateway\n  annotations:\n    app.kubernetes.io/name: vxbs-gateway\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: 3000\n      protocol: TCP\n      name: tcp-gateway\n  selector:\n    app: vxbs-gateway\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/services/ui.yaml"}, "region": {"startLine": 3, "endLine": 17, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: vxbs-ui\n  annotations:\n    app.kubernetes.io/name: vxbs-ui \nspec:\n  type: ClusterIP\n  ports:\n    - name: ui\n      port: 80\n      protocol: TCP\n      targetPort: 8080\n  selector:\n    app: vxbs-ui\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 52, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-project\n  labels:\n    app: vxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-project\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 52, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-project\n  labels:\n    app: vxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-project\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 52, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-project\n  labels:\n    app: vxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-project\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 52, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-project\n  labels:\n    app: vxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-project\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 52, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-project\n  labels:\n    app: vxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-project\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 52, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-project\n  labels:\n    app: vxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-project\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 52, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-project\n  labels:\n    app: vxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-project\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 52, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-project\n  labels:\n    app: vxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-project\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 52, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-project\n  labels:\n    app: vxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-project\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 52, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-project\n  labels:\n    app: vxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-project\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 52, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-project\n  labels:\n    app: vxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-project\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 52, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-project\n  labels:\n    app: vxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-project\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 52, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-project\n  labels:\n    app: vxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-project\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 52, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-project\n  labels:\n    app: vxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-project\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 52, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-project\n  labels:\n    app: vxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-project\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-policy\n  labels:\n    app: vxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-policy\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-policy\n  labels:\n    app: vxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-policy\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-policy\n  labels:\n    app: vxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-policy\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-policy\n  labels:\n    app: vxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-policy\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-policy\n  labels:\n    app: vxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-policy\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-policy\n  labels:\n    app: vxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-policy\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-policy\n  labels:\n    app: vxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-policy\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-policy\n  labels:\n    app: vxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-policy\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-policy\n  labels:\n    app: vxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-policy\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-policy\n  labels:\n    app: vxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-policy\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-policy\n  labels:\n    app: vxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-policy\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-policy\n  labels:\n    app: vxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-policy\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-policy\n  labels:\n    app: vxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-policy\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-policy\n  labels:\n    app: vxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-policy\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-policy\n  labels:\n    app: vxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-policy\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-sync\n  labels:\n    app: vxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-sync\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-sync\n  labels:\n    app: vxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-sync\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-sync\n  labels:\n    app: vxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-sync\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-sync\n  labels:\n    app: vxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-sync\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-sync\n  labels:\n    app: vxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-sync\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-sync\n  labels:\n    app: vxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-sync\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-sync\n  labels:\n    app: vxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-sync\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-sync\n  labels:\n    app: vxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-sync\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-sync\n  labels:\n    app: vxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-sync\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-sync\n  labels:\n    app: vxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-sync\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-sync\n  labels:\n    app: vxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-sync\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-sync\n  labels:\n    app: vxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-sync\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-sync\n  labels:\n    app: vxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-sync\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-sync\n  labels:\n    app: vxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-sync\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-sync\n  labels:\n    app: vxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-sync\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-repository\n  labels:\n    app: vxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-repository\n    spec:\n      serviceAccountName: \n      repositoryContext:\n        null\n      containers:\n        - name: repository\n          repositoryContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-repository\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-repository\n  labels:\n    app: vxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-repository\n    spec:\n      serviceAccountName: \n      repositoryContext:\n        null\n      containers:\n        - name: repository\n          repositoryContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-repository\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-repository\n  labels:\n    app: vxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-repository\n    spec:\n      serviceAccountName: \n      repositoryContext:\n        null\n      containers:\n        - name: repository\n          repositoryContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-repository\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-repository\n  labels:\n    app: vxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-repository\n    spec:\n      serviceAccountName: \n      repositoryContext:\n        null\n      containers:\n        - name: repository\n          repositoryContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-repository\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-repository\n  labels:\n    app: vxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-repository\n    spec:\n      serviceAccountName: \n      repositoryContext:\n        null\n      containers:\n        - name: repository\n          repositoryContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-repository\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-repository\n  labels:\n    app: vxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-repository\n    spec:\n      serviceAccountName: \n      repositoryContext:\n        null\n      containers:\n        - name: repository\n          repositoryContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-repository\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-repository\n  labels:\n    app: vxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-repository\n    spec:\n      serviceAccountName: \n      repositoryContext:\n        null\n      containers:\n        - name: repository\n          repositoryContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-repository\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-repository\n  labels:\n    app: vxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-repository\n    spec:\n      serviceAccountName: \n      repositoryContext:\n        null\n      containers:\n        - name: repository\n          repositoryContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-repository\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-repository\n  labels:\n    app: vxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-repository\n    spec:\n      serviceAccountName: \n      repositoryContext:\n        null\n      containers:\n        - name: repository\n          repositoryContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-repository\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-repository\n  labels:\n    app: vxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-repository\n    spec:\n      serviceAccountName: \n      repositoryContext:\n        null\n      containers:\n        - name: repository\n          repositoryContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-repository\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-repository\n  labels:\n    app: vxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-repository\n    spec:\n      serviceAccountName: \n      repositoryContext:\n        null\n      containers:\n        - name: repository\n          repositoryContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-repository\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-repository\n  labels:\n    app: vxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-repository\n    spec:\n      serviceAccountName: \n      repositoryContext:\n        null\n      containers:\n        - name: repository\n          repositoryContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-repository\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-repository\n  labels:\n    app: vxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-repository\n    spec:\n      serviceAccountName: \n      repositoryContext:\n        null\n      containers:\n        - name: repository\n          repositoryContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-repository\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-repository\n  labels:\n    app: vxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-repository\n    spec:\n      serviceAccountName: \n      repositoryContext:\n        null\n      containers:\n        - name: repository\n          repositoryContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-repository\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-repository\n  labels:\n    app: vxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-repository\n    spec:\n      serviceAccountName: \n      repositoryContext:\n        null\n      containers:\n        - name: repository\n          repositoryContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-repository\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-repository\n  labels:\n    app: vxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-repository\n    spec:\n      serviceAccountName: \n      repositoryContext:\n        null\n      containers:\n        - name: repository\n          repositoryContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-repository\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-repository\n  labels:\n    app: vxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-repository\n    spec:\n      serviceAccountName: \n      repositoryContext:\n        null\n      containers:\n        - name: repository\n          repositoryContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-repository\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-gateway\n  labels:\n    app: vxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-gateway\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-gateway\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-gateway\n  labels:\n    app: vxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-gateway\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-gateway\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-gateway\n  labels:\n    app: vxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-gateway\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-gateway\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-gateway\n  labels:\n    app: vxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-gateway\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-gateway\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-gateway\n  labels:\n    app: vxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-gateway\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-gateway\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-gateway\n  labels:\n    app: vxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-gateway\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-gateway\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-gateway\n  labels:\n    app: vxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-gateway\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-gateway\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-gateway\n  labels:\n    app: vxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-gateway\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-gateway\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-gateway\n  labels:\n    app: vxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-gateway\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-gateway\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-gateway\n  labels:\n    app: vxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-gateway\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-gateway\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-gateway\n  labels:\n    app: vxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-gateway\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-gateway\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-gateway\n  labels:\n    app: vxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-gateway\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-gateway\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-gateway\n  labels:\n    app: vxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-gateway\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-gateway\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-gateway\n  labels:\n    app: vxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-gateway\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-gateway\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-gateway\n  labels:\n    app: vxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-gateway\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          envFrom:\n          - configMapRef:\n              name: vxconfig-available-releases\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-gateway\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 38, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-ui\n  labels:\n    app: vxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 38, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-ui\n  labels:\n    app: vxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 38, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-ui\n  labels:\n    app: vxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 38, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-ui\n  labels:\n    app: vxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 38, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-ui\n  labels:\n    app: vxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 38, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-ui\n  labels:\n    app: vxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 38, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-ui\n  labels:\n    app: vxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 38, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-ui\n  labels:\n    app: vxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 38, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-ui\n  labels:\n    app: vxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 38, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-ui\n  labels:\n    app: vxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 38, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-ui\n  labels:\n    app: vxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 38, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-ui\n  labels:\n    app: vxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 38, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-ui\n  labels:\n    app: vxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 38, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-ui\n  labels:\n    app: vxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 38, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-ui\n  labels:\n    app: vxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/artifact.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-artifact\n  labels:\n    app: vxbs-artifact\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-artifact\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-artifact\n    spec:\n      serviceAccountName: \n      artifactContext:\n        null\n      containers:\n        - name: artifact\n          artifactContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-artifact\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/artifact.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-artifact\n  labels:\n    app: vxbs-artifact\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-artifact\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-artifact\n    spec:\n      serviceAccountName: \n      artifactContext:\n        null\n      containers:\n        - name: artifact\n          artifactContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-artifact\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/artifact.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-artifact\n  labels:\n    app: vxbs-artifact\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-artifact\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-artifact\n    spec:\n      serviceAccountName: \n      artifactContext:\n        null\n      containers:\n        - name: artifact\n          artifactContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-artifact\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/artifact.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-artifact\n  labels:\n    app: vxbs-artifact\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-artifact\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-artifact\n    spec:\n      serviceAccountName: \n      artifactContext:\n        null\n      containers:\n        - name: artifact\n          artifactContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-artifact\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/artifact.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-artifact\n  labels:\n    app: vxbs-artifact\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-artifact\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-artifact\n    spec:\n      serviceAccountName: \n      artifactContext:\n        null\n      containers:\n        - name: artifact\n          artifactContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-artifact\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/artifact.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-artifact\n  labels:\n    app: vxbs-artifact\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-artifact\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-artifact\n    spec:\n      serviceAccountName: \n      artifactContext:\n        null\n      containers:\n        - name: artifact\n          artifactContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-artifact\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/artifact.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-artifact\n  labels:\n    app: vxbs-artifact\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-artifact\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-artifact\n    spec:\n      serviceAccountName: \n      artifactContext:\n        null\n      containers:\n        - name: artifact\n          artifactContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-artifact\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/artifact.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-artifact\n  labels:\n    app: vxbs-artifact\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-artifact\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-artifact\n    spec:\n      serviceAccountName: \n      artifactContext:\n        null\n      containers:\n        - name: artifact\n          artifactContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-artifact\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/artifact.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-artifact\n  labels:\n    app: vxbs-artifact\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-artifact\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-artifact\n    spec:\n      serviceAccountName: \n      artifactContext:\n        null\n      containers:\n        - name: artifact\n          artifactContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-artifact\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/artifact.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-artifact\n  labels:\n    app: vxbs-artifact\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-artifact\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-artifact\n    spec:\n      serviceAccountName: \n      artifactContext:\n        null\n      containers:\n        - name: artifact\n          artifactContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-artifact\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/artifact.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-artifact\n  labels:\n    app: vxbs-artifact\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-artifact\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-artifact\n    spec:\n      serviceAccountName: \n      artifactContext:\n        null\n      containers:\n        - name: artifact\n          artifactContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-artifact\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/artifact.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-artifact\n  labels:\n    app: vxbs-artifact\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-artifact\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-artifact\n    spec:\n      serviceAccountName: \n      artifactContext:\n        null\n      containers:\n        - name: artifact\n          artifactContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-artifact\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/artifact.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-artifact\n  labels:\n    app: vxbs-artifact\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-artifact\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-artifact\n    spec:\n      serviceAccountName: \n      artifactContext:\n        null\n      containers:\n        - name: artifact\n          artifactContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-artifact\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/artifact.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-artifact\n  labels:\n    app: vxbs-artifact\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-artifact\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-artifact\n    spec:\n      serviceAccountName: \n      artifactContext:\n        null\n      containers:\n        - name: artifact\n          artifactContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-artifact\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/artifact.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-artifact\n  labels:\n    app: vxbs-artifact\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-artifact\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-artifact\n    spec:\n      serviceAccountName: \n      artifactContext:\n        null\n      containers:\n        - name: artifact\n          artifactContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-artifact\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/artifact.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-artifact\n  labels:\n    app: vxbs-artifact\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-artifact\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-artifact\n    spec:\n      serviceAccountName: \n      artifactContext:\n        null\n      containers:\n        - name: artifact\n          artifactContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-artifact\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/artifact.yaml"}, "region": {"startLine": 3, "endLine": 49, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-artifact\n  labels:\n    app: vxbs-artifact\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-artifact\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-artifact\n    spec:\n      serviceAccountName: \n      artifactContext:\n        null\n      containers:\n        - name: artifact\n          artifactContext:\n            null\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-artifact\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-build\n  labels:\n    app: vxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-build\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-build\n  labels:\n    app: vxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-build\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-build\n  labels:\n    app: vxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-build\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-build\n  labels:\n    app: vxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-build\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-build\n  labels:\n    app: vxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-build\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-build\n  labels:\n    app: vxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-build\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-build\n  labels:\n    app: vxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-build\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-build\n  labels:\n    app: vxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-build\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-build\n  labels:\n    app: vxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-build\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-build\n  labels:\n    app: vxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-build\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-build\n  labels:\n    app: vxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-build\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-build\n  labels:\n    app: vxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-build\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-build\n  labels:\n    app: vxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-build\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-build\n  labels:\n    app: vxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-build\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vxbs-build\n  labels:\n    app: vxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: vxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"vxbs-build\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"artifactMaxRetry\\\":null,\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncArtifactTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseReleaseUrl\\\":\\\"\\\",\\\"defaultBranch\\\":\\\"\\\",\\\"groupName\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"blueApiSuffix\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"accessKey\\\":\\\"\\\",\\\"apiBaseUrl\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-slave-svc.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: redis-slave\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  type: ClusterIP\n  ports:\n    - name: redis\n      port: 6379\n      targetPort: redis\n  selector:\n    app: redis\n    release: redis\n    role: slave\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-master-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 136, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: master\n  serviceName: redis-headless\n  template:\n    metadata:\n      labels:\n        app: redis\n        chart: redis-10.6.19\n        release: redis\n        role: master\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 6261420001a03a0850e8785e74c6441a727f2df7dc684102383c1b1ff87c13da\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/master.conf\")\n              /run.sh ${ARGS[@]}\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: master\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_PORT\n              value: \"6379\"\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n              subPath: \n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc/\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: master\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-master-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 136, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: master\n  serviceName: redis-headless\n  template:\n    metadata:\n      labels:\n        app: redis\n        chart: redis-10.6.19\n        release: redis\n        role: master\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 6261420001a03a0850e8785e74c6441a727f2df7dc684102383c1b1ff87c13da\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/master.conf\")\n              /run.sh ${ARGS[@]}\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: master\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_PORT\n              value: \"6379\"\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n              subPath: \n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc/\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: master\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-master-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 136, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: master\n  serviceName: redis-headless\n  template:\n    metadata:\n      labels:\n        app: redis\n        chart: redis-10.6.19\n        release: redis\n        role: master\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 6261420001a03a0850e8785e74c6441a727f2df7dc684102383c1b1ff87c13da\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/master.conf\")\n              /run.sh ${ARGS[@]}\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: master\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_PORT\n              value: \"6379\"\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n              subPath: \n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc/\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: master\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-master-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 136, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: master\n  serviceName: redis-headless\n  template:\n    metadata:\n      labels:\n        app: redis\n        chart: redis-10.6.19\n        release: redis\n        role: master\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 6261420001a03a0850e8785e74c6441a727f2df7dc684102383c1b1ff87c13da\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/master.conf\")\n              /run.sh ${ARGS[@]}\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: master\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_PORT\n              value: \"6379\"\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n              subPath: \n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc/\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: master\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-master-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 136, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: master\n  serviceName: redis-headless\n  template:\n    metadata:\n      labels:\n        app: redis\n        chart: redis-10.6.19\n        release: redis\n        role: master\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 6261420001a03a0850e8785e74c6441a727f2df7dc684102383c1b1ff87c13da\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/master.conf\")\n              /run.sh ${ARGS[@]}\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: master\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_PORT\n              value: \"6379\"\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n              subPath: \n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc/\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: master\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-master-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 136, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: master\n  serviceName: redis-headless\n  template:\n    metadata:\n      labels:\n        app: redis\n        chart: redis-10.6.19\n        release: redis\n        role: master\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 6261420001a03a0850e8785e74c6441a727f2df7dc684102383c1b1ff87c13da\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/master.conf\")\n              /run.sh ${ARGS[@]}\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: master\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_PORT\n              value: \"6379\"\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n              subPath: \n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc/\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: master\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-master-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 136, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: master\n  serviceName: redis-headless\n  template:\n    metadata:\n      labels:\n        app: redis\n        chart: redis-10.6.19\n        release: redis\n        role: master\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 6261420001a03a0850e8785e74c6441a727f2df7dc684102383c1b1ff87c13da\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/master.conf\")\n              /run.sh ${ARGS[@]}\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: master\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_PORT\n              value: \"6379\"\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n              subPath: \n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc/\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: master\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-master-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 136, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: master\n  serviceName: redis-headless\n  template:\n    metadata:\n      labels:\n        app: redis\n        chart: redis-10.6.19\n        release: redis\n        role: master\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 6261420001a03a0850e8785e74c6441a727f2df7dc684102383c1b1ff87c13da\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/master.conf\")\n              /run.sh ${ARGS[@]}\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: master\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_PORT\n              value: \"6379\"\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n              subPath: \n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc/\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: master\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-master-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 136, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: master\n  serviceName: redis-headless\n  template:\n    metadata:\n      labels:\n        app: redis\n        chart: redis-10.6.19\n        release: redis\n        role: master\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 6261420001a03a0850e8785e74c6441a727f2df7dc684102383c1b1ff87c13da\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/master.conf\")\n              /run.sh ${ARGS[@]}\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: master\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_PORT\n              value: \"6379\"\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n              subPath: \n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc/\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: master\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_35", "ruleIndex": 6, "level": "error", "attachments": [], "message": {"text": "Prefer using secrets as files over secrets as environment variables"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-master-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 136, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: master\n  serviceName: redis-headless\n  template:\n    metadata:\n      labels:\n        app: redis\n        chart: redis-10.6.19\n        release: redis\n        role: master\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 6261420001a03a0850e8785e74c6441a727f2df7dc684102383c1b1ff87c13da\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/master.conf\")\n              /run.sh ${ARGS[@]}\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: master\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_PORT\n              value: \"6379\"\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n              subPath: \n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc/\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: master\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-master-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 136, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: master\n  serviceName: redis-headless\n  template:\n    metadata:\n      labels:\n        app: redis\n        chart: redis-10.6.19\n        release: redis\n        role: master\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 6261420001a03a0850e8785e74c6441a727f2df7dc684102383c1b1ff87c13da\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/master.conf\")\n              /run.sh ${ARGS[@]}\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: master\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_PORT\n              value: \"6379\"\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n              subPath: \n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc/\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: master\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-master-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 136, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: master\n  serviceName: redis-headless\n  template:\n    metadata:\n      labels:\n        app: redis\n        chart: redis-10.6.19\n        release: redis\n        role: master\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 6261420001a03a0850e8785e74c6441a727f2df7dc684102383c1b1ff87c13da\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/master.conf\")\n              /run.sh ${ARGS[@]}\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: master\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_PORT\n              value: \"6379\"\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n              subPath: \n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc/\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: master\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-master-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 136, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: master\n  serviceName: redis-headless\n  template:\n    metadata:\n      labels:\n        app: redis\n        chart: redis-10.6.19\n        release: redis\n        role: master\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 6261420001a03a0850e8785e74c6441a727f2df7dc684102383c1b1ff87c13da\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/master.conf\")\n              /run.sh ${ARGS[@]}\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: master\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_PORT\n              value: \"6379\"\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n              subPath: \n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc/\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: master\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-master-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 136, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: master\n  serviceName: redis-headless\n  template:\n    metadata:\n      labels:\n        app: redis\n        chart: redis-10.6.19\n        release: redis\n        role: master\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 6261420001a03a0850e8785e74c6441a727f2df7dc684102383c1b1ff87c13da\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/master.conf\")\n              /run.sh ${ARGS[@]}\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: master\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_PORT\n              value: \"6379\"\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n              subPath: \n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc/\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: master\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-master-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 136, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: master\n  serviceName: redis-headless\n  template:\n    metadata:\n      labels:\n        app: redis\n        chart: redis-10.6.19\n        release: redis\n        role: master\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 6261420001a03a0850e8785e74c6441a727f2df7dc684102383c1b1ff87c13da\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/master.conf\")\n              /run.sh ${ARGS[@]}\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: master\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_PORT\n              value: \"6379\"\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n              subPath: \n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc/\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: master\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/configmap-scripts.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: redis-scripts\n  namespace: \"default\"\n  labels:\n    app: redis\n    chart: redis-11.3.4\n    heritage: Helm\n    release: redis\ndata:\n  start-master.sh: |\n    #!/bin/bash\n    useradd redis\n    chown -R redis /data\n    if [[ -n $REDIS_PASSWORD_FILE ]]; then\n      password_aux=`cat ${REDIS_PASSWORD_FILE}`\n      export REDIS_PASSWORD=$password_aux\n    fi\n    if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then\n      cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf\n    fi\n    if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n      cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n    fi\n    ARGS=(\"--port\" \"${REDIS_PORT}\")\n    ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n    ARGS+=(\"--masterauth\" \"${REDIS_PASSWORD}\")\n    ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n    ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/master.conf\")\n    exec /run.sh \"${ARGS[@]}\"\n  start-slave.sh: |\n    #!/bin/bash\n    useradd redis\n    chown -R redis /data\n    if [[ -n $REDIS_PASSWORD_FILE ]]; then\n      password_aux=`cat ${REDIS_PASSWORD_FILE}`\n      export REDIS_PASSWORD=$password_aux\n    fi\n    if [[ -n $REDIS_MASTER_PASSWORD_FILE ]]; then\n      password_aux=`cat ${REDIS_MASTER_PASSWORD_FILE}`\n      export REDIS_MASTER_PASSWORD=$password_aux\n    fi\n    if [[ ! -f /opt/bitnami/redis/etc/replica.conf ]];then\n      cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf\n    fi\n    if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n      cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n    fi\n    ARGS=(\"--port\" \"${REDIS_PORT}\")\n    ARGS+=(\"--slaveof\" \"${REDIS_MASTER_HOST}\" \"${REDIS_MASTER_PORT_NUMBER}\")\n    ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n    ARGS+=(\"--masterauth\" \"${REDIS_MASTER_PASSWORD}\")\n    ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n    ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/replica.conf\")\n    exec /run.sh \"${ARGS[@]}\"\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-slave-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 152, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-slave\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  replicas: 2\n  serviceName: redis-headless\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: slave\n  template:\n    metadata:\n      labels:\n        app: redis\n        release: redis\n        chart: redis-10.6.19\n        role: slave\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 76cc33f4e09fbb49d9a8e8356718b382f61d7425d5a47ba4bb1bf5fbc56270f3\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ -n $REDIS_MASTER_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_MASTER_PASSWORD_FILE}`\n                export REDIS_MASTER_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/replica.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--slaveof\" \"${REDIS_MASTER_HOST}\" \"${REDIS_MASTER_PORT_NUMBER}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_MASTER_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/replica.conf\")\n              /run.sh \"${ARGS[@]}\"\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: slave\n            - name: REDIS_MASTER_HOST\n              value: redis-master-0.redis-headless.default.svc.cluster.local\n            - name: REDIS_PORT\n              value: \"6379\"\n            - name: REDIS_MASTER_PORT_NUMBER\n              value: \"6379\"\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_MASTER_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local_and_master.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 10\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local_and_master.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: sentinel-tmp-conf\n          emptyDir: {}\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: slave\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-slave-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 152, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-slave\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  replicas: 2\n  serviceName: redis-headless\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: slave\n  template:\n    metadata:\n      labels:\n        app: redis\n        release: redis\n        chart: redis-10.6.19\n        role: slave\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 76cc33f4e09fbb49d9a8e8356718b382f61d7425d5a47ba4bb1bf5fbc56270f3\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ -n $REDIS_MASTER_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_MASTER_PASSWORD_FILE}`\n                export REDIS_MASTER_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/replica.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--slaveof\" \"${REDIS_MASTER_HOST}\" \"${REDIS_MASTER_PORT_NUMBER}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_MASTER_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/replica.conf\")\n              /run.sh \"${ARGS[@]}\"\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: slave\n            - name: REDIS_MASTER_HOST\n              value: redis-master-0.redis-headless.default.svc.cluster.local\n            - name: REDIS_PORT\n              value: \"6379\"\n            - name: REDIS_MASTER_PORT_NUMBER\n              value: \"6379\"\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_MASTER_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local_and_master.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 10\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local_and_master.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: sentinel-tmp-conf\n          emptyDir: {}\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: slave\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-slave-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 152, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-slave\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  replicas: 2\n  serviceName: redis-headless\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: slave\n  template:\n    metadata:\n      labels:\n        app: redis\n        release: redis\n        chart: redis-10.6.19\n        role: slave\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 76cc33f4e09fbb49d9a8e8356718b382f61d7425d5a47ba4bb1bf5fbc56270f3\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ -n $REDIS_MASTER_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_MASTER_PASSWORD_FILE}`\n                export REDIS_MASTER_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/replica.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--slaveof\" \"${REDIS_MASTER_HOST}\" \"${REDIS_MASTER_PORT_NUMBER}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_MASTER_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/replica.conf\")\n              /run.sh \"${ARGS[@]}\"\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: slave\n            - name: REDIS_MASTER_HOST\n              value: redis-master-0.redis-headless.default.svc.cluster.local\n            - name: REDIS_PORT\n              value: \"6379\"\n            - name: REDIS_MASTER_PORT_NUMBER\n              value: \"6379\"\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_MASTER_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local_and_master.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 10\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local_and_master.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: sentinel-tmp-conf\n          emptyDir: {}\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: slave\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-slave-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 152, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-slave\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  replicas: 2\n  serviceName: redis-headless\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: slave\n  template:\n    metadata:\n      labels:\n        app: redis\n        release: redis\n        chart: redis-10.6.19\n        role: slave\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 76cc33f4e09fbb49d9a8e8356718b382f61d7425d5a47ba4bb1bf5fbc56270f3\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ -n $REDIS_MASTER_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_MASTER_PASSWORD_FILE}`\n                export REDIS_MASTER_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/replica.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--slaveof\" \"${REDIS_MASTER_HOST}\" \"${REDIS_MASTER_PORT_NUMBER}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_MASTER_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/replica.conf\")\n              /run.sh \"${ARGS[@]}\"\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: slave\n            - name: REDIS_MASTER_HOST\n              value: redis-master-0.redis-headless.default.svc.cluster.local\n            - name: REDIS_PORT\n              value: \"6379\"\n            - name: REDIS_MASTER_PORT_NUMBER\n              value: \"6379\"\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_MASTER_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local_and_master.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 10\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local_and_master.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: sentinel-tmp-conf\n          emptyDir: {}\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: slave\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-slave-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 152, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-slave\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  replicas: 2\n  serviceName: redis-headless\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: slave\n  template:\n    metadata:\n      labels:\n        app: redis\n        release: redis\n        chart: redis-10.6.19\n        role: slave\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 76cc33f4e09fbb49d9a8e8356718b382f61d7425d5a47ba4bb1bf5fbc56270f3\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ -n $REDIS_MASTER_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_MASTER_PASSWORD_FILE}`\n                export REDIS_MASTER_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/replica.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--slaveof\" \"${REDIS_MASTER_HOST}\" \"${REDIS_MASTER_PORT_NUMBER}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_MASTER_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/replica.conf\")\n              /run.sh \"${ARGS[@]}\"\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: slave\n            - name: REDIS_MASTER_HOST\n              value: redis-master-0.redis-headless.default.svc.cluster.local\n            - name: REDIS_PORT\n              value: \"6379\"\n            - name: REDIS_MASTER_PORT_NUMBER\n              value: \"6379\"\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_MASTER_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local_and_master.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 10\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local_and_master.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: sentinel-tmp-conf\n          emptyDir: {}\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: slave\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-slave-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 152, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-slave\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  replicas: 2\n  serviceName: redis-headless\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: slave\n  template:\n    metadata:\n      labels:\n        app: redis\n        release: redis\n        chart: redis-10.6.19\n        role: slave\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 76cc33f4e09fbb49d9a8e8356718b382f61d7425d5a47ba4bb1bf5fbc56270f3\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ -n $REDIS_MASTER_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_MASTER_PASSWORD_FILE}`\n                export REDIS_MASTER_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/replica.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--slaveof\" \"${REDIS_MASTER_HOST}\" \"${REDIS_MASTER_PORT_NUMBER}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_MASTER_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/replica.conf\")\n              /run.sh \"${ARGS[@]}\"\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: slave\n            - name: REDIS_MASTER_HOST\n              value: redis-master-0.redis-headless.default.svc.cluster.local\n            - name: REDIS_PORT\n              value: \"6379\"\n            - name: REDIS_MASTER_PORT_NUMBER\n              value: \"6379\"\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_MASTER_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local_and_master.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 10\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local_and_master.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: sentinel-tmp-conf\n          emptyDir: {}\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: slave\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-slave-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 152, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-slave\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  replicas: 2\n  serviceName: redis-headless\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: slave\n  template:\n    metadata:\n      labels:\n        app: redis\n        release: redis\n        chart: redis-10.6.19\n        role: slave\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 76cc33f4e09fbb49d9a8e8356718b382f61d7425d5a47ba4bb1bf5fbc56270f3\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ -n $REDIS_MASTER_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_MASTER_PASSWORD_FILE}`\n                export REDIS_MASTER_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/replica.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--slaveof\" \"${REDIS_MASTER_HOST}\" \"${REDIS_MASTER_PORT_NUMBER}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_MASTER_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/replica.conf\")\n              /run.sh \"${ARGS[@]}\"\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: slave\n            - name: REDIS_MASTER_HOST\n              value: redis-master-0.redis-headless.default.svc.cluster.local\n            - name: REDIS_PORT\n              value: \"6379\"\n            - name: REDIS_MASTER_PORT_NUMBER\n              value: \"6379\"\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_MASTER_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local_and_master.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 10\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local_and_master.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: sentinel-tmp-conf\n          emptyDir: {}\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: slave\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-slave-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 152, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-slave\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  replicas: 2\n  serviceName: redis-headless\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: slave\n  template:\n    metadata:\n      labels:\n        app: redis\n        release: redis\n        chart: redis-10.6.19\n        role: slave\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 76cc33f4e09fbb49d9a8e8356718b382f61d7425d5a47ba4bb1bf5fbc56270f3\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ -n $REDIS_MASTER_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_MASTER_PASSWORD_FILE}`\n                export REDIS_MASTER_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/replica.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--slaveof\" \"${REDIS_MASTER_HOST}\" \"${REDIS_MASTER_PORT_NUMBER}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_MASTER_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/replica.conf\")\n              /run.sh \"${ARGS[@]}\"\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: slave\n            - name: REDIS_MASTER_HOST\n              value: redis-master-0.redis-headless.default.svc.cluster.local\n            - name: REDIS_PORT\n              value: \"6379\"\n            - name: REDIS_MASTER_PORT_NUMBER\n              value: \"6379\"\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_MASTER_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local_and_master.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 10\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local_and_master.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: sentinel-tmp-conf\n          emptyDir: {}\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: slave\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-slave-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 152, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-slave\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  replicas: 2\n  serviceName: redis-headless\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: slave\n  template:\n    metadata:\n      labels:\n        app: redis\n        release: redis\n        chart: redis-10.6.19\n        role: slave\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 76cc33f4e09fbb49d9a8e8356718b382f61d7425d5a47ba4bb1bf5fbc56270f3\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ -n $REDIS_MASTER_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_MASTER_PASSWORD_FILE}`\n                export REDIS_MASTER_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/replica.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--slaveof\" \"${REDIS_MASTER_HOST}\" \"${REDIS_MASTER_PORT_NUMBER}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_MASTER_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/replica.conf\")\n              /run.sh \"${ARGS[@]}\"\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: slave\n            - name: REDIS_MASTER_HOST\n              value: redis-master-0.redis-headless.default.svc.cluster.local\n            - name: REDIS_PORT\n              value: \"6379\"\n            - name: REDIS_MASTER_PORT_NUMBER\n              value: \"6379\"\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_MASTER_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local_and_master.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 10\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local_and_master.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: sentinel-tmp-conf\n          emptyDir: {}\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: slave\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_35", "ruleIndex": 6, "level": "error", "attachments": [], "message": {"text": "Prefer using secrets as files over secrets as environment variables"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-slave-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 152, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-slave\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  replicas: 2\n  serviceName: redis-headless\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: slave\n  template:\n    metadata:\n      labels:\n        app: redis\n        release: redis\n        chart: redis-10.6.19\n        role: slave\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 76cc33f4e09fbb49d9a8e8356718b382f61d7425d5a47ba4bb1bf5fbc56270f3\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ -n $REDIS_MASTER_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_MASTER_PASSWORD_FILE}`\n                export REDIS_MASTER_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/replica.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--slaveof\" \"${REDIS_MASTER_HOST}\" \"${REDIS_MASTER_PORT_NUMBER}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_MASTER_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/replica.conf\")\n              /run.sh \"${ARGS[@]}\"\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: slave\n            - name: REDIS_MASTER_HOST\n              value: redis-master-0.redis-headless.default.svc.cluster.local\n            - name: REDIS_PORT\n              value: \"6379\"\n            - name: REDIS_MASTER_PORT_NUMBER\n              value: \"6379\"\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_MASTER_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local_and_master.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 10\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local_and_master.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: sentinel-tmp-conf\n          emptyDir: {}\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: slave\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-slave-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 152, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-slave\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  replicas: 2\n  serviceName: redis-headless\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: slave\n  template:\n    metadata:\n      labels:\n        app: redis\n        release: redis\n        chart: redis-10.6.19\n        role: slave\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 76cc33f4e09fbb49d9a8e8356718b382f61d7425d5a47ba4bb1bf5fbc56270f3\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ -n $REDIS_MASTER_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_MASTER_PASSWORD_FILE}`\n                export REDIS_MASTER_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/replica.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--slaveof\" \"${REDIS_MASTER_HOST}\" \"${REDIS_MASTER_PORT_NUMBER}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_MASTER_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/replica.conf\")\n              /run.sh \"${ARGS[@]}\"\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: slave\n            - name: REDIS_MASTER_HOST\n              value: redis-master-0.redis-headless.default.svc.cluster.local\n            - name: REDIS_PORT\n              value: \"6379\"\n            - name: REDIS_MASTER_PORT_NUMBER\n              value: \"6379\"\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_MASTER_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local_and_master.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 10\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local_and_master.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: sentinel-tmp-conf\n          emptyDir: {}\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: slave\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-slave-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 152, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-slave\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  replicas: 2\n  serviceName: redis-headless\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: slave\n  template:\n    metadata:\n      labels:\n        app: redis\n        release: redis\n        chart: redis-10.6.19\n        role: slave\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 76cc33f4e09fbb49d9a8e8356718b382f61d7425d5a47ba4bb1bf5fbc56270f3\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ -n $REDIS_MASTER_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_MASTER_PASSWORD_FILE}`\n                export REDIS_MASTER_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/replica.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--slaveof\" \"${REDIS_MASTER_HOST}\" \"${REDIS_MASTER_PORT_NUMBER}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_MASTER_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/replica.conf\")\n              /run.sh \"${ARGS[@]}\"\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: slave\n            - name: REDIS_MASTER_HOST\n              value: redis-master-0.redis-headless.default.svc.cluster.local\n            - name: REDIS_PORT\n              value: \"6379\"\n            - name: REDIS_MASTER_PORT_NUMBER\n              value: \"6379\"\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_MASTER_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local_and_master.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 10\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local_and_master.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: sentinel-tmp-conf\n          emptyDir: {}\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: slave\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-slave-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 152, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-slave\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  replicas: 2\n  serviceName: redis-headless\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: slave\n  template:\n    metadata:\n      labels:\n        app: redis\n        release: redis\n        chart: redis-10.6.19\n        role: slave\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 76cc33f4e09fbb49d9a8e8356718b382f61d7425d5a47ba4bb1bf5fbc56270f3\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ -n $REDIS_MASTER_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_MASTER_PASSWORD_FILE}`\n                export REDIS_MASTER_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/replica.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--slaveof\" \"${REDIS_MASTER_HOST}\" \"${REDIS_MASTER_PORT_NUMBER}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_MASTER_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/replica.conf\")\n              /run.sh \"${ARGS[@]}\"\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: slave\n            - name: REDIS_MASTER_HOST\n              value: redis-master-0.redis-headless.default.svc.cluster.local\n            - name: REDIS_PORT\n              value: \"6379\"\n            - name: REDIS_MASTER_PORT_NUMBER\n              value: \"6379\"\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_MASTER_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local_and_master.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 10\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local_and_master.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: sentinel-tmp-conf\n          emptyDir: {}\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: slave\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-slave-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 152, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-slave\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  replicas: 2\n  serviceName: redis-headless\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: slave\n  template:\n    metadata:\n      labels:\n        app: redis\n        release: redis\n        chart: redis-10.6.19\n        role: slave\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 76cc33f4e09fbb49d9a8e8356718b382f61d7425d5a47ba4bb1bf5fbc56270f3\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ -n $REDIS_MASTER_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_MASTER_PASSWORD_FILE}`\n                export REDIS_MASTER_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/replica.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--slaveof\" \"${REDIS_MASTER_HOST}\" \"${REDIS_MASTER_PORT_NUMBER}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_MASTER_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/replica.conf\")\n              /run.sh \"${ARGS[@]}\"\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: slave\n            - name: REDIS_MASTER_HOST\n              value: redis-master-0.redis-headless.default.svc.cluster.local\n            - name: REDIS_PORT\n              value: \"6379\"\n            - name: REDIS_MASTER_PORT_NUMBER\n              value: \"6379\"\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_MASTER_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local_and_master.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 10\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local_and_master.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: sentinel-tmp-conf\n          emptyDir: {}\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: slave\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-slave-statefulset.yaml"}, "region": {"startLine": 3, "endLine": 152, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-slave\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  replicas: 2\n  serviceName: redis-headless\n  selector:\n    matchLabels:\n      app: redis\n      release: redis\n      role: slave\n  template:\n    metadata:\n      labels:\n        app: redis\n        release: redis\n        chart: redis-10.6.19\n        role: slave\n      annotations:\n        checksum/health: a397655b3bd6a7ae0878fa9d7ec81de78ad3cd5c00545a4390d34d075ef7d9f7\n        checksum/configmap: c1260261670c126167781d5a5bc27cd8ab3943b0c277ac36a587ecf87fe05bcf\n        checksum/secret: 76cc33f4e09fbb49d9a8e8356718b382f61d7425d5a47ba4bb1bf5fbc56270f3\n    spec:\n      \n      securityContext:\n        fsGroup: 1001\n      serviceAccountName: default\n      containers:\n        - name: redis\n          image: docker.io/bitnami/redis:6.0.5-debian-10-r0\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsUser: 1001\n          command:\n            - /bin/bash\n            - -c\n            - |\n              if [[ -n $REDIS_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_PASSWORD_FILE}`\n                export REDIS_PASSWORD=$password_aux\n              fi\n              if [[ -n $REDIS_MASTER_PASSWORD_FILE ]]; then\n                password_aux=`cat ${REDIS_MASTER_PASSWORD_FILE}`\n                export REDIS_MASTER_PASSWORD=$password_aux\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/replica.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf\n              fi\n              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then\n                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              ARGS=(\"--port\" \"${REDIS_PORT}\")\n              ARGS+=(\"--slaveof\" \"${REDIS_MASTER_HOST}\" \"${REDIS_MASTER_PORT_NUMBER}\")\n              ARGS+=(\"--requirepass\" \"${REDIS_PASSWORD}\")\n              ARGS+=(\"--masterauth\" \"${REDIS_MASTER_PASSWORD}\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/redis.conf\")\n              ARGS+=(\"--include\" \"/opt/bitnami/redis/etc/replica.conf\")\n              /run.sh \"${ARGS[@]}\"\n          env:\n            - name: REDIS_REPLICATION_MODE\n              value: slave\n            - name: REDIS_MASTER_HOST\n              value: redis-master-0.redis-headless.default.svc.cluster.local\n            - name: REDIS_PORT\n              value: \"6379\"\n            - name: REDIS_MASTER_PORT_NUMBER\n              value: \"6379\"\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n            - name: REDIS_MASTER_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis\n                  key: redis-password\n          ports:\n            - name: redis\n              containerPort: 6379\n          livenessProbe:\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            timeoutSeconds: 5\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_liveness_local_and_master.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 10\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /health/ping_readiness_local_and_master.sh 5\n          resources:\n            null\n          volumeMounts:\n            - name: health\n              mountPath: /health\n            - name: redis-data\n              mountPath: /data\n            - name: config\n              mountPath: /opt/bitnami/redis/mounted-etc\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc\n      volumes:\n        - name: health\n          configMap:\n            name: redis-health\n            defaultMode: 0755\n        - name: config\n          configMap:\n            name: redis\n        - name: sentinel-tmp-conf\n          emptyDir: {}\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app: redis\n          release: redis\n          heritage: Helm\n          component: slave\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n        \n        selector:\n  updateStrategy:\n    type: RollingUpdate\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/redis-master-svc.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  type: ClusterIP\n  ports:\n    - name: redis\n      port: 6379\n      targetPort: redis\n  selector:\n    app: redis\n    release: redis\n    role: master\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/headless-svc.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: redis-headless\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: redis\n    heritage: Helm\nspec:\n  type: ClusterIP\n  clusterIP: None\n  ports:\n    - name: redis\n      port: 6379\n      targetPort: redis\n  selector:\n    app: redis\n    release: redis\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/configmap.yaml"}, "region": {"startLine": 3, "endLine": 28, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: redis\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    heritage: Helm\n    release: redis\ndata:\n  redis.conf: |-\n    # User-supplied configuration:\n    # Enable AOF https://redis.io/topics/persistence#append-only-file\n    appendonly yes\n    # Disable RDB persistence, AOF persistence already enabled.\n    save \"\"\n  master.conf: |-\n    dir /data\n    rename-command FLUSHDB \"\"\n    rename-command FLUSHALL \"\"\n  replica.conf: |-\n    dir /data\n    slave-read-only yes\n    rename-command FLUSHDB \"\"\n    rename-command FLUSHALL \"\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/secret.yaml"}, "region": {"startLine": 3, "endLine": 15, "snippet": {"text": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: redis\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    release: \"redis\"\n    heritage: \"Helm\"\ntype: Opaque\ndata:\n  redis-password: \"WWMxR2J2UVhaMg==\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/redis/templates/health-configmap.yaml"}, "region": {"startLine": 3, "endLine": 85, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: redis-health\n  namespace: default\n  labels:\n    app: redis\n    chart: redis-10.6.19\n    heritage: Helm\n    release: redis\ndata:\n  ping_readiness_local.sh: |-\n    #!/bin/bash\n    no_auth_warning=$([[ \"$(redis-cli --version)\" =~ (redis-cli 5.*) ]] && echo --no-auth-warning)\n    response=$(\n      timeout -s 3 $1 \\\n      redis-cli \\\n        -a $REDIS_PASSWORD $no_auth_warning \\\n        -h localhost \\\n        -p $REDIS_PORT \\\n        ping\n    )\n    if [ \"$response\" != \"PONG\" ]; then\n      echo \"$response\"\n      exit 1\n    fi\n  ping_liveness_local.sh: |-\n    #!/bin/bash\n    no_auth_warning=$([[ \"$(redis-cli --version)\" =~ (redis-cli 5.*) ]] && echo --no-auth-warning)\n    response=$(\n      timeout -s 3 $1 \\\n      redis-cli \\\n        -a $REDIS_PASSWORD $no_auth_warning \\\n        -h localhost \\\n        -p $REDIS_PORT \\\n        ping\n    )\n    if [ \"$response\" != \"PONG\" ] && [ \"$response\" != \"LOADING Redis is loading the dataset in memory\" ]; then\n      echo \"$response\"\n      exit 1\n    fi\n  ping_readiness_master.sh: |-\n    #!/bin/bash\n    no_auth_warning=$([[ \"$(redis-cli --version)\" =~ (redis-cli 5.*) ]] && echo --no-auth-warning)\n     response=$(\n      timeout -s 3 $1 \\\n      redis-cli \\\n        -a $REDIS_MASTER_PASSWORD $no_auth_warning \\\n        -h $REDIS_MASTER_HOST \\\n        -p $REDIS_MASTER_PORT_NUMBER \\\n        ping\n    )\n    if [ \"$response\" != \"PONG\" ]; then\n      echo \"$response\"\n      exit 1\n    fi\n  ping_liveness_master.sh: |-\n    #!/bin/bash\n    no_auth_warning=$([[ \"$(redis-cli --version)\" =~ (redis-cli 5.*) ]] && echo --no-auth-warning)\n    response=$(\n      timeout -s 3 $1 \\\n      redis-cli \\\n        -a $REDIS_MASTER_PASSWORD $no_auth_warning \\\n        -h $REDIS_MASTER_HOST \\\n        -p $REDIS_MASTER_PORT_NUMBER \\\n        ping\n    )\n    if [ \"$response\" != \"PONG\" ] && [ \"$response\" != \"LOADING Redis is loading the dataset in memory\" ]; then\n      echo \"$response\"\n      exit 1\n    fi\n  ping_readiness_local_and_master.sh: |-\n    script_dir=\"$(dirname \"$0\")\"\n    exit_status=0\n    \"$script_dir/ping_readiness_local.sh\" $1 || exit_status=$?\n    \"$script_dir/ping_readiness_master.sh\" $1 || exit_status=$?\n    exit $exit_status\n  ping_liveness_local_and_master.sh: |-\n    script_dir=\"$(dirname \"$0\")\"\n    exit_status=0\n    \"$script_dir/ping_liveness_local.sh\" $1 || exit_status=$?\n    \"$script_dir/ping_liveness_master.sh\" $1 || exit_status=$?\n    exit $exit_status\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/oauth2-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy    \n    helm.sh/chart: oauth2-proxy-5.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: oauth2-proxy\n    app.kubernetes.io/version: \"7.2.0\"\n  name: oauth2-proxy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: oauth2-proxy\n  template:\n    metadata:\n      annotations:\n        checksum/config: bffba9ea1ed9347873dc113026d3d4c0645b1ced9180fc723537f917425a986f\n        checksum/config-emails: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 911c11690be7e5003c7ad8d9258639647e0ce1c4a11e07fbc2b10b13762f6d75\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n      labels:\n        app: oauth2-proxy        \n        helm.sh/chart: oauth2-proxy-5.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: oauth2-proxy\n        app.kubernetes.io/version: \"7.2.0\"\n    spec:\n      serviceAccountName: oauth2-proxy\n      containers:\n      - name: oauth2-proxy\n        image: \"quay.io/oauth2-proxy/oauth2-proxy:v7.2.0\"\n        imagePullPolicy: IfNotPresent\n        args:\n          - --http-address=0.0.0.0:4180\n          - --metrics-address=0.0.0.0:44180\n          - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: cookie-secret\n        ports:\n          - containerPort: 4180\n            name: http\n            protocol: TCP\n          - containerPort: 44180\n            protocol: TCP\n            name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n          successThreshold: 1\n          periodSeconds: 10\n        resources:\n          {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy\n          name: configmain\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: oauth2-proxy\n        name: configmain\n      tolerations:\n        []\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/oauth2-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy    \n    helm.sh/chart: oauth2-proxy-5.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: oauth2-proxy\n    app.kubernetes.io/version: \"7.2.0\"\n  name: oauth2-proxy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: oauth2-proxy\n  template:\n    metadata:\n      annotations:\n        checksum/config: bffba9ea1ed9347873dc113026d3d4c0645b1ced9180fc723537f917425a986f\n        checksum/config-emails: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 911c11690be7e5003c7ad8d9258639647e0ce1c4a11e07fbc2b10b13762f6d75\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n      labels:\n        app: oauth2-proxy        \n        helm.sh/chart: oauth2-proxy-5.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: oauth2-proxy\n        app.kubernetes.io/version: \"7.2.0\"\n    spec:\n      serviceAccountName: oauth2-proxy\n      containers:\n      - name: oauth2-proxy\n        image: \"quay.io/oauth2-proxy/oauth2-proxy:v7.2.0\"\n        imagePullPolicy: IfNotPresent\n        args:\n          - --http-address=0.0.0.0:4180\n          - --metrics-address=0.0.0.0:44180\n          - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: cookie-secret\n        ports:\n          - containerPort: 4180\n            name: http\n            protocol: TCP\n          - containerPort: 44180\n            protocol: TCP\n            name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n          successThreshold: 1\n          periodSeconds: 10\n        resources:\n          {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy\n          name: configmain\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: oauth2-proxy\n        name: configmain\n      tolerations:\n        []\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/oauth2-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy    \n    helm.sh/chart: oauth2-proxy-5.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: oauth2-proxy\n    app.kubernetes.io/version: \"7.2.0\"\n  name: oauth2-proxy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: oauth2-proxy\n  template:\n    metadata:\n      annotations:\n        checksum/config: bffba9ea1ed9347873dc113026d3d4c0645b1ced9180fc723537f917425a986f\n        checksum/config-emails: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 911c11690be7e5003c7ad8d9258639647e0ce1c4a11e07fbc2b10b13762f6d75\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n      labels:\n        app: oauth2-proxy        \n        helm.sh/chart: oauth2-proxy-5.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: oauth2-proxy\n        app.kubernetes.io/version: \"7.2.0\"\n    spec:\n      serviceAccountName: oauth2-proxy\n      containers:\n      - name: oauth2-proxy\n        image: \"quay.io/oauth2-proxy/oauth2-proxy:v7.2.0\"\n        imagePullPolicy: IfNotPresent\n        args:\n          - --http-address=0.0.0.0:4180\n          - --metrics-address=0.0.0.0:44180\n          - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: cookie-secret\n        ports:\n          - containerPort: 4180\n            name: http\n            protocol: TCP\n          - containerPort: 44180\n            protocol: TCP\n            name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n          successThreshold: 1\n          periodSeconds: 10\n        resources:\n          {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy\n          name: configmain\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: oauth2-proxy\n        name: configmain\n      tolerations:\n        []\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/oauth2-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy    \n    helm.sh/chart: oauth2-proxy-5.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: oauth2-proxy\n    app.kubernetes.io/version: \"7.2.0\"\n  name: oauth2-proxy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: oauth2-proxy\n  template:\n    metadata:\n      annotations:\n        checksum/config: bffba9ea1ed9347873dc113026d3d4c0645b1ced9180fc723537f917425a986f\n        checksum/config-emails: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 911c11690be7e5003c7ad8d9258639647e0ce1c4a11e07fbc2b10b13762f6d75\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n      labels:\n        app: oauth2-proxy        \n        helm.sh/chart: oauth2-proxy-5.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: oauth2-proxy\n        app.kubernetes.io/version: \"7.2.0\"\n    spec:\n      serviceAccountName: oauth2-proxy\n      containers:\n      - name: oauth2-proxy\n        image: \"quay.io/oauth2-proxy/oauth2-proxy:v7.2.0\"\n        imagePullPolicy: IfNotPresent\n        args:\n          - --http-address=0.0.0.0:4180\n          - --metrics-address=0.0.0.0:44180\n          - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: cookie-secret\n        ports:\n          - containerPort: 4180\n            name: http\n            protocol: TCP\n          - containerPort: 44180\n            protocol: TCP\n            name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n          successThreshold: 1\n          periodSeconds: 10\n        resources:\n          {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy\n          name: configmain\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: oauth2-proxy\n        name: configmain\n      tolerations:\n        []\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/oauth2-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy    \n    helm.sh/chart: oauth2-proxy-5.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: oauth2-proxy\n    app.kubernetes.io/version: \"7.2.0\"\n  name: oauth2-proxy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: oauth2-proxy\n  template:\n    metadata:\n      annotations:\n        checksum/config: bffba9ea1ed9347873dc113026d3d4c0645b1ced9180fc723537f917425a986f\n        checksum/config-emails: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 911c11690be7e5003c7ad8d9258639647e0ce1c4a11e07fbc2b10b13762f6d75\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n      labels:\n        app: oauth2-proxy        \n        helm.sh/chart: oauth2-proxy-5.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: oauth2-proxy\n        app.kubernetes.io/version: \"7.2.0\"\n    spec:\n      serviceAccountName: oauth2-proxy\n      containers:\n      - name: oauth2-proxy\n        image: \"quay.io/oauth2-proxy/oauth2-proxy:v7.2.0\"\n        imagePullPolicy: IfNotPresent\n        args:\n          - --http-address=0.0.0.0:4180\n          - --metrics-address=0.0.0.0:44180\n          - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: cookie-secret\n        ports:\n          - containerPort: 4180\n            name: http\n            protocol: TCP\n          - containerPort: 44180\n            protocol: TCP\n            name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n          successThreshold: 1\n          periodSeconds: 10\n        resources:\n          {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy\n          name: configmain\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: oauth2-proxy\n        name: configmain\n      tolerations:\n        []\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/oauth2-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy    \n    helm.sh/chart: oauth2-proxy-5.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: oauth2-proxy\n    app.kubernetes.io/version: \"7.2.0\"\n  name: oauth2-proxy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: oauth2-proxy\n  template:\n    metadata:\n      annotations:\n        checksum/config: bffba9ea1ed9347873dc113026d3d4c0645b1ced9180fc723537f917425a986f\n        checksum/config-emails: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 911c11690be7e5003c7ad8d9258639647e0ce1c4a11e07fbc2b10b13762f6d75\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n      labels:\n        app: oauth2-proxy        \n        helm.sh/chart: oauth2-proxy-5.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: oauth2-proxy\n        app.kubernetes.io/version: \"7.2.0\"\n    spec:\n      serviceAccountName: oauth2-proxy\n      containers:\n      - name: oauth2-proxy\n        image: \"quay.io/oauth2-proxy/oauth2-proxy:v7.2.0\"\n        imagePullPolicy: IfNotPresent\n        args:\n          - --http-address=0.0.0.0:4180\n          - --metrics-address=0.0.0.0:44180\n          - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: cookie-secret\n        ports:\n          - containerPort: 4180\n            name: http\n            protocol: TCP\n          - containerPort: 44180\n            protocol: TCP\n            name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n          successThreshold: 1\n          periodSeconds: 10\n        resources:\n          {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy\n          name: configmain\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: oauth2-proxy\n        name: configmain\n      tolerations:\n        []\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/oauth2-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy    \n    helm.sh/chart: oauth2-proxy-5.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: oauth2-proxy\n    app.kubernetes.io/version: \"7.2.0\"\n  name: oauth2-proxy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: oauth2-proxy\n  template:\n    metadata:\n      annotations:\n        checksum/config: bffba9ea1ed9347873dc113026d3d4c0645b1ced9180fc723537f917425a986f\n        checksum/config-emails: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 911c11690be7e5003c7ad8d9258639647e0ce1c4a11e07fbc2b10b13762f6d75\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n      labels:\n        app: oauth2-proxy        \n        helm.sh/chart: oauth2-proxy-5.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: oauth2-proxy\n        app.kubernetes.io/version: \"7.2.0\"\n    spec:\n      serviceAccountName: oauth2-proxy\n      containers:\n      - name: oauth2-proxy\n        image: \"quay.io/oauth2-proxy/oauth2-proxy:v7.2.0\"\n        imagePullPolicy: IfNotPresent\n        args:\n          - --http-address=0.0.0.0:4180\n          - --metrics-address=0.0.0.0:44180\n          - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: cookie-secret\n        ports:\n          - containerPort: 4180\n            name: http\n            protocol: TCP\n          - containerPort: 44180\n            protocol: TCP\n            name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n          successThreshold: 1\n          periodSeconds: 10\n        resources:\n          {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy\n          name: configmain\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: oauth2-proxy\n        name: configmain\n      tolerations:\n        []\n"}}}}]}, {"ruleId": "CKV_K8S_35", "ruleIndex": 6, "level": "error", "attachments": [], "message": {"text": "Prefer using secrets as files over secrets as environment variables"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/oauth2-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy    \n    helm.sh/chart: oauth2-proxy-5.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: oauth2-proxy\n    app.kubernetes.io/version: \"7.2.0\"\n  name: oauth2-proxy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: oauth2-proxy\n  template:\n    metadata:\n      annotations:\n        checksum/config: bffba9ea1ed9347873dc113026d3d4c0645b1ced9180fc723537f917425a986f\n        checksum/config-emails: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 911c11690be7e5003c7ad8d9258639647e0ce1c4a11e07fbc2b10b13762f6d75\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n      labels:\n        app: oauth2-proxy        \n        helm.sh/chart: oauth2-proxy-5.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: oauth2-proxy\n        app.kubernetes.io/version: \"7.2.0\"\n    spec:\n      serviceAccountName: oauth2-proxy\n      containers:\n      - name: oauth2-proxy\n        image: \"quay.io/oauth2-proxy/oauth2-proxy:v7.2.0\"\n        imagePullPolicy: IfNotPresent\n        args:\n          - --http-address=0.0.0.0:4180\n          - --metrics-address=0.0.0.0:44180\n          - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: cookie-secret\n        ports:\n          - containerPort: 4180\n            name: http\n            protocol: TCP\n          - containerPort: 44180\n            protocol: TCP\n            name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n          successThreshold: 1\n          periodSeconds: 10\n        resources:\n          {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy\n          name: configmain\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: oauth2-proxy\n        name: configmain\n      tolerations:\n        []\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/oauth2-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy    \n    helm.sh/chart: oauth2-proxy-5.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: oauth2-proxy\n    app.kubernetes.io/version: \"7.2.0\"\n  name: oauth2-proxy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: oauth2-proxy\n  template:\n    metadata:\n      annotations:\n        checksum/config: bffba9ea1ed9347873dc113026d3d4c0645b1ced9180fc723537f917425a986f\n        checksum/config-emails: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 911c11690be7e5003c7ad8d9258639647e0ce1c4a11e07fbc2b10b13762f6d75\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n      labels:\n        app: oauth2-proxy        \n        helm.sh/chart: oauth2-proxy-5.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: oauth2-proxy\n        app.kubernetes.io/version: \"7.2.0\"\n    spec:\n      serviceAccountName: oauth2-proxy\n      containers:\n      - name: oauth2-proxy\n        image: \"quay.io/oauth2-proxy/oauth2-proxy:v7.2.0\"\n        imagePullPolicy: IfNotPresent\n        args:\n          - --http-address=0.0.0.0:4180\n          - --metrics-address=0.0.0.0:44180\n          - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: cookie-secret\n        ports:\n          - containerPort: 4180\n            name: http\n            protocol: TCP\n          - containerPort: 44180\n            protocol: TCP\n            name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n          successThreshold: 1\n          periodSeconds: 10\n        resources:\n          {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy\n          name: configmain\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: oauth2-proxy\n        name: configmain\n      tolerations:\n        []\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/oauth2-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy    \n    helm.sh/chart: oauth2-proxy-5.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: oauth2-proxy\n    app.kubernetes.io/version: \"7.2.0\"\n  name: oauth2-proxy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: oauth2-proxy\n  template:\n    metadata:\n      annotations:\n        checksum/config: bffba9ea1ed9347873dc113026d3d4c0645b1ced9180fc723537f917425a986f\n        checksum/config-emails: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 911c11690be7e5003c7ad8d9258639647e0ce1c4a11e07fbc2b10b13762f6d75\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n      labels:\n        app: oauth2-proxy        \n        helm.sh/chart: oauth2-proxy-5.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: oauth2-proxy\n        app.kubernetes.io/version: \"7.2.0\"\n    spec:\n      serviceAccountName: oauth2-proxy\n      containers:\n      - name: oauth2-proxy\n        image: \"quay.io/oauth2-proxy/oauth2-proxy:v7.2.0\"\n        imagePullPolicy: IfNotPresent\n        args:\n          - --http-address=0.0.0.0:4180\n          - --metrics-address=0.0.0.0:44180\n          - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: cookie-secret\n        ports:\n          - containerPort: 4180\n            name: http\n            protocol: TCP\n          - containerPort: 44180\n            protocol: TCP\n            name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n          successThreshold: 1\n          periodSeconds: 10\n        resources:\n          {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy\n          name: configmain\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: oauth2-proxy\n        name: configmain\n      tolerations:\n        []\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/oauth2-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy    \n    helm.sh/chart: oauth2-proxy-5.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: oauth2-proxy\n    app.kubernetes.io/version: \"7.2.0\"\n  name: oauth2-proxy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: oauth2-proxy\n  template:\n    metadata:\n      annotations:\n        checksum/config: bffba9ea1ed9347873dc113026d3d4c0645b1ced9180fc723537f917425a986f\n        checksum/config-emails: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 911c11690be7e5003c7ad8d9258639647e0ce1c4a11e07fbc2b10b13762f6d75\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n      labels:\n        app: oauth2-proxy        \n        helm.sh/chart: oauth2-proxy-5.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: oauth2-proxy\n        app.kubernetes.io/version: \"7.2.0\"\n    spec:\n      serviceAccountName: oauth2-proxy\n      containers:\n      - name: oauth2-proxy\n        image: \"quay.io/oauth2-proxy/oauth2-proxy:v7.2.0\"\n        imagePullPolicy: IfNotPresent\n        args:\n          - --http-address=0.0.0.0:4180\n          - --metrics-address=0.0.0.0:44180\n          - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: cookie-secret\n        ports:\n          - containerPort: 4180\n            name: http\n            protocol: TCP\n          - containerPort: 44180\n            protocol: TCP\n            name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n          successThreshold: 1\n          periodSeconds: 10\n        resources:\n          {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy\n          name: configmain\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: oauth2-proxy\n        name: configmain\n      tolerations:\n        []\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/oauth2-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy    \n    helm.sh/chart: oauth2-proxy-5.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: oauth2-proxy\n    app.kubernetes.io/version: \"7.2.0\"\n  name: oauth2-proxy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: oauth2-proxy\n  template:\n    metadata:\n      annotations:\n        checksum/config: bffba9ea1ed9347873dc113026d3d4c0645b1ced9180fc723537f917425a986f\n        checksum/config-emails: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 911c11690be7e5003c7ad8d9258639647e0ce1c4a11e07fbc2b10b13762f6d75\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n      labels:\n        app: oauth2-proxy        \n        helm.sh/chart: oauth2-proxy-5.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: oauth2-proxy\n        app.kubernetes.io/version: \"7.2.0\"\n    spec:\n      serviceAccountName: oauth2-proxy\n      containers:\n      - name: oauth2-proxy\n        image: \"quay.io/oauth2-proxy/oauth2-proxy:v7.2.0\"\n        imagePullPolicy: IfNotPresent\n        args:\n          - --http-address=0.0.0.0:4180\n          - --metrics-address=0.0.0.0:44180\n          - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: cookie-secret\n        ports:\n          - containerPort: 4180\n            name: http\n            protocol: TCP\n          - containerPort: 44180\n            protocol: TCP\n            name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n          successThreshold: 1\n          periodSeconds: 10\n        resources:\n          {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy\n          name: configmain\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: oauth2-proxy\n        name: configmain\n      tolerations:\n        []\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/oauth2-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy    \n    helm.sh/chart: oauth2-proxy-5.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: oauth2-proxy\n    app.kubernetes.io/version: \"7.2.0\"\n  name: oauth2-proxy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: oauth2-proxy\n  template:\n    metadata:\n      annotations:\n        checksum/config: bffba9ea1ed9347873dc113026d3d4c0645b1ced9180fc723537f917425a986f\n        checksum/config-emails: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 911c11690be7e5003c7ad8d9258639647e0ce1c4a11e07fbc2b10b13762f6d75\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n      labels:\n        app: oauth2-proxy        \n        helm.sh/chart: oauth2-proxy-5.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: oauth2-proxy\n        app.kubernetes.io/version: \"7.2.0\"\n    spec:\n      serviceAccountName: oauth2-proxy\n      containers:\n      - name: oauth2-proxy\n        image: \"quay.io/oauth2-proxy/oauth2-proxy:v7.2.0\"\n        imagePullPolicy: IfNotPresent\n        args:\n          - --http-address=0.0.0.0:4180\n          - --metrics-address=0.0.0.0:44180\n          - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: cookie-secret\n        ports:\n          - containerPort: 4180\n            name: http\n            protocol: TCP\n          - containerPort: 44180\n            protocol: TCP\n            name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n          successThreshold: 1\n          periodSeconds: 10\n        resources:\n          {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy\n          name: configmain\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: oauth2-proxy\n        name: configmain\n      tolerations:\n        []\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/oauth2-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy    \n    helm.sh/chart: oauth2-proxy-5.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: oauth2-proxy\n    app.kubernetes.io/version: \"7.2.0\"\n  name: oauth2-proxy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: oauth2-proxy\n  template:\n    metadata:\n      annotations:\n        checksum/config: bffba9ea1ed9347873dc113026d3d4c0645b1ced9180fc723537f917425a986f\n        checksum/config-emails: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 911c11690be7e5003c7ad8d9258639647e0ce1c4a11e07fbc2b10b13762f6d75\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n      labels:\n        app: oauth2-proxy        \n        helm.sh/chart: oauth2-proxy-5.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: oauth2-proxy\n        app.kubernetes.io/version: \"7.2.0\"\n    spec:\n      serviceAccountName: oauth2-proxy\n      containers:\n      - name: oauth2-proxy\n        image: \"quay.io/oauth2-proxy/oauth2-proxy:v7.2.0\"\n        imagePullPolicy: IfNotPresent\n        args:\n          - --http-address=0.0.0.0:4180\n          - --metrics-address=0.0.0.0:44180\n          - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: cookie-secret\n        ports:\n          - containerPort: 4180\n            name: http\n            protocol: TCP\n          - containerPort: 44180\n            protocol: TCP\n            name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n          successThreshold: 1\n          periodSeconds: 10\n        resources:\n          {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy\n          name: configmain\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: oauth2-proxy\n        name: configmain\n      tolerations:\n        []\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/oauth2-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy    \n    helm.sh/chart: oauth2-proxy-5.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: oauth2-proxy\n    app.kubernetes.io/version: \"7.2.0\"\n  name: oauth2-proxy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: oauth2-proxy\n  template:\n    metadata:\n      annotations:\n        checksum/config: bffba9ea1ed9347873dc113026d3d4c0645b1ced9180fc723537f917425a986f\n        checksum/config-emails: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 911c11690be7e5003c7ad8d9258639647e0ce1c4a11e07fbc2b10b13762f6d75\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n      labels:\n        app: oauth2-proxy        \n        helm.sh/chart: oauth2-proxy-5.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: oauth2-proxy\n        app.kubernetes.io/version: \"7.2.0\"\n    spec:\n      serviceAccountName: oauth2-proxy\n      containers:\n      - name: oauth2-proxy\n        image: \"quay.io/oauth2-proxy/oauth2-proxy:v7.2.0\"\n        imagePullPolicy: IfNotPresent\n        args:\n          - --http-address=0.0.0.0:4180\n          - --metrics-address=0.0.0.0:44180\n          - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: cookie-secret\n        ports:\n          - containerPort: 4180\n            name: http\n            protocol: TCP\n          - containerPort: 44180\n            protocol: TCP\n            name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n          successThreshold: 1\n          periodSeconds: 10\n        resources:\n          {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy\n          name: configmain\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: oauth2-proxy\n        name: configmain\n      tolerations:\n        []\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/oauth2-proxy/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy    \n    helm.sh/chart: oauth2-proxy-5.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: oauth2-proxy\n    app.kubernetes.io/version: \"7.2.0\"\n  name: oauth2-proxy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: oauth2-proxy\n  template:\n    metadata:\n      annotations:\n        checksum/config: bffba9ea1ed9347873dc113026d3d4c0645b1ced9180fc723537f917425a986f\n        checksum/config-emails: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 911c11690be7e5003c7ad8d9258639647e0ce1c4a11e07fbc2b10b13762f6d75\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n      labels:\n        app: oauth2-proxy        \n        helm.sh/chart: oauth2-proxy-5.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: oauth2-proxy\n        app.kubernetes.io/version: \"7.2.0\"\n    spec:\n      serviceAccountName: oauth2-proxy\n      containers:\n      - name: oauth2-proxy\n        image: \"quay.io/oauth2-proxy/oauth2-proxy:v7.2.0\"\n        imagePullPolicy: IfNotPresent\n        args:\n          - --http-address=0.0.0.0:4180\n          - --metrics-address=0.0.0.0:44180\n          - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name:  oauth2-proxy\n              key: cookie-secret\n        ports:\n          - containerPort: 4180\n            name: http\n            protocol: TCP\n          - containerPort: 44180\n            protocol: TCP\n            name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n          successThreshold: 1\n          periodSeconds: 10\n        resources:\n          {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy\n          name: configmain\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: oauth2-proxy\n        name: configmain\n      tolerations:\n        []\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/oauth2-proxy/templates/service.yaml"}, "region": {"startLine": 3, "endLine": 29, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: oauth2-proxy    \n    helm.sh/chart: oauth2-proxy-5.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: oauth2-proxy\n    app.kubernetes.io/version: \"7.2.0\"\n  name: oauth2-proxy\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: http\n      protocol: TCP\n      name: http\n    - port: 44180\n      protocol: TCP\n      targetPort: metrics\n      name: metrics\n  selector:    \n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: oauth2-proxy\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/oauth2-proxy/templates/serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 15, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app: oauth2-proxy    \n    helm.sh/chart: oauth2-proxy-5.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: oauth2-proxy\n    app.kubernetes.io/version: \"7.2.0\"\n  name: oauth2-proxy\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/oauth2-proxy/templates/configmap.yaml"}, "region": {"startLine": 3, "endLine": 17, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  labels:\n    app: oauth2-proxy    \n    helm.sh/chart: oauth2-proxy-5.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: oauth2-proxy\n    app.kubernetes.io/version: \"7.2.0\"\n  name: oauth2-proxy\ndata:\n  oauth2_proxy.cfg: \"email_domains = [ \\\"*\\\" ]\\nupstreams = [ \\\"file:///dev/null\\\" ]\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/oauth2-proxy/templates/secret.yaml"}, "region": {"startLine": 3, "endLine": 20, "snippet": {"text": "apiVersion: v1\nkind: Secret\nmetadata:\n  labels:\n    app: oauth2-proxy    \n    helm.sh/chart: oauth2-proxy-5.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: oauth2-proxy\n    app.kubernetes.io/version: \"7.2.0\"\n  name: oauth2-proxy\ntype: Opaque\ndata:\n  cookie-secret: \"WFhYWFhYWFhYWFhYWFhYWA==\"\n  client-secret: \"WFhYWFhYWFg=\"\n  client-id: \"WFhYWFhYWA==\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrstudio-content/templates/configmap.yml"}, "region": {"startLine": 3, "endLine": 14, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: wrstudio-content-configmap\ndata:\n  external_vars.yml: |\n    aws_ecr_region: ECR_REGION\n    content:\n    - product\n    docker_dest_registry_domain_name: ECR_DOMAIN_NAME.amazonaws.com\n    linux_docker_src_registry_domain_name: ECR_DOMAIN_NAME.amazonaws.com\n    wrstudio_domain_name: WRSTUDIO_DOMAIN_NAME.cloud\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrstudio-content/templates/job.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: 1-wrstudio-content\n  labels:\n    helm.sh/chart: wrstudio-content-0.1.0\n    app.kubernetes.io/name: wrstudio-content\n    app.kubernetes.io/instance: wrstudio-content\n    app.kubernetes.io/version: \"1.0.0\"\n    helm.sh/chart: \"wrstudio-content-0.1.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n      labels:\n        app.kubernetes.io/name: wrstudio-content\n        app.kubernetes.io/instance: wrstudio-content\n    spec:\n      securityContext:\n        {}\n      restartPolicy: Never\n      containers:\n        - name: wrstudio-content\n          securityContext:\n            {}\n          image: \"853502460716.dkr.ecr.us-east-1.amazonaws.com/wrai-tools/wrai-lxvx-installer:0.0.3-alpha\"\n          imagePullPolicy: Always\n          command:\n          - \"/home/ansibleuser/scripts/entrypoint.sh\"\n          args:\n            - \"--tags\"\n            - \"product\"\n          env:\n            - name: JENKINS_USER_ID\n              value: not-set\n            - name: JENKINS_API_TOKEN\n              value: not-set\n            - name: GITLAB_API_TOKEN\n              value: not-set\n            - name: MINIO_ACCESS_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_access_key\n            - name: MINIO_SECRET_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_secret_key\n            - name: GIT_USERNAME\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_username\n            - name: GIT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_password\n          volumeMounts:\n            - name: wrstudio-content-configmap-volume\n              mountPath: \"/home/ansibleuser/ansible/vars\"\n              readOnly: true\n          resources:\n            {}\n      volumes:\n        - name: wrstudio-content-configmap-volume\n          configMap:\n            name: wrstudio-content-configmap\n  backoffLimit: 3\n---\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrstudio-content/templates/job.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: 1-wrstudio-content\n  labels:\n    helm.sh/chart: wrstudio-content-0.1.0\n    app.kubernetes.io/name: wrstudio-content\n    app.kubernetes.io/instance: wrstudio-content\n    app.kubernetes.io/version: \"1.0.0\"\n    helm.sh/chart: \"wrstudio-content-0.1.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n      labels:\n        app.kubernetes.io/name: wrstudio-content\n        app.kubernetes.io/instance: wrstudio-content\n    spec:\n      securityContext:\n        {}\n      restartPolicy: Never\n      containers:\n        - name: wrstudio-content\n          securityContext:\n            {}\n          image: \"853502460716.dkr.ecr.us-east-1.amazonaws.com/wrai-tools/wrai-lxvx-installer:0.0.3-alpha\"\n          imagePullPolicy: Always\n          command:\n          - \"/home/ansibleuser/scripts/entrypoint.sh\"\n          args:\n            - \"--tags\"\n            - \"product\"\n          env:\n            - name: JENKINS_USER_ID\n              value: not-set\n            - name: JENKINS_API_TOKEN\n              value: not-set\n            - name: GITLAB_API_TOKEN\n              value: not-set\n            - name: MINIO_ACCESS_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_access_key\n            - name: MINIO_SECRET_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_secret_key\n            - name: GIT_USERNAME\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_username\n            - name: GIT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_password\n          volumeMounts:\n            - name: wrstudio-content-configmap-volume\n              mountPath: \"/home/ansibleuser/ansible/vars\"\n              readOnly: true\n          resources:\n            {}\n      volumes:\n        - name: wrstudio-content-configmap-volume\n          configMap:\n            name: wrstudio-content-configmap\n  backoffLimit: 3\n---\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrstudio-content/templates/job.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: 1-wrstudio-content\n  labels:\n    helm.sh/chart: wrstudio-content-0.1.0\n    app.kubernetes.io/name: wrstudio-content\n    app.kubernetes.io/instance: wrstudio-content\n    app.kubernetes.io/version: \"1.0.0\"\n    helm.sh/chart: \"wrstudio-content-0.1.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n      labels:\n        app.kubernetes.io/name: wrstudio-content\n        app.kubernetes.io/instance: wrstudio-content\n    spec:\n      securityContext:\n        {}\n      restartPolicy: Never\n      containers:\n        - name: wrstudio-content\n          securityContext:\n            {}\n          image: \"853502460716.dkr.ecr.us-east-1.amazonaws.com/wrai-tools/wrai-lxvx-installer:0.0.3-alpha\"\n          imagePullPolicy: Always\n          command:\n          - \"/home/ansibleuser/scripts/entrypoint.sh\"\n          args:\n            - \"--tags\"\n            - \"product\"\n          env:\n            - name: JENKINS_USER_ID\n              value: not-set\n            - name: JENKINS_API_TOKEN\n              value: not-set\n            - name: GITLAB_API_TOKEN\n              value: not-set\n            - name: MINIO_ACCESS_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_access_key\n            - name: MINIO_SECRET_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_secret_key\n            - name: GIT_USERNAME\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_username\n            - name: GIT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_password\n          volumeMounts:\n            - name: wrstudio-content-configmap-volume\n              mountPath: \"/home/ansibleuser/ansible/vars\"\n              readOnly: true\n          resources:\n            {}\n      volumes:\n        - name: wrstudio-content-configmap-volume\n          configMap:\n            name: wrstudio-content-configmap\n  backoffLimit: 3\n---\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrstudio-content/templates/job.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: 1-wrstudio-content\n  labels:\n    helm.sh/chart: wrstudio-content-0.1.0\n    app.kubernetes.io/name: wrstudio-content\n    app.kubernetes.io/instance: wrstudio-content\n    app.kubernetes.io/version: \"1.0.0\"\n    helm.sh/chart: \"wrstudio-content-0.1.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n      labels:\n        app.kubernetes.io/name: wrstudio-content\n        app.kubernetes.io/instance: wrstudio-content\n    spec:\n      securityContext:\n        {}\n      restartPolicy: Never\n      containers:\n        - name: wrstudio-content\n          securityContext:\n            {}\n          image: \"853502460716.dkr.ecr.us-east-1.amazonaws.com/wrai-tools/wrai-lxvx-installer:0.0.3-alpha\"\n          imagePullPolicy: Always\n          command:\n          - \"/home/ansibleuser/scripts/entrypoint.sh\"\n          args:\n            - \"--tags\"\n            - \"product\"\n          env:\n            - name: JENKINS_USER_ID\n              value: not-set\n            - name: JENKINS_API_TOKEN\n              value: not-set\n            - name: GITLAB_API_TOKEN\n              value: not-set\n            - name: MINIO_ACCESS_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_access_key\n            - name: MINIO_SECRET_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_secret_key\n            - name: GIT_USERNAME\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_username\n            - name: GIT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_password\n          volumeMounts:\n            - name: wrstudio-content-configmap-volume\n              mountPath: \"/home/ansibleuser/ansible/vars\"\n              readOnly: true\n          resources:\n            {}\n      volumes:\n        - name: wrstudio-content-configmap-volume\n          configMap:\n            name: wrstudio-content-configmap\n  backoffLimit: 3\n---\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrstudio-content/templates/job.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: 1-wrstudio-content\n  labels:\n    helm.sh/chart: wrstudio-content-0.1.0\n    app.kubernetes.io/name: wrstudio-content\n    app.kubernetes.io/instance: wrstudio-content\n    app.kubernetes.io/version: \"1.0.0\"\n    helm.sh/chart: \"wrstudio-content-0.1.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n      labels:\n        app.kubernetes.io/name: wrstudio-content\n        app.kubernetes.io/instance: wrstudio-content\n    spec:\n      securityContext:\n        {}\n      restartPolicy: Never\n      containers:\n        - name: wrstudio-content\n          securityContext:\n            {}\n          image: \"853502460716.dkr.ecr.us-east-1.amazonaws.com/wrai-tools/wrai-lxvx-installer:0.0.3-alpha\"\n          imagePullPolicy: Always\n          command:\n          - \"/home/ansibleuser/scripts/entrypoint.sh\"\n          args:\n            - \"--tags\"\n            - \"product\"\n          env:\n            - name: JENKINS_USER_ID\n              value: not-set\n            - name: JENKINS_API_TOKEN\n              value: not-set\n            - name: GITLAB_API_TOKEN\n              value: not-set\n            - name: MINIO_ACCESS_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_access_key\n            - name: MINIO_SECRET_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_secret_key\n            - name: GIT_USERNAME\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_username\n            - name: GIT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_password\n          volumeMounts:\n            - name: wrstudio-content-configmap-volume\n              mountPath: \"/home/ansibleuser/ansible/vars\"\n              readOnly: true\n          resources:\n            {}\n      volumes:\n        - name: wrstudio-content-configmap-volume\n          configMap:\n            name: wrstudio-content-configmap\n  backoffLimit: 3\n---\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrstudio-content/templates/job.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: 1-wrstudio-content\n  labels:\n    helm.sh/chart: wrstudio-content-0.1.0\n    app.kubernetes.io/name: wrstudio-content\n    app.kubernetes.io/instance: wrstudio-content\n    app.kubernetes.io/version: \"1.0.0\"\n    helm.sh/chart: \"wrstudio-content-0.1.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n      labels:\n        app.kubernetes.io/name: wrstudio-content\n        app.kubernetes.io/instance: wrstudio-content\n    spec:\n      securityContext:\n        {}\n      restartPolicy: Never\n      containers:\n        - name: wrstudio-content\n          securityContext:\n            {}\n          image: \"853502460716.dkr.ecr.us-east-1.amazonaws.com/wrai-tools/wrai-lxvx-installer:0.0.3-alpha\"\n          imagePullPolicy: Always\n          command:\n          - \"/home/ansibleuser/scripts/entrypoint.sh\"\n          args:\n            - \"--tags\"\n            - \"product\"\n          env:\n            - name: JENKINS_USER_ID\n              value: not-set\n            - name: JENKINS_API_TOKEN\n              value: not-set\n            - name: GITLAB_API_TOKEN\n              value: not-set\n            - name: MINIO_ACCESS_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_access_key\n            - name: MINIO_SECRET_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_secret_key\n            - name: GIT_USERNAME\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_username\n            - name: GIT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_password\n          volumeMounts:\n            - name: wrstudio-content-configmap-volume\n              mountPath: \"/home/ansibleuser/ansible/vars\"\n              readOnly: true\n          resources:\n            {}\n      volumes:\n        - name: wrstudio-content-configmap-volume\n          configMap:\n            name: wrstudio-content-configmap\n  backoffLimit: 3\n---\n"}}}}]}, {"ruleId": "CKV_K8S_35", "ruleIndex": 6, "level": "error", "attachments": [], "message": {"text": "Prefer using secrets as files over secrets as environment variables"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrstudio-content/templates/job.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: 1-wrstudio-content\n  labels:\n    helm.sh/chart: wrstudio-content-0.1.0\n    app.kubernetes.io/name: wrstudio-content\n    app.kubernetes.io/instance: wrstudio-content\n    app.kubernetes.io/version: \"1.0.0\"\n    helm.sh/chart: \"wrstudio-content-0.1.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n      labels:\n        app.kubernetes.io/name: wrstudio-content\n        app.kubernetes.io/instance: wrstudio-content\n    spec:\n      securityContext:\n        {}\n      restartPolicy: Never\n      containers:\n        - name: wrstudio-content\n          securityContext:\n            {}\n          image: \"853502460716.dkr.ecr.us-east-1.amazonaws.com/wrai-tools/wrai-lxvx-installer:0.0.3-alpha\"\n          imagePullPolicy: Always\n          command:\n          - \"/home/ansibleuser/scripts/entrypoint.sh\"\n          args:\n            - \"--tags\"\n            - \"product\"\n          env:\n            - name: JENKINS_USER_ID\n              value: not-set\n            - name: JENKINS_API_TOKEN\n              value: not-set\n            - name: GITLAB_API_TOKEN\n              value: not-set\n            - name: MINIO_ACCESS_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_access_key\n            - name: MINIO_SECRET_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_secret_key\n            - name: GIT_USERNAME\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_username\n            - name: GIT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_password\n          volumeMounts:\n            - name: wrstudio-content-configmap-volume\n              mountPath: \"/home/ansibleuser/ansible/vars\"\n              readOnly: true\n          resources:\n            {}\n      volumes:\n        - name: wrstudio-content-configmap-volume\n          configMap:\n            name: wrstudio-content-configmap\n  backoffLimit: 3\n---\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrstudio-content/templates/job.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: 1-wrstudio-content\n  labels:\n    helm.sh/chart: wrstudio-content-0.1.0\n    app.kubernetes.io/name: wrstudio-content\n    app.kubernetes.io/instance: wrstudio-content\n    app.kubernetes.io/version: \"1.0.0\"\n    helm.sh/chart: \"wrstudio-content-0.1.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n      labels:\n        app.kubernetes.io/name: wrstudio-content\n        app.kubernetes.io/instance: wrstudio-content\n    spec:\n      securityContext:\n        {}\n      restartPolicy: Never\n      containers:\n        - name: wrstudio-content\n          securityContext:\n            {}\n          image: \"853502460716.dkr.ecr.us-east-1.amazonaws.com/wrai-tools/wrai-lxvx-installer:0.0.3-alpha\"\n          imagePullPolicy: Always\n          command:\n          - \"/home/ansibleuser/scripts/entrypoint.sh\"\n          args:\n            - \"--tags\"\n            - \"product\"\n          env:\n            - name: JENKINS_USER_ID\n              value: not-set\n            - name: JENKINS_API_TOKEN\n              value: not-set\n            - name: GITLAB_API_TOKEN\n              value: not-set\n            - name: MINIO_ACCESS_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_access_key\n            - name: MINIO_SECRET_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_secret_key\n            - name: GIT_USERNAME\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_username\n            - name: GIT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_password\n          volumeMounts:\n            - name: wrstudio-content-configmap-volume\n              mountPath: \"/home/ansibleuser/ansible/vars\"\n              readOnly: true\n          resources:\n            {}\n      volumes:\n        - name: wrstudio-content-configmap-volume\n          configMap:\n            name: wrstudio-content-configmap\n  backoffLimit: 3\n---\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrstudio-content/templates/job.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: 1-wrstudio-content\n  labels:\n    helm.sh/chart: wrstudio-content-0.1.0\n    app.kubernetes.io/name: wrstudio-content\n    app.kubernetes.io/instance: wrstudio-content\n    app.kubernetes.io/version: \"1.0.0\"\n    helm.sh/chart: \"wrstudio-content-0.1.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n      labels:\n        app.kubernetes.io/name: wrstudio-content\n        app.kubernetes.io/instance: wrstudio-content\n    spec:\n      securityContext:\n        {}\n      restartPolicy: Never\n      containers:\n        - name: wrstudio-content\n          securityContext:\n            {}\n          image: \"853502460716.dkr.ecr.us-east-1.amazonaws.com/wrai-tools/wrai-lxvx-installer:0.0.3-alpha\"\n          imagePullPolicy: Always\n          command:\n          - \"/home/ansibleuser/scripts/entrypoint.sh\"\n          args:\n            - \"--tags\"\n            - \"product\"\n          env:\n            - name: JENKINS_USER_ID\n              value: not-set\n            - name: JENKINS_API_TOKEN\n              value: not-set\n            - name: GITLAB_API_TOKEN\n              value: not-set\n            - name: MINIO_ACCESS_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_access_key\n            - name: MINIO_SECRET_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_secret_key\n            - name: GIT_USERNAME\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_username\n            - name: GIT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_password\n          volumeMounts:\n            - name: wrstudio-content-configmap-volume\n              mountPath: \"/home/ansibleuser/ansible/vars\"\n              readOnly: true\n          resources:\n            {}\n      volumes:\n        - name: wrstudio-content-configmap-volume\n          configMap:\n            name: wrstudio-content-configmap\n  backoffLimit: 3\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrstudio-content/templates/job.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: 1-wrstudio-content\n  labels:\n    helm.sh/chart: wrstudio-content-0.1.0\n    app.kubernetes.io/name: wrstudio-content\n    app.kubernetes.io/instance: wrstudio-content\n    app.kubernetes.io/version: \"1.0.0\"\n    helm.sh/chart: \"wrstudio-content-0.1.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n      labels:\n        app.kubernetes.io/name: wrstudio-content\n        app.kubernetes.io/instance: wrstudio-content\n    spec:\n      securityContext:\n        {}\n      restartPolicy: Never\n      containers:\n        - name: wrstudio-content\n          securityContext:\n            {}\n          image: \"853502460716.dkr.ecr.us-east-1.amazonaws.com/wrai-tools/wrai-lxvx-installer:0.0.3-alpha\"\n          imagePullPolicy: Always\n          command:\n          - \"/home/ansibleuser/scripts/entrypoint.sh\"\n          args:\n            - \"--tags\"\n            - \"product\"\n          env:\n            - name: JENKINS_USER_ID\n              value: not-set\n            - name: JENKINS_API_TOKEN\n              value: not-set\n            - name: GITLAB_API_TOKEN\n              value: not-set\n            - name: MINIO_ACCESS_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_access_key\n            - name: MINIO_SECRET_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_secret_key\n            - name: GIT_USERNAME\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_username\n            - name: GIT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_password\n          volumeMounts:\n            - name: wrstudio-content-configmap-volume\n              mountPath: \"/home/ansibleuser/ansible/vars\"\n              readOnly: true\n          resources:\n            {}\n      volumes:\n        - name: wrstudio-content-configmap-volume\n          configMap:\n            name: wrstudio-content-configmap\n  backoffLimit: 3\n---\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrstudio-content/templates/job.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: 1-wrstudio-content\n  labels:\n    helm.sh/chart: wrstudio-content-0.1.0\n    app.kubernetes.io/name: wrstudio-content\n    app.kubernetes.io/instance: wrstudio-content\n    app.kubernetes.io/version: \"1.0.0\"\n    helm.sh/chart: \"wrstudio-content-0.1.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n      labels:\n        app.kubernetes.io/name: wrstudio-content\n        app.kubernetes.io/instance: wrstudio-content\n    spec:\n      securityContext:\n        {}\n      restartPolicy: Never\n      containers:\n        - name: wrstudio-content\n          securityContext:\n            {}\n          image: \"853502460716.dkr.ecr.us-east-1.amazonaws.com/wrai-tools/wrai-lxvx-installer:0.0.3-alpha\"\n          imagePullPolicy: Always\n          command:\n          - \"/home/ansibleuser/scripts/entrypoint.sh\"\n          args:\n            - \"--tags\"\n            - \"product\"\n          env:\n            - name: JENKINS_USER_ID\n              value: not-set\n            - name: JENKINS_API_TOKEN\n              value: not-set\n            - name: GITLAB_API_TOKEN\n              value: not-set\n            - name: MINIO_ACCESS_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_access_key\n            - name: MINIO_SECRET_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_secret_key\n            - name: GIT_USERNAME\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_username\n            - name: GIT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_password\n          volumeMounts:\n            - name: wrstudio-content-configmap-volume\n              mountPath: \"/home/ansibleuser/ansible/vars\"\n              readOnly: true\n          resources:\n            {}\n      volumes:\n        - name: wrstudio-content-configmap-volume\n          configMap:\n            name: wrstudio-content-configmap\n  backoffLimit: 3\n---\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrstudio-content/templates/job.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: 1-wrstudio-content\n  labels:\n    helm.sh/chart: wrstudio-content-0.1.0\n    app.kubernetes.io/name: wrstudio-content\n    app.kubernetes.io/instance: wrstudio-content\n    app.kubernetes.io/version: \"1.0.0\"\n    helm.sh/chart: \"wrstudio-content-0.1.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n      labels:\n        app.kubernetes.io/name: wrstudio-content\n        app.kubernetes.io/instance: wrstudio-content\n    spec:\n      securityContext:\n        {}\n      restartPolicy: Never\n      containers:\n        - name: wrstudio-content\n          securityContext:\n            {}\n          image: \"853502460716.dkr.ecr.us-east-1.amazonaws.com/wrai-tools/wrai-lxvx-installer:0.0.3-alpha\"\n          imagePullPolicy: Always\n          command:\n          - \"/home/ansibleuser/scripts/entrypoint.sh\"\n          args:\n            - \"--tags\"\n            - \"product\"\n          env:\n            - name: JENKINS_USER_ID\n              value: not-set\n            - name: JENKINS_API_TOKEN\n              value: not-set\n            - name: GITLAB_API_TOKEN\n              value: not-set\n            - name: MINIO_ACCESS_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_access_key\n            - name: MINIO_SECRET_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_secret_key\n            - name: GIT_USERNAME\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_username\n            - name: GIT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_password\n          volumeMounts:\n            - name: wrstudio-content-configmap-volume\n              mountPath: \"/home/ansibleuser/ansible/vars\"\n              readOnly: true\n          resources:\n            {}\n      volumes:\n        - name: wrstudio-content-configmap-volume\n          configMap:\n            name: wrstudio-content-configmap\n  backoffLimit: 3\n---\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrstudio-content/templates/job.yaml"}, "region": {"startLine": 3, "endLine": 75, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: 1-wrstudio-content\n  labels:\n    helm.sh/chart: wrstudio-content-0.1.0\n    app.kubernetes.io/name: wrstudio-content\n    app.kubernetes.io/instance: wrstudio-content\n    app.kubernetes.io/version: \"1.0.0\"\n    helm.sh/chart: \"wrstudio-content-0.1.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n      labels:\n        app.kubernetes.io/name: wrstudio-content\n        app.kubernetes.io/instance: wrstudio-content\n    spec:\n      securityContext:\n        {}\n      restartPolicy: Never\n      containers:\n        - name: wrstudio-content\n          securityContext:\n            {}\n          image: \"853502460716.dkr.ecr.us-east-1.amazonaws.com/wrai-tools/wrai-lxvx-installer:0.0.3-alpha\"\n          imagePullPolicy: Always\n          command:\n          - \"/home/ansibleuser/scripts/entrypoint.sh\"\n          args:\n            - \"--tags\"\n            - \"product\"\n          env:\n            - name: JENKINS_USER_ID\n              value: not-set\n            - name: JENKINS_API_TOKEN\n              value: not-set\n            - name: GITLAB_API_TOKEN\n              value: not-set\n            - name: MINIO_ACCESS_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_access_key\n            - name: MINIO_SECRET_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   minio_secret_key\n            - name: GIT_USERNAME\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_username\n            - name: GIT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name:  wrstudio-content-installer-secrets\n                  key:   git_password\n          volumeMounts:\n            - name: wrstudio-content-configmap-volume\n              mountPath: \"/home/ansibleuser/ansible/vars\"\n              readOnly: true\n          resources:\n            {}\n      volumes:\n        - name: wrstudio-content-configmap-volume\n          configMap:\n            name: wrstudio-content-configmap\n  backoffLimit: 3\n---\n"}}}}]}, {"ruleId": "CKV_K8S_158", "ruleIndex": 22, "level": "error", "attachments": [], "message": {"text": "Minimize Roles and ClusterRoles that grant permissions to escalate Roles or ClusterRoles"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/role.yaml"}, "region": {"startLine": 3, "endLine": 20, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: velero-server\n  namespace: default\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nrules:\n- apiGroups:\n    - \"*\"\n  resources:\n    - \"*\"\n  verbs:\n    - \"*\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/role.yaml"}, "region": {"startLine": 3, "endLine": 20, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: velero-server\n  namespace: default\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nrules:\n- apiGroups:\n    - \"*\"\n  resources:\n    - \"*\"\n  verbs:\n    - \"*\"\n"}}}}]}, {"ruleId": "CKV_K8S_157", "ruleIndex": 23, "level": "error", "attachments": [], "message": {"text": "Minimize Roles and ClusterRoles that grant permissions to bind RoleBindings or ClusterRoleBindings"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/role.yaml"}, "region": {"startLine": 3, "endLine": 20, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: velero-server\n  namespace: default\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nrules:\n- apiGroups:\n    - \"*\"\n  resources:\n    - \"*\"\n  verbs:\n    - \"*\"\n"}}}}]}, {"ruleId": "CKV_K8S_49", "ruleIndex": 24, "level": "error", "attachments": [], "message": {"text": "Minimize wildcard use in Roles and ClusterRoles"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/role.yaml"}, "region": {"startLine": 3, "endLine": 20, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: velero-server\n  namespace: default\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nrules:\n- apiGroups:\n    - \"*\"\n  resources:\n    - \"*\"\n  verbs:\n    - \"*\"\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/upgrade-crds.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: velero-upgrade-crds\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade,post-rollback\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: velero-server\n      initContainers:\n        - name: kubectl\n          image: \"docker.io/bitnami/kubectl:1.27\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /bin/sh\n          args:\n            - -c\n            - cp `which sh` /tmp && cp `which kubectl` /tmp\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      containers:\n        - name: velero\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /tmp/sh\n          args:\n            - -c\n            - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      volumes:\n        - name: crds\n          emptyDir: {}\n      restartPolicy: OnFailure\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/upgrade-crds.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: velero-upgrade-crds\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade,post-rollback\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: velero-server\n      initContainers:\n        - name: kubectl\n          image: \"docker.io/bitnami/kubectl:1.27\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /bin/sh\n          args:\n            - -c\n            - cp `which sh` /tmp && cp `which kubectl` /tmp\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      containers:\n        - name: velero\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /tmp/sh\n          args:\n            - -c\n            - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      volumes:\n        - name: crds\n          emptyDir: {}\n      restartPolicy: OnFailure\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/upgrade-crds.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: velero-upgrade-crds\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade,post-rollback\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: velero-server\n      initContainers:\n        - name: kubectl\n          image: \"docker.io/bitnami/kubectl:1.27\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /bin/sh\n          args:\n            - -c\n            - cp `which sh` /tmp && cp `which kubectl` /tmp\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      containers:\n        - name: velero\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /tmp/sh\n          args:\n            - -c\n            - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      volumes:\n        - name: crds\n          emptyDir: {}\n      restartPolicy: OnFailure\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/upgrade-crds.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: velero-upgrade-crds\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade,post-rollback\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: velero-server\n      initContainers:\n        - name: kubectl\n          image: \"docker.io/bitnami/kubectl:1.27\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /bin/sh\n          args:\n            - -c\n            - cp `which sh` /tmp && cp `which kubectl` /tmp\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      containers:\n        - name: velero\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /tmp/sh\n          args:\n            - -c\n            - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      volumes:\n        - name: crds\n          emptyDir: {}\n      restartPolicy: OnFailure\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/upgrade-crds.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: velero-upgrade-crds\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade,post-rollback\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: velero-server\n      initContainers:\n        - name: kubectl\n          image: \"docker.io/bitnami/kubectl:1.27\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /bin/sh\n          args:\n            - -c\n            - cp `which sh` /tmp && cp `which kubectl` /tmp\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      containers:\n        - name: velero\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /tmp/sh\n          args:\n            - -c\n            - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      volumes:\n        - name: crds\n          emptyDir: {}\n      restartPolicy: OnFailure\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/upgrade-crds.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: velero-upgrade-crds\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade,post-rollback\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: velero-server\n      initContainers:\n        - name: kubectl\n          image: \"docker.io/bitnami/kubectl:1.27\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /bin/sh\n          args:\n            - -c\n            - cp `which sh` /tmp && cp `which kubectl` /tmp\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      containers:\n        - name: velero\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /tmp/sh\n          args:\n            - -c\n            - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      volumes:\n        - name: crds\n          emptyDir: {}\n      restartPolicy: OnFailure\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/upgrade-crds.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: velero-upgrade-crds\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade,post-rollback\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: velero-server\n      initContainers:\n        - name: kubectl\n          image: \"docker.io/bitnami/kubectl:1.27\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /bin/sh\n          args:\n            - -c\n            - cp `which sh` /tmp && cp `which kubectl` /tmp\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      containers:\n        - name: velero\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /tmp/sh\n          args:\n            - -c\n            - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      volumes:\n        - name: crds\n          emptyDir: {}\n      restartPolicy: OnFailure\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/upgrade-crds.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: velero-upgrade-crds\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade,post-rollback\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: velero-server\n      initContainers:\n        - name: kubectl\n          image: \"docker.io/bitnami/kubectl:1.27\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /bin/sh\n          args:\n            - -c\n            - cp `which sh` /tmp && cp `which kubectl` /tmp\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      containers:\n        - name: velero\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /tmp/sh\n          args:\n            - -c\n            - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      volumes:\n        - name: crds\n          emptyDir: {}\n      restartPolicy: OnFailure\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/upgrade-crds.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: velero-upgrade-crds\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade,post-rollback\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: velero-server\n      initContainers:\n        - name: kubectl\n          image: \"docker.io/bitnami/kubectl:1.27\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /bin/sh\n          args:\n            - -c\n            - cp `which sh` /tmp && cp `which kubectl` /tmp\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      containers:\n        - name: velero\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /tmp/sh\n          args:\n            - -c\n            - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      volumes:\n        - name: crds\n          emptyDir: {}\n      restartPolicy: OnFailure\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/upgrade-crds.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: velero-upgrade-crds\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade,post-rollback\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: velero-server\n      initContainers:\n        - name: kubectl\n          image: \"docker.io/bitnami/kubectl:1.27\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /bin/sh\n          args:\n            - -c\n            - cp `which sh` /tmp && cp `which kubectl` /tmp\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      containers:\n        - name: velero\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /tmp/sh\n          args:\n            - -c\n            - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      volumes:\n        - name: crds\n          emptyDir: {}\n      restartPolicy: OnFailure\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/upgrade-crds.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: velero-upgrade-crds\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade,post-rollback\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: velero-server\n      initContainers:\n        - name: kubectl\n          image: \"docker.io/bitnami/kubectl:1.27\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /bin/sh\n          args:\n            - -c\n            - cp `which sh` /tmp && cp `which kubectl` /tmp\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      containers:\n        - name: velero\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /tmp/sh\n          args:\n            - -c\n            - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      volumes:\n        - name: crds\n          emptyDir: {}\n      restartPolicy: OnFailure\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/upgrade-crds.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: velero-upgrade-crds\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade,post-rollback\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: velero-server\n      initContainers:\n        - name: kubectl\n          image: \"docker.io/bitnami/kubectl:1.27\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /bin/sh\n          args:\n            - -c\n            - cp `which sh` /tmp && cp `which kubectl` /tmp\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      containers:\n        - name: velero\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /tmp/sh\n          args:\n            - -c\n            - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      volumes:\n        - name: crds\n          emptyDir: {}\n      restartPolicy: OnFailure\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/upgrade-crds.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: velero-upgrade-crds\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade,post-rollback\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: velero-server\n      initContainers:\n        - name: kubectl\n          image: \"docker.io/bitnami/kubectl:1.27\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /bin/sh\n          args:\n            - -c\n            - cp `which sh` /tmp && cp `which kubectl` /tmp\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      containers:\n        - name: velero\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /tmp/sh\n          args:\n            - -c\n            - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      volumes:\n        - name: crds\n          emptyDir: {}\n      restartPolicy: OnFailure\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/upgrade-crds.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: velero-upgrade-crds\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade,post-rollback\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: velero-server\n      initContainers:\n        - name: kubectl\n          image: \"docker.io/bitnami/kubectl:1.27\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /bin/sh\n          args:\n            - -c\n            - cp `which sh` /tmp && cp `which kubectl` /tmp\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      containers:\n        - name: velero\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /tmp/sh\n          args:\n            - -c\n            - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      volumes:\n        - name: crds\n          emptyDir: {}\n      restartPolicy: OnFailure\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/upgrade-crds.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: velero-upgrade-crds\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade,post-rollback\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: velero-server\n      initContainers:\n        - name: kubectl\n          image: \"docker.io/bitnami/kubectl:1.27\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /bin/sh\n          args:\n            - -c\n            - cp `which sh` /tmp && cp `which kubectl` /tmp\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      containers:\n        - name: velero\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /tmp/sh\n          args:\n            - -c\n            - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      volumes:\n        - name: crds\n          emptyDir: {}\n      restartPolicy: OnFailure\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/upgrade-crds.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: velero-upgrade-crds\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade,post-rollback\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: velero-server\n      initContainers:\n        - name: kubectl\n          image: \"docker.io/bitnami/kubectl:1.27\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /bin/sh\n          args:\n            - -c\n            - cp `which sh` /tmp && cp `which kubectl` /tmp\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      containers:\n        - name: velero\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /tmp/sh\n          args:\n            - -c\n            - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      volumes:\n        - name: crds\n          emptyDir: {}\n      restartPolicy: OnFailure\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/upgrade-crds.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: velero-upgrade-crds\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade,post-rollback\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: velero-server\n      initContainers:\n        - name: kubectl\n          image: \"docker.io/bitnami/kubectl:1.27\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /bin/sh\n          args:\n            - -c\n            - cp `which sh` /tmp && cp `which kubectl` /tmp\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      containers:\n        - name: velero\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /tmp/sh\n          args:\n            - -c\n            - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n          volumeMounts:\n            - mountPath: /tmp\n              name: crds\n      volumes:\n        - name: crds\n          emptyDir: {}\n      restartPolicy: OnFailure\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/serviceaccount-server.yaml"}, "region": {"startLine": 3, "endLine": 12, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: velero-server\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/restic-daemonset.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: restic\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  selector:\n    matchLabels:\n      name: restic\n  template:\n    metadata:\n      labels:\n        name: restic\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: velero\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: velero-2.30.1\n    spec:\n      serviceAccountName: velero-server\n      securityContext:\n        runAsUser: 0\n      volumes:\n        - name: cloud-credentials\n          secret:\n            secretName: velero\n        - name: host-pods\n          hostPath:\n            path: /var/lib/kubelet/pods\n        - name: scratch\n          emptyDir: {}\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: restic\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /velero\n          args:\n            - restic\n            - server\n            - --log-level=info\n          volumeMounts:\n            - name: cloud-credentials\n              mountPath: /credentials\n            - name: host-pods\n              mountPath: /host_pods\n              mountPropagation: HostToContainer\n            - name: scratch\n              mountPath: /scratch\n          env:\n            - name: VELERO_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: VELERO_SCRATCH_DIR\n              value: /scratch\n          securityContext:\n            privileged: false\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/restic-daemonset.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: restic\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  selector:\n    matchLabels:\n      name: restic\n  template:\n    metadata:\n      labels:\n        name: restic\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: velero\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: velero-2.30.1\n    spec:\n      serviceAccountName: velero-server\n      securityContext:\n        runAsUser: 0\n      volumes:\n        - name: cloud-credentials\n          secret:\n            secretName: velero\n        - name: host-pods\n          hostPath:\n            path: /var/lib/kubelet/pods\n        - name: scratch\n          emptyDir: {}\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: restic\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /velero\n          args:\n            - restic\n            - server\n            - --log-level=info\n          volumeMounts:\n            - name: cloud-credentials\n              mountPath: /credentials\n            - name: host-pods\n              mountPath: /host_pods\n              mountPropagation: HostToContainer\n            - name: scratch\n              mountPath: /scratch\n          env:\n            - name: VELERO_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: VELERO_SCRATCH_DIR\n              value: /scratch\n          securityContext:\n            privileged: false\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/restic-daemonset.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: restic\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  selector:\n    matchLabels:\n      name: restic\n  template:\n    metadata:\n      labels:\n        name: restic\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: velero\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: velero-2.30.1\n    spec:\n      serviceAccountName: velero-server\n      securityContext:\n        runAsUser: 0\n      volumes:\n        - name: cloud-credentials\n          secret:\n            secretName: velero\n        - name: host-pods\n          hostPath:\n            path: /var/lib/kubelet/pods\n        - name: scratch\n          emptyDir: {}\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: restic\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /velero\n          args:\n            - restic\n            - server\n            - --log-level=info\n          volumeMounts:\n            - name: cloud-credentials\n              mountPath: /credentials\n            - name: host-pods\n              mountPath: /host_pods\n              mountPropagation: HostToContainer\n            - name: scratch\n              mountPath: /scratch\n          env:\n            - name: VELERO_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: VELERO_SCRATCH_DIR\n              value: /scratch\n          securityContext:\n            privileged: false\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/restic-daemonset.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: restic\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  selector:\n    matchLabels:\n      name: restic\n  template:\n    metadata:\n      labels:\n        name: restic\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: velero\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: velero-2.30.1\n    spec:\n      serviceAccountName: velero-server\n      securityContext:\n        runAsUser: 0\n      volumes:\n        - name: cloud-credentials\n          secret:\n            secretName: velero\n        - name: host-pods\n          hostPath:\n            path: /var/lib/kubelet/pods\n        - name: scratch\n          emptyDir: {}\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: restic\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /velero\n          args:\n            - restic\n            - server\n            - --log-level=info\n          volumeMounts:\n            - name: cloud-credentials\n              mountPath: /credentials\n            - name: host-pods\n              mountPath: /host_pods\n              mountPropagation: HostToContainer\n            - name: scratch\n              mountPath: /scratch\n          env:\n            - name: VELERO_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: VELERO_SCRATCH_DIR\n              value: /scratch\n          securityContext:\n            privileged: false\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/restic-daemonset.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: restic\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  selector:\n    matchLabels:\n      name: restic\n  template:\n    metadata:\n      labels:\n        name: restic\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: velero\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: velero-2.30.1\n    spec:\n      serviceAccountName: velero-server\n      securityContext:\n        runAsUser: 0\n      volumes:\n        - name: cloud-credentials\n          secret:\n            secretName: velero\n        - name: host-pods\n          hostPath:\n            path: /var/lib/kubelet/pods\n        - name: scratch\n          emptyDir: {}\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: restic\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /velero\n          args:\n            - restic\n            - server\n            - --log-level=info\n          volumeMounts:\n            - name: cloud-credentials\n              mountPath: /credentials\n            - name: host-pods\n              mountPath: /host_pods\n              mountPropagation: HostToContainer\n            - name: scratch\n              mountPath: /scratch\n          env:\n            - name: VELERO_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: VELERO_SCRATCH_DIR\n              value: /scratch\n          securityContext:\n            privileged: false\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/restic-daemonset.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: restic\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  selector:\n    matchLabels:\n      name: restic\n  template:\n    metadata:\n      labels:\n        name: restic\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: velero\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: velero-2.30.1\n    spec:\n      serviceAccountName: velero-server\n      securityContext:\n        runAsUser: 0\n      volumes:\n        - name: cloud-credentials\n          secret:\n            secretName: velero\n        - name: host-pods\n          hostPath:\n            path: /var/lib/kubelet/pods\n        - name: scratch\n          emptyDir: {}\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: restic\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /velero\n          args:\n            - restic\n            - server\n            - --log-level=info\n          volumeMounts:\n            - name: cloud-credentials\n              mountPath: /credentials\n            - name: host-pods\n              mountPath: /host_pods\n              mountPropagation: HostToContainer\n            - name: scratch\n              mountPath: /scratch\n          env:\n            - name: VELERO_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: VELERO_SCRATCH_DIR\n              value: /scratch\n          securityContext:\n            privileged: false\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/restic-daemonset.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: restic\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  selector:\n    matchLabels:\n      name: restic\n  template:\n    metadata:\n      labels:\n        name: restic\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: velero\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: velero-2.30.1\n    spec:\n      serviceAccountName: velero-server\n      securityContext:\n        runAsUser: 0\n      volumes:\n        - name: cloud-credentials\n          secret:\n            secretName: velero\n        - name: host-pods\n          hostPath:\n            path: /var/lib/kubelet/pods\n        - name: scratch\n          emptyDir: {}\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: restic\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /velero\n          args:\n            - restic\n            - server\n            - --log-level=info\n          volumeMounts:\n            - name: cloud-credentials\n              mountPath: /credentials\n            - name: host-pods\n              mountPath: /host_pods\n              mountPropagation: HostToContainer\n            - name: scratch\n              mountPath: /scratch\n          env:\n            - name: VELERO_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: VELERO_SCRATCH_DIR\n              value: /scratch\n          securityContext:\n            privileged: false\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/restic-daemonset.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: restic\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  selector:\n    matchLabels:\n      name: restic\n  template:\n    metadata:\n      labels:\n        name: restic\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: velero\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: velero-2.30.1\n    spec:\n      serviceAccountName: velero-server\n      securityContext:\n        runAsUser: 0\n      volumes:\n        - name: cloud-credentials\n          secret:\n            secretName: velero\n        - name: host-pods\n          hostPath:\n            path: /var/lib/kubelet/pods\n        - name: scratch\n          emptyDir: {}\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: restic\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /velero\n          args:\n            - restic\n            - server\n            - --log-level=info\n          volumeMounts:\n            - name: cloud-credentials\n              mountPath: /credentials\n            - name: host-pods\n              mountPath: /host_pods\n              mountPropagation: HostToContainer\n            - name: scratch\n              mountPath: /scratch\n          env:\n            - name: VELERO_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: VELERO_SCRATCH_DIR\n              value: /scratch\n          securityContext:\n            privileged: false\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/restic-daemonset.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: restic\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  selector:\n    matchLabels:\n      name: restic\n  template:\n    metadata:\n      labels:\n        name: restic\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: velero\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: velero-2.30.1\n    spec:\n      serviceAccountName: velero-server\n      securityContext:\n        runAsUser: 0\n      volumes:\n        - name: cloud-credentials\n          secret:\n            secretName: velero\n        - name: host-pods\n          hostPath:\n            path: /var/lib/kubelet/pods\n        - name: scratch\n          emptyDir: {}\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: restic\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /velero\n          args:\n            - restic\n            - server\n            - --log-level=info\n          volumeMounts:\n            - name: cloud-credentials\n              mountPath: /credentials\n            - name: host-pods\n              mountPath: /host_pods\n              mountPropagation: HostToContainer\n            - name: scratch\n              mountPath: /scratch\n          env:\n            - name: VELERO_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: VELERO_SCRATCH_DIR\n              value: /scratch\n          securityContext:\n            privileged: false\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/restic-daemonset.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: restic\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  selector:\n    matchLabels:\n      name: restic\n  template:\n    metadata:\n      labels:\n        name: restic\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: velero\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: velero-2.30.1\n    spec:\n      serviceAccountName: velero-server\n      securityContext:\n        runAsUser: 0\n      volumes:\n        - name: cloud-credentials\n          secret:\n            secretName: velero\n        - name: host-pods\n          hostPath:\n            path: /var/lib/kubelet/pods\n        - name: scratch\n          emptyDir: {}\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: restic\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /velero\n          args:\n            - restic\n            - server\n            - --log-level=info\n          volumeMounts:\n            - name: cloud-credentials\n              mountPath: /credentials\n            - name: host-pods\n              mountPath: /host_pods\n              mountPropagation: HostToContainer\n            - name: scratch\n              mountPath: /scratch\n          env:\n            - name: VELERO_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: VELERO_SCRATCH_DIR\n              value: /scratch\n          securityContext:\n            privileged: false\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/restic-daemonset.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: restic\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  selector:\n    matchLabels:\n      name: restic\n  template:\n    metadata:\n      labels:\n        name: restic\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: velero\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: velero-2.30.1\n    spec:\n      serviceAccountName: velero-server\n      securityContext:\n        runAsUser: 0\n      volumes:\n        - name: cloud-credentials\n          secret:\n            secretName: velero\n        - name: host-pods\n          hostPath:\n            path: /var/lib/kubelet/pods\n        - name: scratch\n          emptyDir: {}\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: restic\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /velero\n          args:\n            - restic\n            - server\n            - --log-level=info\n          volumeMounts:\n            - name: cloud-credentials\n              mountPath: /credentials\n            - name: host-pods\n              mountPath: /host_pods\n              mountPropagation: HostToContainer\n            - name: scratch\n              mountPath: /scratch\n          env:\n            - name: VELERO_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: VELERO_SCRATCH_DIR\n              value: /scratch\n          securityContext:\n            privileged: false\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/restic-daemonset.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: restic\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  selector:\n    matchLabels:\n      name: restic\n  template:\n    metadata:\n      labels:\n        name: restic\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: velero\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: velero-2.30.1\n    spec:\n      serviceAccountName: velero-server\n      securityContext:\n        runAsUser: 0\n      volumes:\n        - name: cloud-credentials\n          secret:\n            secretName: velero\n        - name: host-pods\n          hostPath:\n            path: /var/lib/kubelet/pods\n        - name: scratch\n          emptyDir: {}\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: restic\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /velero\n          args:\n            - restic\n            - server\n            - --log-level=info\n          volumeMounts:\n            - name: cloud-credentials\n              mountPath: /credentials\n            - name: host-pods\n              mountPath: /host_pods\n              mountPropagation: HostToContainer\n            - name: scratch\n              mountPath: /scratch\n          env:\n            - name: VELERO_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: VELERO_SCRATCH_DIR\n              value: /scratch\n          securityContext:\n            privileged: false\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/restic-daemonset.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: restic\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  selector:\n    matchLabels:\n      name: restic\n  template:\n    metadata:\n      labels:\n        name: restic\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: velero\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: velero-2.30.1\n    spec:\n      serviceAccountName: velero-server\n      securityContext:\n        runAsUser: 0\n      volumes:\n        - name: cloud-credentials\n          secret:\n            secretName: velero\n        - name: host-pods\n          hostPath:\n            path: /var/lib/kubelet/pods\n        - name: scratch\n          emptyDir: {}\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: restic\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /velero\n          args:\n            - restic\n            - server\n            - --log-level=info\n          volumeMounts:\n            - name: cloud-credentials\n              mountPath: /credentials\n            - name: host-pods\n              mountPath: /host_pods\n              mountPropagation: HostToContainer\n            - name: scratch\n              mountPath: /scratch\n          env:\n            - name: VELERO_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: VELERO_SCRATCH_DIR\n              value: /scratch\n          securityContext:\n            privileged: false\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/restic-daemonset.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: restic\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  selector:\n    matchLabels:\n      name: restic\n  template:\n    metadata:\n      labels:\n        name: restic\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: velero\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: velero-2.30.1\n    spec:\n      serviceAccountName: velero-server\n      securityContext:\n        runAsUser: 0\n      volumes:\n        - name: cloud-credentials\n          secret:\n            secretName: velero\n        - name: host-pods\n          hostPath:\n            path: /var/lib/kubelet/pods\n        - name: scratch\n          emptyDir: {}\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: restic\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /velero\n          args:\n            - restic\n            - server\n            - --log-level=info\n          volumeMounts:\n            - name: cloud-credentials\n              mountPath: /credentials\n            - name: host-pods\n              mountPath: /host_pods\n              mountPropagation: HostToContainer\n            - name: scratch\n              mountPath: /scratch\n          env:\n            - name: VELERO_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: VELERO_SCRATCH_DIR\n              value: /scratch\n          securityContext:\n            privileged: false\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/restic-daemonset.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: restic\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  selector:\n    matchLabels:\n      name: restic\n  template:\n    metadata:\n      labels:\n        name: restic\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: velero\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: velero-2.30.1\n    spec:\n      serviceAccountName: velero-server\n      securityContext:\n        runAsUser: 0\n      volumes:\n        - name: cloud-credentials\n          secret:\n            secretName: velero\n        - name: host-pods\n          hostPath:\n            path: /var/lib/kubelet/pods\n        - name: scratch\n          emptyDir: {}\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: restic\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /velero\n          args:\n            - restic\n            - server\n            - --log-level=info\n          volumeMounts:\n            - name: cloud-credentials\n              mountPath: /credentials\n            - name: host-pods\n              mountPath: /host_pods\n              mountPropagation: HostToContainer\n            - name: scratch\n              mountPath: /scratch\n          env:\n            - name: VELERO_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: VELERO_SCRATCH_DIR\n              value: /scratch\n          securityContext:\n            privileged: false\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/restic-daemonset.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: restic\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  selector:\n    matchLabels:\n      name: restic\n  template:\n    metadata:\n      labels:\n        name: restic\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: velero\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: velero-2.30.1\n    spec:\n      serviceAccountName: velero-server\n      securityContext:\n        runAsUser: 0\n      volumes:\n        - name: cloud-credentials\n          secret:\n            secretName: velero\n        - name: host-pods\n          hostPath:\n            path: /var/lib/kubelet/pods\n        - name: scratch\n          emptyDir: {}\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: restic\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /velero\n          args:\n            - restic\n            - server\n            - --log-level=info\n          volumeMounts:\n            - name: cloud-credentials\n              mountPath: /credentials\n            - name: host-pods\n              mountPath: /host_pods\n              mountPropagation: HostToContainer\n            - name: scratch\n              mountPath: /scratch\n          env:\n            - name: VELERO_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: VELERO_SCRATCH_DIR\n              value: /scratch\n          securityContext:\n            privileged: false\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/restic-daemonset.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: restic\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  selector:\n    matchLabels:\n      name: restic\n  template:\n    metadata:\n      labels:\n        name: restic\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: velero\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: velero-2.30.1\n    spec:\n      serviceAccountName: velero-server\n      securityContext:\n        runAsUser: 0\n      volumes:\n        - name: cloud-credentials\n          secret:\n            secretName: velero\n        - name: host-pods\n          hostPath:\n            path: /var/lib/kubelet/pods\n        - name: scratch\n          emptyDir: {}\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: restic\n          image: \"velero/velero:v1.9.0\"\n          imagePullPolicy: IfNotPresent\n          command:\n            - /velero\n          args:\n            - restic\n            - server\n            - --log-level=info\n          volumeMounts:\n            - name: cloud-credentials\n              mountPath: /credentials\n            - name: host-pods\n              mountPath: /host_pods\n              mountPropagation: HostToContainer\n            - name: scratch\n              mountPath: /scratch\n          env:\n            - name: VELERO_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: VELERO_SCRATCH_DIR\n              value: /scratch\n          securityContext:\n            privileged: false\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/service.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nspec:\n  type: ClusterIP\n  ports:\n    - name: http-monitoring\n      port: 8085\n      targetPort: http-monitoring\n  selector:\n    name: velero\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/rolebinding.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: velero-server\n  namespace: default\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\nsubjects:\n  - kind: ServiceAccount\n    namespace: default\n    name: velero-server\nroleRef:\n  kind: Role\n  name: velero-server\n  apiGroup: rbac.authorization.k8s.io\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/secret.yaml"}, "region": {"startLine": 3, "endLine": 14, "snippet": {"text": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\ntype: Opaque\ndata:\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/velero/templates/configmaps.yaml"}, "region": {"startLine": 3, "endLine": 16, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: velero-restic-restore-action-config\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: velero\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-2.30.1\n    velero.io/plugin-config: \"\"\n    velero.io/restic: RestoreItemAction\ndata:\n  image: velero/velero-restic-restore-helper:v1.3.1\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-operator/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: istio-operator\n  name: istio-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: istio-operator\n  template:\n    metadata:\n      labels:\n        name: istio-operator\n    spec:\n      serviceAccountName: istio-operator\n      containers:\n        - name: istio-operator\n          image: docker.io/istio/operator:1.10.4\n          command:\n          - operator\n          - server\n          securityContext:\n            allowPrivilegeEscalation: false\n            capabilities:\n              drop:\n              - ALL\n            privileged: false\n            readOnlyRootFilesystem: true\n            runAsGroup: 1337\n            runAsUser: 1337\n            runAsNonRoot: true\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              cpu: 200m\n              memory: 256Mi\n            requests:\n              cpu: 50m\n              memory: 128Mi\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"istio-system\"\n            - name: LEADER_ELECTION_NAMESPACE\n              value: \"istio-operator\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"istio-operator\"\n            - name: WAIT_FOR_RESOURCES_TIMEOUT\n              value: \"300s\"\n            - name: REVISION\n              value: \"\"\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-operator/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: istio-operator\n  name: istio-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: istio-operator\n  template:\n    metadata:\n      labels:\n        name: istio-operator\n    spec:\n      serviceAccountName: istio-operator\n      containers:\n        - name: istio-operator\n          image: docker.io/istio/operator:1.10.4\n          command:\n          - operator\n          - server\n          securityContext:\n            allowPrivilegeEscalation: false\n            capabilities:\n              drop:\n              - ALL\n            privileged: false\n            readOnlyRootFilesystem: true\n            runAsGroup: 1337\n            runAsUser: 1337\n            runAsNonRoot: true\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              cpu: 200m\n              memory: 256Mi\n            requests:\n              cpu: 50m\n              memory: 128Mi\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"istio-system\"\n            - name: LEADER_ELECTION_NAMESPACE\n              value: \"istio-operator\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"istio-operator\"\n            - name: WAIT_FOR_RESOURCES_TIMEOUT\n              value: \"300s\"\n            - name: REVISION\n              value: \"\"\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-operator/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: istio-operator\n  name: istio-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: istio-operator\n  template:\n    metadata:\n      labels:\n        name: istio-operator\n    spec:\n      serviceAccountName: istio-operator\n      containers:\n        - name: istio-operator\n          image: docker.io/istio/operator:1.10.4\n          command:\n          - operator\n          - server\n          securityContext:\n            allowPrivilegeEscalation: false\n            capabilities:\n              drop:\n              - ALL\n            privileged: false\n            readOnlyRootFilesystem: true\n            runAsGroup: 1337\n            runAsUser: 1337\n            runAsNonRoot: true\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              cpu: 200m\n              memory: 256Mi\n            requests:\n              cpu: 50m\n              memory: 128Mi\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"istio-system\"\n            - name: LEADER_ELECTION_NAMESPACE\n              value: \"istio-operator\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"istio-operator\"\n            - name: WAIT_FOR_RESOURCES_TIMEOUT\n              value: \"300s\"\n            - name: REVISION\n              value: \"\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-operator/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: istio-operator\n  name: istio-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: istio-operator\n  template:\n    metadata:\n      labels:\n        name: istio-operator\n    spec:\n      serviceAccountName: istio-operator\n      containers:\n        - name: istio-operator\n          image: docker.io/istio/operator:1.10.4\n          command:\n          - operator\n          - server\n          securityContext:\n            allowPrivilegeEscalation: false\n            capabilities:\n              drop:\n              - ALL\n            privileged: false\n            readOnlyRootFilesystem: true\n            runAsGroup: 1337\n            runAsUser: 1337\n            runAsNonRoot: true\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              cpu: 200m\n              memory: 256Mi\n            requests:\n              cpu: 50m\n              memory: 128Mi\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"istio-system\"\n            - name: LEADER_ELECTION_NAMESPACE\n              value: \"istio-operator\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"istio-operator\"\n            - name: WAIT_FOR_RESOURCES_TIMEOUT\n              value: \"300s\"\n            - name: REVISION\n              value: \"\"\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-operator/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: istio-operator\n  name: istio-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: istio-operator\n  template:\n    metadata:\n      labels:\n        name: istio-operator\n    spec:\n      serviceAccountName: istio-operator\n      containers:\n        - name: istio-operator\n          image: docker.io/istio/operator:1.10.4\n          command:\n          - operator\n          - server\n          securityContext:\n            allowPrivilegeEscalation: false\n            capabilities:\n              drop:\n              - ALL\n            privileged: false\n            readOnlyRootFilesystem: true\n            runAsGroup: 1337\n            runAsUser: 1337\n            runAsNonRoot: true\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              cpu: 200m\n              memory: 256Mi\n            requests:\n              cpu: 50m\n              memory: 128Mi\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"istio-system\"\n            - name: LEADER_ELECTION_NAMESPACE\n              value: \"istio-operator\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"istio-operator\"\n            - name: WAIT_FOR_RESOURCES_TIMEOUT\n              value: \"300s\"\n            - name: REVISION\n              value: \"\"\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-operator/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: istio-operator\n  name: istio-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: istio-operator\n  template:\n    metadata:\n      labels:\n        name: istio-operator\n    spec:\n      serviceAccountName: istio-operator\n      containers:\n        - name: istio-operator\n          image: docker.io/istio/operator:1.10.4\n          command:\n          - operator\n          - server\n          securityContext:\n            allowPrivilegeEscalation: false\n            capabilities:\n              drop:\n              - ALL\n            privileged: false\n            readOnlyRootFilesystem: true\n            runAsGroup: 1337\n            runAsUser: 1337\n            runAsNonRoot: true\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              cpu: 200m\n              memory: 256Mi\n            requests:\n              cpu: 50m\n              memory: 128Mi\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"istio-system\"\n            - name: LEADER_ELECTION_NAMESPACE\n              value: \"istio-operator\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"istio-operator\"\n            - name: WAIT_FOR_RESOURCES_TIMEOUT\n              value: \"300s\"\n            - name: REVISION\n              value: \"\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-operator/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: istio-operator\n  name: istio-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: istio-operator\n  template:\n    metadata:\n      labels:\n        name: istio-operator\n    spec:\n      serviceAccountName: istio-operator\n      containers:\n        - name: istio-operator\n          image: docker.io/istio/operator:1.10.4\n          command:\n          - operator\n          - server\n          securityContext:\n            allowPrivilegeEscalation: false\n            capabilities:\n              drop:\n              - ALL\n            privileged: false\n            readOnlyRootFilesystem: true\n            runAsGroup: 1337\n            runAsUser: 1337\n            runAsNonRoot: true\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              cpu: 200m\n              memory: 256Mi\n            requests:\n              cpu: 50m\n              memory: 128Mi\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"istio-system\"\n            - name: LEADER_ELECTION_NAMESPACE\n              value: \"istio-operator\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"istio-operator\"\n            - name: WAIT_FOR_RESOURCES_TIMEOUT\n              value: \"300s\"\n            - name: REVISION\n              value: \"\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-operator/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: istio-operator\n  name: istio-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: istio-operator\n  template:\n    metadata:\n      labels:\n        name: istio-operator\n    spec:\n      serviceAccountName: istio-operator\n      containers:\n        - name: istio-operator\n          image: docker.io/istio/operator:1.10.4\n          command:\n          - operator\n          - server\n          securityContext:\n            allowPrivilegeEscalation: false\n            capabilities:\n              drop:\n              - ALL\n            privileged: false\n            readOnlyRootFilesystem: true\n            runAsGroup: 1337\n            runAsUser: 1337\n            runAsNonRoot: true\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              cpu: 200m\n              memory: 256Mi\n            requests:\n              cpu: 50m\n              memory: 128Mi\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"istio-system\"\n            - name: LEADER_ELECTION_NAMESPACE\n              value: \"istio-operator\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"istio-operator\"\n            - name: WAIT_FOR_RESOURCES_TIMEOUT\n              value: \"300s\"\n            - name: REVISION\n              value: \"\"\n"}}}}]}, {"ruleId": "CKV_K8S_158", "ruleIndex": 22, "level": "error", "attachments": [], "message": {"text": "Minimize Roles and ClusterRoles that grant permissions to escalate Roles or ClusterRoles"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-operator/templates/clusterrole.yaml"}, "region": {"startLine": 3, "endLine": 116, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  creationTimestamp: null\n  name: istio-operator\nrules:\n# istio groups\n- apiGroups:\n  - authentication.istio.io\n  resources:\n  - '*'\n  verbs:\n  - '*'\n- apiGroups:\n  - config.istio.io\n  resources:\n  - '*'\n  verbs:\n  - '*'\n- apiGroups:\n  - install.istio.io\n  resources:\n  - '*'\n  verbs:\n  - '*'\n- apiGroups:\n  - networking.istio.io\n  resources:\n  - '*'\n  verbs:\n  - '*'\n- apiGroups:\n  - security.istio.io\n  resources:\n  - '*'\n  verbs:\n  - '*'\n# k8s groups\n- apiGroups:\n  - admissionregistration.k8s.io\n  resources:\n  - mutatingwebhookconfigurations\n  - validatingwebhookconfigurations\n  verbs:\n  - '*'\n- apiGroups:\n  - apiextensions.k8s.io\n  resources:\n  - customresourcedefinitions.apiextensions.k8s.io\n  - customresourcedefinitions\n  verbs:\n  - '*'\n- apiGroups:\n  - apps\n  - extensions\n  resources:\n  - daemonsets\n  - deployments\n  - deployments/finalizers\n  - replicasets\n  verbs:\n  - '*'\n- apiGroups:\n  - autoscaling\n  resources:\n  - horizontalpodautoscalers\n  verbs:\n  - '*'\n- apiGroups:\n  - monitoring.coreos.com\n  resources:\n  - servicemonitors\n  verbs:\n  - get\n  - create\n  - update\n- apiGroups:\n  - policy\n  resources:\n  - poddisruptionbudgets\n  verbs:\n  - '*'\n- apiGroups:\n  - rbac.authorization.k8s.io\n  resources:\n  - clusterrolebindings\n  - clusterroles\n  - roles\n  - rolebindings\n  verbs:\n  - '*'\n- apiGroups:\n  - coordination.k8s.io\n  resources:\n  - leases\n  verbs:\n  - get\n  - create\n  - update\n- apiGroups:\n  - \"\"\n  resources:\n  - configmaps\n  - endpoints\n  - events\n  - namespaces\n  - pods\n  - pods/proxy\n  - persistentvolumeclaims\n  - secrets\n  - services\n  - serviceaccounts\n  verbs:\n  - '*'\n"}}}}]}, {"ruleId": "CKV_K8S_155", "ruleIndex": 25, "level": "error", "attachments": [], "message": {"text": "Minimize ClusterRoles that grant control over validating or mutating admission webhook configurations"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-operator/templates/clusterrole.yaml"}, "region": {"startLine": 3, "endLine": 116, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  creationTimestamp: null\n  name: istio-operator\nrules:\n# istio groups\n- apiGroups:\n  - authentication.istio.io\n  resources:\n  - '*'\n  verbs:\n  - '*'\n- apiGroups:\n  - config.istio.io\n  resources:\n  - '*'\n  verbs:\n  - '*'\n- apiGroups:\n  - install.istio.io\n  resources:\n  - '*'\n  verbs:\n  - '*'\n- apiGroups:\n  - networking.istio.io\n  resources:\n  - '*'\n  verbs:\n  - '*'\n- apiGroups:\n  - security.istio.io\n  resources:\n  - '*'\n  verbs:\n  - '*'\n# k8s groups\n- apiGroups:\n  - admissionregistration.k8s.io\n  resources:\n  - mutatingwebhookconfigurations\n  - validatingwebhookconfigurations\n  verbs:\n  - '*'\n- apiGroups:\n  - apiextensions.k8s.io\n  resources:\n  - customresourcedefinitions.apiextensions.k8s.io\n  - customresourcedefinitions\n  verbs:\n  - '*'\n- apiGroups:\n  - apps\n  - extensions\n  resources:\n  - daemonsets\n  - deployments\n  - deployments/finalizers\n  - replicasets\n  verbs:\n  - '*'\n- apiGroups:\n  - autoscaling\n  resources:\n  - horizontalpodautoscalers\n  verbs:\n  - '*'\n- apiGroups:\n  - monitoring.coreos.com\n  resources:\n  - servicemonitors\n  verbs:\n  - get\n  - create\n  - update\n- apiGroups:\n  - policy\n  resources:\n  - poddisruptionbudgets\n  verbs:\n  - '*'\n- apiGroups:\n  - rbac.authorization.k8s.io\n  resources:\n  - clusterrolebindings\n  - clusterroles\n  - roles\n  - rolebindings\n  verbs:\n  - '*'\n- apiGroups:\n  - coordination.k8s.io\n  resources:\n  - leases\n  verbs:\n  - get\n  - create\n  - update\n- apiGroups:\n  - \"\"\n  resources:\n  - configmaps\n  - endpoints\n  - events\n  - namespaces\n  - pods\n  - pods/proxy\n  - persistentvolumeclaims\n  - secrets\n  - services\n  - serviceaccounts\n  verbs:\n  - '*'\n"}}}}]}, {"ruleId": "CKV_K8S_157", "ruleIndex": 23, "level": "error", "attachments": [], "message": {"text": "Minimize Roles and ClusterRoles that grant permissions to bind RoleBindings or ClusterRoleBindings"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-operator/templates/clusterrole.yaml"}, "region": {"startLine": 3, "endLine": 116, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  creationTimestamp: null\n  name: istio-operator\nrules:\n# istio groups\n- apiGroups:\n  - authentication.istio.io\n  resources:\n  - '*'\n  verbs:\n  - '*'\n- apiGroups:\n  - config.istio.io\n  resources:\n  - '*'\n  verbs:\n  - '*'\n- apiGroups:\n  - install.istio.io\n  resources:\n  - '*'\n  verbs:\n  - '*'\n- apiGroups:\n  - networking.istio.io\n  resources:\n  - '*'\n  verbs:\n  - '*'\n- apiGroups:\n  - security.istio.io\n  resources:\n  - '*'\n  verbs:\n  - '*'\n# k8s groups\n- apiGroups:\n  - admissionregistration.k8s.io\n  resources:\n  - mutatingwebhookconfigurations\n  - validatingwebhookconfigurations\n  verbs:\n  - '*'\n- apiGroups:\n  - apiextensions.k8s.io\n  resources:\n  - customresourcedefinitions.apiextensions.k8s.io\n  - customresourcedefinitions\n  verbs:\n  - '*'\n- apiGroups:\n  - apps\n  - extensions\n  resources:\n  - daemonsets\n  - deployments\n  - deployments/finalizers\n  - replicasets\n  verbs:\n  - '*'\n- apiGroups:\n  - autoscaling\n  resources:\n  - horizontalpodautoscalers\n  verbs:\n  - '*'\n- apiGroups:\n  - monitoring.coreos.com\n  resources:\n  - servicemonitors\n  verbs:\n  - get\n  - create\n  - update\n- apiGroups:\n  - policy\n  resources:\n  - poddisruptionbudgets\n  verbs:\n  - '*'\n- apiGroups:\n  - rbac.authorization.k8s.io\n  resources:\n  - clusterrolebindings\n  - clusterroles\n  - roles\n  - rolebindings\n  verbs:\n  - '*'\n- apiGroups:\n  - coordination.k8s.io\n  resources:\n  - leases\n  verbs:\n  - get\n  - create\n  - update\n- apiGroups:\n  - \"\"\n  resources:\n  - configmaps\n  - endpoints\n  - events\n  - namespaces\n  - pods\n  - pods/proxy\n  - persistentvolumeclaims\n  - secrets\n  - services\n  - serviceaccounts\n  verbs:\n  - '*'\n"}}}}]}, {"ruleId": "CKV_K8S_49", "ruleIndex": 24, "level": "error", "attachments": [], "message": {"text": "Minimize wildcard use in Roles and ClusterRoles"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-operator/templates/clusterrole.yaml"}, "region": {"startLine": 3, "endLine": 116, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  creationTimestamp: null\n  name: istio-operator\nrules:\n# istio groups\n- apiGroups:\n  - authentication.istio.io\n  resources:\n  - '*'\n  verbs:\n  - '*'\n- apiGroups:\n  - config.istio.io\n  resources:\n  - '*'\n  verbs:\n  - '*'\n- apiGroups:\n  - install.istio.io\n  resources:\n  - '*'\n  verbs:\n  - '*'\n- apiGroups:\n  - networking.istio.io\n  resources:\n  - '*'\n  verbs:\n  - '*'\n- apiGroups:\n  - security.istio.io\n  resources:\n  - '*'\n  verbs:\n  - '*'\n# k8s groups\n- apiGroups:\n  - admissionregistration.k8s.io\n  resources:\n  - mutatingwebhookconfigurations\n  - validatingwebhookconfigurations\n  verbs:\n  - '*'\n- apiGroups:\n  - apiextensions.k8s.io\n  resources:\n  - customresourcedefinitions.apiextensions.k8s.io\n  - customresourcedefinitions\n  verbs:\n  - '*'\n- apiGroups:\n  - apps\n  - extensions\n  resources:\n  - daemonsets\n  - deployments\n  - deployments/finalizers\n  - replicasets\n  verbs:\n  - '*'\n- apiGroups:\n  - autoscaling\n  resources:\n  - horizontalpodautoscalers\n  verbs:\n  - '*'\n- apiGroups:\n  - monitoring.coreos.com\n  resources:\n  - servicemonitors\n  verbs:\n  - get\n  - create\n  - update\n- apiGroups:\n  - policy\n  resources:\n  - poddisruptionbudgets\n  verbs:\n  - '*'\n- apiGroups:\n  - rbac.authorization.k8s.io\n  resources:\n  - clusterrolebindings\n  - clusterroles\n  - roles\n  - rolebindings\n  verbs:\n  - '*'\n- apiGroups:\n  - coordination.k8s.io\n  resources:\n  - leases\n  verbs:\n  - get\n  - create\n  - update\n- apiGroups:\n  - \"\"\n  resources:\n  - configmaps\n  - endpoints\n  - events\n  - namespaces\n  - pods\n  - pods/proxy\n  - persistentvolumeclaims\n  - secrets\n  - services\n  - serviceaccounts\n  verbs:\n  - '*'\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"apigateway-test-connection\"\n  labels:\n     app: \"apigateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['apigateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"apigateway-test-connection\"\n  labels:\n     app: \"apigateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['apigateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"apigateway-test-connection\"\n  labels:\n     app: \"apigateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['apigateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"apigateway-test-connection\"\n  labels:\n     app: \"apigateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['apigateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"apigateway-test-connection\"\n  labels:\n     app: \"apigateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['apigateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"apigateway-test-connection\"\n  labels:\n     app: \"apigateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['apigateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"apigateway-test-connection\"\n  labels:\n     app: \"apigateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['apigateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"apigateway-test-connection\"\n  labels:\n     app: \"apigateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['apigateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"apigateway-test-connection\"\n  labels:\n     app: \"apigateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['apigateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"apigateway-test-connection\"\n  labels:\n     app: \"apigateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['apigateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"apigateway-test-connection\"\n  labels:\n     app: \"apigateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['apigateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"apigateway-test-connection\"\n  labels:\n     app: \"apigateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['apigateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"apigateway-test-connection\"\n  labels:\n     app: \"apigateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['apigateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"apigateway-test-connection\"\n  labels:\n     app: \"apigateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['apigateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"apigateway-test-connection\"\n  labels:\n     app: \"apigateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['apigateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"apigateway-test-connection\"\n  labels:\n     app: \"apigateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['apigateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"apigateway-test-connection\"\n  labels:\n     app: \"apigateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['apigateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"apigateway-test-connection\"\n  labels:\n     app: \"apigateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['apigateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"apigateway-test-connection\"\n  labels:\n     app: \"apigateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-apigateway\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['apigateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"ui-test-connection\"\n  labels:\n     app: \"ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"ui-test-connection\"\n  labels:\n     app: \"ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"ui-test-connection\"\n  labels:\n     app: \"ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"ui-test-connection\"\n  labels:\n     app: \"ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"ui-test-connection\"\n  labels:\n     app: \"ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"ui-test-connection\"\n  labels:\n     app: \"ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"ui-test-connection\"\n  labels:\n     app: \"ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"ui-test-connection\"\n  labels:\n     app: \"ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"ui-test-connection\"\n  labels:\n     app: \"ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"ui-test-connection\"\n  labels:\n     app: \"ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"ui-test-connection\"\n  labels:\n     app: \"ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"ui-test-connection\"\n  labels:\n     app: \"ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"ui-test-connection\"\n  labels:\n     app: \"ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"ui-test-connection\"\n  labels:\n     app: \"ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"ui-test-connection\"\n  labels:\n     app: \"ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"ui-test-connection\"\n  labels:\n     app: \"ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"ui-test-connection\"\n  labels:\n     app: \"ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"ui-test-connection\"\n  labels:\n     app: \"ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"ui-test-connection\"\n  labels:\n     app: \"ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/roles/Role.yaml"}, "region": {"startLine": 3, "endLine": 38, "snippet": {"text": "kind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: gallery-container-manager\n  namespace: default\nrules:\n - apiGroups:\n     - \"\"\n   resources:\n     - secrets\n   verbs:\n     - create\n     - update\n     - patch\n     - delete\n     - get\n     - watch\n     - list\n - apiGroups:\n   - \"\"\n   - \"extensions\"\n   - \"networking.k8s.io\"\n   - \"apps\"\n   resources:\n   - pods\n   - ingresses\n   - deployments\n   - services\n   verbs:\n   - create\n   - update\n   - patch\n   - delete\n   - get\n   - watch\n   - list\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/roles/ServiceAccount.yaml"}, "region": {"startLine": 3, "endLine": 7, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: gallery-container-manager\n  namespace: default\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/roles/RoleBinding.yaml"}, "region": {"startLine": 3, "endLine": 15, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: gallery-container-manager\n  namespace: default\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: gallery-container-manager\nsubjects:\n- kind: ServiceAccount\n  name: gallery-container-manager\n  namespace: default\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/services/apigateway.yaml"}, "region": {"startLine": 3, "endLine": 18, "snippet": {"text": "kind: Service\napiVersion: v1\nmetadata:\n  name: apigateway\n  namespace: default\n  labels:\n    app: apigateway\n  annotations:\nspec:\n  ports:\n    - name: http\n      port: 80\n      targetPort: 3000\n  selector:\n    app: apigateway\n  type: ClusterIP\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/services/ui.yaml"}, "region": {"startLine": 3, "endLine": 18, "snippet": {"text": "kind: Service\napiVersion: v1\nmetadata:\n  name: ui\n  namespace: default\n  labels:\n    app: ui\n  annotations:\nspec:\n  ports:\n    - name: http\n      port: 80\n      targetPort: 80\n  selector:\n    app: ui\n  type: ClusterIP\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/tool.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: tool\n  namespace: default\n  labels:\n    app: tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tool\n  template:\n    metadata:\n      name: tool\n      labels:\n        app: tool\n    spec:\n      volumes:\n        - name: \"default-tool-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-tool-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: tool\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-tool:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"tool\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-tool-logs\"\n              mountPath: \"/logs/tool/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/tool.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: tool\n  namespace: default\n  labels:\n    app: tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tool\n  template:\n    metadata:\n      name: tool\n      labels:\n        app: tool\n    spec:\n      volumes:\n        - name: \"default-tool-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-tool-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: tool\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-tool:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"tool\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-tool-logs\"\n              mountPath: \"/logs/tool/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/tool.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: tool\n  namespace: default\n  labels:\n    app: tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tool\n  template:\n    metadata:\n      name: tool\n      labels:\n        app: tool\n    spec:\n      volumes:\n        - name: \"default-tool-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-tool-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: tool\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-tool:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"tool\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-tool-logs\"\n              mountPath: \"/logs/tool/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/tool.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: tool\n  namespace: default\n  labels:\n    app: tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tool\n  template:\n    metadata:\n      name: tool\n      labels:\n        app: tool\n    spec:\n      volumes:\n        - name: \"default-tool-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-tool-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: tool\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-tool:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"tool\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-tool-logs\"\n              mountPath: \"/logs/tool/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/tool.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: tool\n  namespace: default\n  labels:\n    app: tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tool\n  template:\n    metadata:\n      name: tool\n      labels:\n        app: tool\n    spec:\n      volumes:\n        - name: \"default-tool-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-tool-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: tool\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-tool:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"tool\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-tool-logs\"\n              mountPath: \"/logs/tool/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/tool.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: tool\n  namespace: default\n  labels:\n    app: tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tool\n  template:\n    metadata:\n      name: tool\n      labels:\n        app: tool\n    spec:\n      volumes:\n        - name: \"default-tool-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-tool-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: tool\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-tool:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"tool\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-tool-logs\"\n              mountPath: \"/logs/tool/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/tool.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: tool\n  namespace: default\n  labels:\n    app: tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tool\n  template:\n    metadata:\n      name: tool\n      labels:\n        app: tool\n    spec:\n      volumes:\n        - name: \"default-tool-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-tool-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: tool\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-tool:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"tool\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-tool-logs\"\n              mountPath: \"/logs/tool/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/tool.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: tool\n  namespace: default\n  labels:\n    app: tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tool\n  template:\n    metadata:\n      name: tool\n      labels:\n        app: tool\n    spec:\n      volumes:\n        - name: \"default-tool-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-tool-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: tool\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-tool:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"tool\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-tool-logs\"\n              mountPath: \"/logs/tool/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/tool.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: tool\n  namespace: default\n  labels:\n    app: tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tool\n  template:\n    metadata:\n      name: tool\n      labels:\n        app: tool\n    spec:\n      volumes:\n        - name: \"default-tool-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-tool-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: tool\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-tool:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"tool\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-tool-logs\"\n              mountPath: \"/logs/tool/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/tool.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: tool\n  namespace: default\n  labels:\n    app: tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tool\n  template:\n    metadata:\n      name: tool\n      labels:\n        app: tool\n    spec:\n      volumes:\n        - name: \"default-tool-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-tool-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: tool\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-tool:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"tool\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-tool-logs\"\n              mountPath: \"/logs/tool/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/tool.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: tool\n  namespace: default\n  labels:\n    app: tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tool\n  template:\n    metadata:\n      name: tool\n      labels:\n        app: tool\n    spec:\n      volumes:\n        - name: \"default-tool-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-tool-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: tool\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-tool:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"tool\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-tool-logs\"\n              mountPath: \"/logs/tool/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/tool.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: tool\n  namespace: default\n  labels:\n    app: tool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tool\n  template:\n    metadata:\n      name: tool\n      labels:\n        app: tool\n    spec:\n      volumes:\n        - name: \"default-tool-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-tool-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: tool\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-tool:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"tool\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-tool-logs\"\n              mountPath: \"/logs/tool/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/apigateway.yaml"}, "region": {"startLine": 3, "endLine": 81, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: apigateway\n  namespace: default\n  labels:\n    app: apigateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apigateway\n  template:\n    metadata:\n      name: apigateway\n      labels:\n        app: apigateway\n    spec:\n      volumes:\n        - name: \"default-apigateway-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-apigateway-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: apigateway\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-gateway:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  3000\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-apigateway-logs\"\n              mountPath: \"/logs/apigateway/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n      initContainers:\n        - name: \"gallery-init-db\"\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-init-db:1.1.7-22.03\"\n          imagePullPolicy: Always\n          env:\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"postgres\"\n            - name: DB_PASSWORD\n              value: \"tea\"\n            - name: ATTEMPTS\n              value: \"15\"             \n            - name: DELAY_RETRY\n              value: \"30000\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/apigateway.yaml"}, "region": {"startLine": 3, "endLine": 81, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: apigateway\n  namespace: default\n  labels:\n    app: apigateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apigateway\n  template:\n    metadata:\n      name: apigateway\n      labels:\n        app: apigateway\n    spec:\n      volumes:\n        - name: \"default-apigateway-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-apigateway-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: apigateway\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-gateway:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  3000\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-apigateway-logs\"\n              mountPath: \"/logs/apigateway/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n      initContainers:\n        - name: \"gallery-init-db\"\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-init-db:1.1.7-22.03\"\n          imagePullPolicy: Always\n          env:\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"postgres\"\n            - name: DB_PASSWORD\n              value: \"tea\"\n            - name: ATTEMPTS\n              value: \"15\"             \n            - name: DELAY_RETRY\n              value: \"30000\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/apigateway.yaml"}, "region": {"startLine": 3, "endLine": 81, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: apigateway\n  namespace: default\n  labels:\n    app: apigateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apigateway\n  template:\n    metadata:\n      name: apigateway\n      labels:\n        app: apigateway\n    spec:\n      volumes:\n        - name: \"default-apigateway-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-apigateway-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: apigateway\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-gateway:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  3000\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-apigateway-logs\"\n              mountPath: \"/logs/apigateway/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n      initContainers:\n        - name: \"gallery-init-db\"\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-init-db:1.1.7-22.03\"\n          imagePullPolicy: Always\n          env:\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"postgres\"\n            - name: DB_PASSWORD\n              value: \"tea\"\n            - name: ATTEMPTS\n              value: \"15\"             \n            - name: DELAY_RETRY\n              value: \"30000\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/apigateway.yaml"}, "region": {"startLine": 3, "endLine": 81, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: apigateway\n  namespace: default\n  labels:\n    app: apigateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apigateway\n  template:\n    metadata:\n      name: apigateway\n      labels:\n        app: apigateway\n    spec:\n      volumes:\n        - name: \"default-apigateway-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-apigateway-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: apigateway\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-gateway:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  3000\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-apigateway-logs\"\n              mountPath: \"/logs/apigateway/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n      initContainers:\n        - name: \"gallery-init-db\"\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-init-db:1.1.7-22.03\"\n          imagePullPolicy: Always\n          env:\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"postgres\"\n            - name: DB_PASSWORD\n              value: \"tea\"\n            - name: ATTEMPTS\n              value: \"15\"             \n            - name: DELAY_RETRY\n              value: \"30000\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/apigateway.yaml"}, "region": {"startLine": 3, "endLine": 81, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: apigateway\n  namespace: default\n  labels:\n    app: apigateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apigateway\n  template:\n    metadata:\n      name: apigateway\n      labels:\n        app: apigateway\n    spec:\n      volumes:\n        - name: \"default-apigateway-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-apigateway-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: apigateway\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-gateway:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  3000\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-apigateway-logs\"\n              mountPath: \"/logs/apigateway/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n      initContainers:\n        - name: \"gallery-init-db\"\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-init-db:1.1.7-22.03\"\n          imagePullPolicy: Always\n          env:\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"postgres\"\n            - name: DB_PASSWORD\n              value: \"tea\"\n            - name: ATTEMPTS\n              value: \"15\"             \n            - name: DELAY_RETRY\n              value: \"30000\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/apigateway.yaml"}, "region": {"startLine": 3, "endLine": 81, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: apigateway\n  namespace: default\n  labels:\n    app: apigateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apigateway\n  template:\n    metadata:\n      name: apigateway\n      labels:\n        app: apigateway\n    spec:\n      volumes:\n        - name: \"default-apigateway-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-apigateway-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: apigateway\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-gateway:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  3000\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-apigateway-logs\"\n              mountPath: \"/logs/apigateway/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n      initContainers:\n        - name: \"gallery-init-db\"\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-init-db:1.1.7-22.03\"\n          imagePullPolicy: Always\n          env:\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"postgres\"\n            - name: DB_PASSWORD\n              value: \"tea\"\n            - name: ATTEMPTS\n              value: \"15\"             \n            - name: DELAY_RETRY\n              value: \"30000\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/apigateway.yaml"}, "region": {"startLine": 3, "endLine": 81, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: apigateway\n  namespace: default\n  labels:\n    app: apigateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apigateway\n  template:\n    metadata:\n      name: apigateway\n      labels:\n        app: apigateway\n    spec:\n      volumes:\n        - name: \"default-apigateway-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-apigateway-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: apigateway\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-gateway:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  3000\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-apigateway-logs\"\n              mountPath: \"/logs/apigateway/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n      initContainers:\n        - name: \"gallery-init-db\"\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-init-db:1.1.7-22.03\"\n          imagePullPolicy: Always\n          env:\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"postgres\"\n            - name: DB_PASSWORD\n              value: \"tea\"\n            - name: ATTEMPTS\n              value: \"15\"             \n            - name: DELAY_RETRY\n              value: \"30000\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/apigateway.yaml"}, "region": {"startLine": 3, "endLine": 81, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: apigateway\n  namespace: default\n  labels:\n    app: apigateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apigateway\n  template:\n    metadata:\n      name: apigateway\n      labels:\n        app: apigateway\n    spec:\n      volumes:\n        - name: \"default-apigateway-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-apigateway-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: apigateway\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-gateway:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  3000\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-apigateway-logs\"\n              mountPath: \"/logs/apigateway/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n      initContainers:\n        - name: \"gallery-init-db\"\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-init-db:1.1.7-22.03\"\n          imagePullPolicy: Always\n          env:\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"postgres\"\n            - name: DB_PASSWORD\n              value: \"tea\"\n            - name: ATTEMPTS\n              value: \"15\"             \n            - name: DELAY_RETRY\n              value: \"30000\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/apigateway.yaml"}, "region": {"startLine": 3, "endLine": 81, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: apigateway\n  namespace: default\n  labels:\n    app: apigateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apigateway\n  template:\n    metadata:\n      name: apigateway\n      labels:\n        app: apigateway\n    spec:\n      volumes:\n        - name: \"default-apigateway-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-apigateway-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: apigateway\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-gateway:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  3000\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-apigateway-logs\"\n              mountPath: \"/logs/apigateway/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n      initContainers:\n        - name: \"gallery-init-db\"\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-init-db:1.1.7-22.03\"\n          imagePullPolicy: Always\n          env:\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"postgres\"\n            - name: DB_PASSWORD\n              value: \"tea\"\n            - name: ATTEMPTS\n              value: \"15\"             \n            - name: DELAY_RETRY\n              value: \"30000\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/apigateway.yaml"}, "region": {"startLine": 3, "endLine": 81, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: apigateway\n  namespace: default\n  labels:\n    app: apigateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apigateway\n  template:\n    metadata:\n      name: apigateway\n      labels:\n        app: apigateway\n    spec:\n      volumes:\n        - name: \"default-apigateway-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-apigateway-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: apigateway\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-gateway:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  3000\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-apigateway-logs\"\n              mountPath: \"/logs/apigateway/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n      initContainers:\n        - name: \"gallery-init-db\"\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-init-db:1.1.7-22.03\"\n          imagePullPolicy: Always\n          env:\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"postgres\"\n            - name: DB_PASSWORD\n              value: \"tea\"\n            - name: ATTEMPTS\n              value: \"15\"             \n            - name: DELAY_RETRY\n              value: \"30000\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/apigateway.yaml"}, "region": {"startLine": 3, "endLine": 81, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: apigateway\n  namespace: default\n  labels:\n    app: apigateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apigateway\n  template:\n    metadata:\n      name: apigateway\n      labels:\n        app: apigateway\n    spec:\n      volumes:\n        - name: \"default-apigateway-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-apigateway-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: apigateway\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-gateway:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  3000\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-apigateway-logs\"\n              mountPath: \"/logs/apigateway/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n      initContainers:\n        - name: \"gallery-init-db\"\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-init-db:1.1.7-22.03\"\n          imagePullPolicy: Always\n          env:\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"postgres\"\n            - name: DB_PASSWORD\n              value: \"tea\"\n            - name: ATTEMPTS\n              value: \"15\"             \n            - name: DELAY_RETRY\n              value: \"30000\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/apigateway.yaml"}, "region": {"startLine": 3, "endLine": 81, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: apigateway\n  namespace: default\n  labels:\n    app: apigateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apigateway\n  template:\n    metadata:\n      name: apigateway\n      labels:\n        app: apigateway\n    spec:\n      volumes:\n        - name: \"default-apigateway-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-apigateway-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: apigateway\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-gateway:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  3000\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-apigateway-logs\"\n              mountPath: \"/logs/apigateway/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n      initContainers:\n        - name: \"gallery-init-db\"\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-init-db:1.1.7-22.03\"\n          imagePullPolicy: Always\n          env:\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"postgres\"\n            - name: DB_PASSWORD\n              value: \"tea\"\n            - name: ATTEMPTS\n              value: \"15\"             \n            - name: DELAY_RETRY\n              value: \"30000\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/apigateway.yaml"}, "region": {"startLine": 3, "endLine": 81, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: apigateway\n  namespace: default\n  labels:\n    app: apigateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apigateway\n  template:\n    metadata:\n      name: apigateway\n      labels:\n        app: apigateway\n    spec:\n      volumes:\n        - name: \"default-apigateway-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-apigateway-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: apigateway\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-gateway:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  3000\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-apigateway-logs\"\n              mountPath: \"/logs/apigateway/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n      initContainers:\n        - name: \"gallery-init-db\"\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-init-db:1.1.7-22.03\"\n          imagePullPolicy: Always\n          env:\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"postgres\"\n            - name: DB_PASSWORD\n              value: \"tea\"\n            - name: ATTEMPTS\n              value: \"15\"             \n            - name: DELAY_RETRY\n              value: \"30000\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/apigateway.yaml"}, "region": {"startLine": 3, "endLine": 81, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: apigateway\n  namespace: default\n  labels:\n    app: apigateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apigateway\n  template:\n    metadata:\n      name: apigateway\n      labels:\n        app: apigateway\n    spec:\n      volumes:\n        - name: \"default-apigateway-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-apigateway-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: apigateway\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-gateway:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  3000\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-apigateway-logs\"\n              mountPath: \"/logs/apigateway/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n      initContainers:\n        - name: \"gallery-init-db\"\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-init-db:1.1.7-22.03\"\n          imagePullPolicy: Always\n          env:\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"postgres\"\n            - name: DB_PASSWORD\n              value: \"tea\"\n            - name: ATTEMPTS\n              value: \"15\"             \n            - name: DELAY_RETRY\n              value: \"30000\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/apigateway.yaml"}, "region": {"startLine": 3, "endLine": 81, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: apigateway\n  namespace: default\n  labels:\n    app: apigateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apigateway\n  template:\n    metadata:\n      name: apigateway\n      labels:\n        app: apigateway\n    spec:\n      volumes:\n        - name: \"default-apigateway-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-apigateway-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: apigateway\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-gateway:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  3000\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-apigateway-logs\"\n              mountPath: \"/logs/apigateway/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n      initContainers:\n        - name: \"gallery-init-db\"\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-init-db:1.1.7-22.03\"\n          imagePullPolicy: Always\n          env:\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"postgres\"\n            - name: DB_PASSWORD\n              value: \"tea\"\n            - name: ATTEMPTS\n              value: \"15\"             \n            - name: DELAY_RETRY\n              value: \"30000\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/apigateway.yaml"}, "region": {"startLine": 3, "endLine": 81, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: apigateway\n  namespace: default\n  labels:\n    app: apigateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apigateway\n  template:\n    metadata:\n      name: apigateway\n      labels:\n        app: apigateway\n    spec:\n      volumes:\n        - name: \"default-apigateway-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-apigateway-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: apigateway\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-gateway:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  3000\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-apigateway-logs\"\n              mountPath: \"/logs/apigateway/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n      initContainers:\n        - name: \"gallery-init-db\"\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-init-db:1.1.7-22.03\"\n          imagePullPolicy: Always\n          env:\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"postgres\"\n            - name: DB_PASSWORD\n              value: \"tea\"\n            - name: ATTEMPTS\n              value: \"15\"             \n            - name: DELAY_RETRY\n              value: \"30000\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/apigateway.yaml"}, "region": {"startLine": 3, "endLine": 81, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: apigateway\n  namespace: default\n  labels:\n    app: apigateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apigateway\n  template:\n    metadata:\n      name: apigateway\n      labels:\n        app: apigateway\n    spec:\n      volumes:\n        - name: \"default-apigateway-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-apigateway-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: apigateway\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-gateway:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  3000\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-apigateway-logs\"\n              mountPath: \"/logs/apigateway/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n      initContainers:\n        - name: \"gallery-init-db\"\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-init-db:1.1.7-22.03\"\n          imagePullPolicy: Always\n          env:\n            - name: USP_ENV_MS_NAME\n              value: \"apigateway\"\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"postgres\"\n            - name: DB_PASSWORD\n              value: \"tea\"\n            - name: ATTEMPTS\n              value: \"15\"             \n            - name: DELAY_RETRY\n              value: \"30000\"\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  namespace: default\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      securityContext:\n        null\n      containers:\n        - name: ui\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-ui:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  80\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://gallery.gateway.usp.dev.wrstudio1.cloud\\\",\\\"authMsUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.id.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n            - name: USP_ENV_MS_NAME\n              value: \"ui\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          securityContext:\n            null\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  namespace: default\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      securityContext:\n        null\n      containers:\n        - name: ui\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-ui:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  80\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://gallery.gateway.usp.dev.wrstudio1.cloud\\\",\\\"authMsUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.id.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n            - name: USP_ENV_MS_NAME\n              value: \"ui\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          securityContext:\n            null\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  namespace: default\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      securityContext:\n        null\n      containers:\n        - name: ui\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-ui:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  80\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://gallery.gateway.usp.dev.wrstudio1.cloud\\\",\\\"authMsUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.id.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n            - name: USP_ENV_MS_NAME\n              value: \"ui\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          securityContext:\n            null\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  namespace: default\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      securityContext:\n        null\n      containers:\n        - name: ui\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-ui:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  80\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://gallery.gateway.usp.dev.wrstudio1.cloud\\\",\\\"authMsUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.id.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n            - name: USP_ENV_MS_NAME\n              value: \"ui\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          securityContext:\n            null\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  namespace: default\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      securityContext:\n        null\n      containers:\n        - name: ui\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-ui:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  80\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://gallery.gateway.usp.dev.wrstudio1.cloud\\\",\\\"authMsUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.id.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n            - name: USP_ENV_MS_NAME\n              value: \"ui\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          securityContext:\n            null\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  namespace: default\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      securityContext:\n        null\n      containers:\n        - name: ui\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-ui:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  80\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://gallery.gateway.usp.dev.wrstudio1.cloud\\\",\\\"authMsUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.id.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n            - name: USP_ENV_MS_NAME\n              value: \"ui\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          securityContext:\n            null\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  namespace: default\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      securityContext:\n        null\n      containers:\n        - name: ui\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-ui:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  80\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://gallery.gateway.usp.dev.wrstudio1.cloud\\\",\\\"authMsUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.id.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n            - name: USP_ENV_MS_NAME\n              value: \"ui\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          securityContext:\n            null\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  namespace: default\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      securityContext:\n        null\n      containers:\n        - name: ui\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-ui:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  80\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://gallery.gateway.usp.dev.wrstudio1.cloud\\\",\\\"authMsUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.id.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n            - name: USP_ENV_MS_NAME\n              value: \"ui\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          securityContext:\n            null\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  namespace: default\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      securityContext:\n        null\n      containers:\n        - name: ui\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-ui:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  80\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://gallery.gateway.usp.dev.wrstudio1.cloud\\\",\\\"authMsUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.id.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n            - name: USP_ENV_MS_NAME\n              value: \"ui\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          securityContext:\n            null\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  namespace: default\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      securityContext:\n        null\n      containers:\n        - name: ui\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-ui:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  80\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://gallery.gateway.usp.dev.wrstudio1.cloud\\\",\\\"authMsUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.id.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n            - name: USP_ENV_MS_NAME\n              value: \"ui\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          securityContext:\n            null\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  namespace: default\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      securityContext:\n        null\n      containers:\n        - name: ui\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-ui:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  80\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://gallery.gateway.usp.dev.wrstudio1.cloud\\\",\\\"authMsUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.id.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n            - name: USP_ENV_MS_NAME\n              value: \"ui\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          securityContext:\n            null\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  namespace: default\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      securityContext:\n        null\n      containers:\n        - name: ui\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-ui:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  80\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://gallery.gateway.usp.dev.wrstudio1.cloud\\\",\\\"authMsUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.id.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n            - name: USP_ENV_MS_NAME\n              value: \"ui\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          securityContext:\n            null\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  namespace: default\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      securityContext:\n        null\n      containers:\n        - name: ui\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-ui:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  80\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://gallery.gateway.usp.dev.wrstudio1.cloud\\\",\\\"authMsUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.id.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n            - name: USP_ENV_MS_NAME\n              value: \"ui\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          securityContext:\n            null\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ui\n  namespace: default\n  labels:\n    app: ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ui\n  template:\n    metadata:\n      name: ui\n      labels:\n        app: ui\n    spec:\n      securityContext:\n        null\n      containers:\n        - name: ui\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-ui:1.1.7-22.03\"\n          imagePullPolicy: Always\n          ports:\n            - containerPort:  80\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://gallery.gateway.usp.dev.wrstudio1.cloud\\\",\\\"authMsUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.id.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n            - name: USP_ENV_MS_NAME\n              value: \"ui\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          securityContext:\n            null\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/container.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: container\n  namespace: default\n  labels:\n    app: container\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: container\n  template:\n    metadata:\n      name: container\n      labels:\n        app: container\n    spec:\n      volumes:\n        - name: \"default-container-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-container-logs\"\n      securityContext:\n        {}\n      serviceAccount: gallery-container-manager\n      serviceAccountName: gallery-container-manager\n      containers:\n        - name: container\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-container:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"container\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-container-logs\"\n              mountPath: \"/logs/container/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/container.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: container\n  namespace: default\n  labels:\n    app: container\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: container\n  template:\n    metadata:\n      name: container\n      labels:\n        app: container\n    spec:\n      volumes:\n        - name: \"default-container-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-container-logs\"\n      securityContext:\n        {}\n      serviceAccount: gallery-container-manager\n      serviceAccountName: gallery-container-manager\n      containers:\n        - name: container\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-container:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"container\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-container-logs\"\n              mountPath: \"/logs/container/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/container.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: container\n  namespace: default\n  labels:\n    app: container\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: container\n  template:\n    metadata:\n      name: container\n      labels:\n        app: container\n    spec:\n      volumes:\n        - name: \"default-container-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-container-logs\"\n      securityContext:\n        {}\n      serviceAccount: gallery-container-manager\n      serviceAccountName: gallery-container-manager\n      containers:\n        - name: container\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-container:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"container\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-container-logs\"\n              mountPath: \"/logs/container/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/container.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: container\n  namespace: default\n  labels:\n    app: container\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: container\n  template:\n    metadata:\n      name: container\n      labels:\n        app: container\n    spec:\n      volumes:\n        - name: \"default-container-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-container-logs\"\n      securityContext:\n        {}\n      serviceAccount: gallery-container-manager\n      serviceAccountName: gallery-container-manager\n      containers:\n        - name: container\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-container:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"container\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-container-logs\"\n              mountPath: \"/logs/container/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/container.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: container\n  namespace: default\n  labels:\n    app: container\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: container\n  template:\n    metadata:\n      name: container\n      labels:\n        app: container\n    spec:\n      volumes:\n        - name: \"default-container-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-container-logs\"\n      securityContext:\n        {}\n      serviceAccount: gallery-container-manager\n      serviceAccountName: gallery-container-manager\n      containers:\n        - name: container\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-container:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"container\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-container-logs\"\n              mountPath: \"/logs/container/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/container.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: container\n  namespace: default\n  labels:\n    app: container\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: container\n  template:\n    metadata:\n      name: container\n      labels:\n        app: container\n    spec:\n      volumes:\n        - name: \"default-container-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-container-logs\"\n      securityContext:\n        {}\n      serviceAccount: gallery-container-manager\n      serviceAccountName: gallery-container-manager\n      containers:\n        - name: container\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-container:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"container\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-container-logs\"\n              mountPath: \"/logs/container/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/container.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: container\n  namespace: default\n  labels:\n    app: container\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: container\n  template:\n    metadata:\n      name: container\n      labels:\n        app: container\n    spec:\n      volumes:\n        - name: \"default-container-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-container-logs\"\n      securityContext:\n        {}\n      serviceAccount: gallery-container-manager\n      serviceAccountName: gallery-container-manager\n      containers:\n        - name: container\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-container:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"container\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-container-logs\"\n              mountPath: \"/logs/container/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/container.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: container\n  namespace: default\n  labels:\n    app: container\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: container\n  template:\n    metadata:\n      name: container\n      labels:\n        app: container\n    spec:\n      volumes:\n        - name: \"default-container-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-container-logs\"\n      securityContext:\n        {}\n      serviceAccount: gallery-container-manager\n      serviceAccountName: gallery-container-manager\n      containers:\n        - name: container\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-container:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"container\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-container-logs\"\n              mountPath: \"/logs/container/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/container.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: container\n  namespace: default\n  labels:\n    app: container\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: container\n  template:\n    metadata:\n      name: container\n      labels:\n        app: container\n    spec:\n      volumes:\n        - name: \"default-container-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-container-logs\"\n      securityContext:\n        {}\n      serviceAccount: gallery-container-manager\n      serviceAccountName: gallery-container-manager\n      containers:\n        - name: container\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-container:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"container\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-container-logs\"\n              mountPath: \"/logs/container/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/container.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: container\n  namespace: default\n  labels:\n    app: container\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: container\n  template:\n    metadata:\n      name: container\n      labels:\n        app: container\n    spec:\n      volumes:\n        - name: \"default-container-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-container-logs\"\n      securityContext:\n        {}\n      serviceAccount: gallery-container-manager\n      serviceAccountName: gallery-container-manager\n      containers:\n        - name: container\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-container:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"container\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-container-logs\"\n              mountPath: \"/logs/container/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/container.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: container\n  namespace: default\n  labels:\n    app: container\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: container\n  template:\n    metadata:\n      name: container\n      labels:\n        app: container\n    spec:\n      volumes:\n        - name: \"default-container-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-container-logs\"\n      securityContext:\n        {}\n      serviceAccount: gallery-container-manager\n      serviceAccountName: gallery-container-manager\n      containers:\n        - name: container\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-container:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"container\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-container-logs\"\n              mountPath: \"/logs/container/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/container.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: container\n  namespace: default\n  labels:\n    app: container\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: container\n  template:\n    metadata:\n      name: container\n      labels:\n        app: container\n    spec:\n      volumes:\n        - name: \"default-container-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-container-logs\"\n      securityContext:\n        {}\n      serviceAccount: gallery-container-manager\n      serviceAccountName: gallery-container-manager\n      containers:\n        - name: container\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-container:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"container\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-container-logs\"\n              mountPath: \"/logs/container/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/media.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: media\n  namespace: default\n  labels:\n    app: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: media\n  template:\n    metadata:\n      name: media\n      labels:\n        app: media\n    spec:\n      volumes:\n        - name: \"default-media-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-media-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: media\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-media:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"media\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-media-logs\"\n              mountPath: \"/logs/media/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/media.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: media\n  namespace: default\n  labels:\n    app: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: media\n  template:\n    metadata:\n      name: media\n      labels:\n        app: media\n    spec:\n      volumes:\n        - name: \"default-media-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-media-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: media\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-media:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"media\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-media-logs\"\n              mountPath: \"/logs/media/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/media.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: media\n  namespace: default\n  labels:\n    app: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: media\n  template:\n    metadata:\n      name: media\n      labels:\n        app: media\n    spec:\n      volumes:\n        - name: \"default-media-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-media-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: media\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-media:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"media\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-media-logs\"\n              mountPath: \"/logs/media/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/media.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: media\n  namespace: default\n  labels:\n    app: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: media\n  template:\n    metadata:\n      name: media\n      labels:\n        app: media\n    spec:\n      volumes:\n        - name: \"default-media-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-media-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: media\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-media:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"media\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-media-logs\"\n              mountPath: \"/logs/media/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/media.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: media\n  namespace: default\n  labels:\n    app: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: media\n  template:\n    metadata:\n      name: media\n      labels:\n        app: media\n    spec:\n      volumes:\n        - name: \"default-media-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-media-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: media\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-media:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"media\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-media-logs\"\n              mountPath: \"/logs/media/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/media.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: media\n  namespace: default\n  labels:\n    app: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: media\n  template:\n    metadata:\n      name: media\n      labels:\n        app: media\n    spec:\n      volumes:\n        - name: \"default-media-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-media-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: media\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-media:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"media\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-media-logs\"\n              mountPath: \"/logs/media/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/media.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: media\n  namespace: default\n  labels:\n    app: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: media\n  template:\n    metadata:\n      name: media\n      labels:\n        app: media\n    spec:\n      volumes:\n        - name: \"default-media-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-media-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: media\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-media:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"media\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-media-logs\"\n              mountPath: \"/logs/media/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/media.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: media\n  namespace: default\n  labels:\n    app: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: media\n  template:\n    metadata:\n      name: media\n      labels:\n        app: media\n    spec:\n      volumes:\n        - name: \"default-media-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-media-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: media\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-media:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"media\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-media-logs\"\n              mountPath: \"/logs/media/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/media.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: media\n  namespace: default\n  labels:\n    app: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: media\n  template:\n    metadata:\n      name: media\n      labels:\n        app: media\n    spec:\n      volumes:\n        - name: \"default-media-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-media-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: media\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-media:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"media\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-media-logs\"\n              mountPath: \"/logs/media/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/media.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: media\n  namespace: default\n  labels:\n    app: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: media\n  template:\n    metadata:\n      name: media\n      labels:\n        app: media\n    spec:\n      volumes:\n        - name: \"default-media-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-media-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: media\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-media:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"media\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-media-logs\"\n              mountPath: \"/logs/media/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/media.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: media\n  namespace: default\n  labels:\n    app: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: media\n  template:\n    metadata:\n      name: media\n      labels:\n        app: media\n    spec:\n      volumes:\n        - name: \"default-media-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-media-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: media\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-media:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"media\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-media-logs\"\n              mountPath: \"/logs/media/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gallery/templates/deployments/media.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: media\n  namespace: default\n  labels:\n    app: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: media\n  template:\n    metadata:\n      name: media\n      labels:\n        app: media\n    spec:\n      volumes:\n        - name: \"default-media-logs\"\n          persistentVolumeClaim:\n            claimName: \"default-media-logs\"\n        - name: \"default-storage\"\n          persistentVolumeClaim:\n            claimName: \"default-storage\"\n      securityContext:\n        {}\n      containers:\n        - name: media\n          image: \"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-media:1.1.7-22.03\"\n          imagePullPolicy: Always\n          command: [\"npm\", \"run\",start:dockerEnvironment]\n          env:\n            - name: USP_ENV_CONFIG\n              value: \"{\\\"authApiUrl\\\":\\\"https://mcp.gateway.usp.dev.wrstudio1.cloud\\\",\\\"installToolImage\\\":\\\"system.registry.dev.wrstudio1.cloud/wr-studio-product/wrai-usp/gallery-install-tool\\\",\\\"installToolNamespace\\\":\\\"prod-wrai-usp-gallery\\\",\\\"installToolTag\\\":\\\"1.1.7-22.03\\\",\\\"kafka\\\":{\\\"clientId\\\":\\\"devKafka\\\",\\\"cluster\\\":[\\\"usp-kafka-0-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-1-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\",\\\"usp-kafka-2-external.prod-wrai-usp-datapipeline.svc.cluster.local:80\\\"]},\\\"plmApiUrl\\\":\\\"https://plm.gateway.dev.wrstudio1.cloud\\\",\\\"postgre\\\":{\\\"database\\\":\\\"usp\\\",\\\"host\\\":\\\"usp-gallery-patroni.prod-wrai-usp-gallery.svc.cluster.local\\\",\\\"password\\\":\\\"tea\\\",\\\"port\\\":5432,\\\"ssl\\\":{\\\"rejectUnauthorized\\\":false},\\\"user\\\":\\\"postgres\\\"}}\"\n            - name: USP_ENV_MS_NAME\n              value: \"media\"\n          resources:\n            limits:\n              cpu: 500m\n              memory: 1024Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n          volumeMounts:\n            - name: \"default-media-logs\"\n              mountPath: \"/logs/media/\"\n            - name: \"default-storage\"\n              mountPath: \"/storage/media/uploaded-files/\"\n          securityContext:\n            {}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager\n  namespace: \"default\"\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"controller\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"controller\"\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"controller\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n      annotations:\n        prometheus.io/path: \"/metrics\"\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: cert-manager\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-controller:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --cluster-resource-namespace=$(POD_NAMESPACE)\n          - --leader-election-namespace=kube-system\n          ports:\n          - containerPort: 9402\n            protocol: TCP\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager\n  namespace: \"default\"\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"controller\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"controller\"\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"controller\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n      annotations:\n        prometheus.io/path: \"/metrics\"\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: cert-manager\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-controller:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --cluster-resource-namespace=$(POD_NAMESPACE)\n          - --leader-election-namespace=kube-system\n          ports:\n          - containerPort: 9402\n            protocol: TCP\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager\n  namespace: \"default\"\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"controller\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"controller\"\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"controller\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n      annotations:\n        prometheus.io/path: \"/metrics\"\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: cert-manager\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-controller:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --cluster-resource-namespace=$(POD_NAMESPACE)\n          - --leader-election-namespace=kube-system\n          ports:\n          - containerPort: 9402\n            protocol: TCP\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager\n  namespace: \"default\"\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"controller\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"controller\"\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"controller\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n      annotations:\n        prometheus.io/path: \"/metrics\"\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: cert-manager\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-controller:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --cluster-resource-namespace=$(POD_NAMESPACE)\n          - --leader-election-namespace=kube-system\n          ports:\n          - containerPort: 9402\n            protocol: TCP\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager\n  namespace: \"default\"\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"controller\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"controller\"\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"controller\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n      annotations:\n        prometheus.io/path: \"/metrics\"\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: cert-manager\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-controller:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --cluster-resource-namespace=$(POD_NAMESPACE)\n          - --leader-election-namespace=kube-system\n          ports:\n          - containerPort: 9402\n            protocol: TCP\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager\n  namespace: \"default\"\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"controller\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"controller\"\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"controller\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n      annotations:\n        prometheus.io/path: \"/metrics\"\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: cert-manager\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-controller:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --cluster-resource-namespace=$(POD_NAMESPACE)\n          - --leader-election-namespace=kube-system\n          ports:\n          - containerPort: 9402\n            protocol: TCP\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager\n  namespace: \"default\"\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"controller\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"controller\"\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"controller\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n      annotations:\n        prometheus.io/path: \"/metrics\"\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: cert-manager\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-controller:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --cluster-resource-namespace=$(POD_NAMESPACE)\n          - --leader-election-namespace=kube-system\n          ports:\n          - containerPort: 9402\n            protocol: TCP\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager\n  namespace: \"default\"\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"controller\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"controller\"\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"controller\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n      annotations:\n        prometheus.io/path: \"/metrics\"\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: cert-manager\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-controller:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --cluster-resource-namespace=$(POD_NAMESPACE)\n          - --leader-election-namespace=kube-system\n          ports:\n          - containerPort: 9402\n            protocol: TCP\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager\n  namespace: \"default\"\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"controller\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"controller\"\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"controller\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n      annotations:\n        prometheus.io/path: \"/metrics\"\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: cert-manager\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-controller:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --cluster-resource-namespace=$(POD_NAMESPACE)\n          - --leader-election-namespace=kube-system\n          ports:\n          - containerPort: 9402\n            protocol: TCP\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager\n  namespace: \"default\"\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"controller\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"controller\"\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"controller\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n      annotations:\n        prometheus.io/path: \"/metrics\"\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: cert-manager\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-controller:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --cluster-resource-namespace=$(POD_NAMESPACE)\n          - --leader-election-namespace=kube-system\n          ports:\n          - containerPort: 9402\n            protocol: TCP\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager\n  namespace: \"default\"\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"controller\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"controller\"\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"controller\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n      annotations:\n        prometheus.io/path: \"/metrics\"\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: cert-manager\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-controller:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --cluster-resource-namespace=$(POD_NAMESPACE)\n          - --leader-election-namespace=kube-system\n          ports:\n          - containerPort: 9402\n            protocol: TCP\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager\n  namespace: \"default\"\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"controller\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"controller\"\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"controller\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n      annotations:\n        prometheus.io/path: \"/metrics\"\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: cert-manager\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-controller:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --cluster-resource-namespace=$(POD_NAMESPACE)\n          - --leader-election-namespace=kube-system\n          ports:\n          - containerPort: 9402\n            protocol: TCP\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager\n  namespace: \"default\"\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"controller\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"controller\"\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"controller\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n      annotations:\n        prometheus.io/path: \"/metrics\"\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: cert-manager\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-controller:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --cluster-resource-namespace=$(POD_NAMESPACE)\n          - --leader-election-namespace=kube-system\n          ports:\n          - containerPort: 9402\n            protocol: TCP\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager\n  namespace: \"default\"\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"controller\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"controller\"\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"controller\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n      annotations:\n        prometheus.io/path: \"/metrics\"\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: cert-manager\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-controller:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --cluster-resource-namespace=$(POD_NAMESPACE)\n          - --leader-election-namespace=kube-system\n          ports:\n          - containerPort: 9402\n            protocol: TCP\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 58, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager\n  namespace: \"default\"\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"controller\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"controller\"\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"controller\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n      annotations:\n        prometheus.io/path: \"/metrics\"\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: cert-manager\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-controller:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --cluster-resource-namespace=$(POD_NAMESPACE)\n          - --leader-election-namespace=kube-system\n          ports:\n          - containerPort: 9402\n            protocol: TCP\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 77, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-webhook\n  namespace: \"default\"\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"webhook\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"webhook\"\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"webhook\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-webhook\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-webhook:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --secure-port=10250\n          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca\n          - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.default,cert-manager-webhook.default.svc\n          ports:\n          - name: https\n            protocol: TCP\n            containerPort: 10250\n          livenessProbe:\n            httpGet:\n              path: /livez\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 60\n            periodSeconds: 10\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /healthz\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 77, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-webhook\n  namespace: \"default\"\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"webhook\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"webhook\"\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"webhook\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-webhook\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-webhook:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --secure-port=10250\n          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca\n          - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.default,cert-manager-webhook.default.svc\n          ports:\n          - name: https\n            protocol: TCP\n            containerPort: 10250\n          livenessProbe:\n            httpGet:\n              path: /livez\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 60\n            periodSeconds: 10\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /healthz\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 77, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-webhook\n  namespace: \"default\"\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"webhook\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"webhook\"\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"webhook\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-webhook\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-webhook:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --secure-port=10250\n          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca\n          - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.default,cert-manager-webhook.default.svc\n          ports:\n          - name: https\n            protocol: TCP\n            containerPort: 10250\n          livenessProbe:\n            httpGet:\n              path: /livez\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 60\n            periodSeconds: 10\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /healthz\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 77, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-webhook\n  namespace: \"default\"\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"webhook\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"webhook\"\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"webhook\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-webhook\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-webhook:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --secure-port=10250\n          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca\n          - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.default,cert-manager-webhook.default.svc\n          ports:\n          - name: https\n            protocol: TCP\n            containerPort: 10250\n          livenessProbe:\n            httpGet:\n              path: /livez\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 60\n            periodSeconds: 10\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /healthz\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 77, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-webhook\n  namespace: \"default\"\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"webhook\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"webhook\"\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"webhook\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-webhook\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-webhook:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --secure-port=10250\n          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca\n          - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.default,cert-manager-webhook.default.svc\n          ports:\n          - name: https\n            protocol: TCP\n            containerPort: 10250\n          livenessProbe:\n            httpGet:\n              path: /livez\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 60\n            periodSeconds: 10\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /healthz\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 77, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-webhook\n  namespace: \"default\"\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"webhook\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"webhook\"\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"webhook\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-webhook\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-webhook:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --secure-port=10250\n          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca\n          - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.default,cert-manager-webhook.default.svc\n          ports:\n          - name: https\n            protocol: TCP\n            containerPort: 10250\n          livenessProbe:\n            httpGet:\n              path: /livez\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 60\n            periodSeconds: 10\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /healthz\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 77, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-webhook\n  namespace: \"default\"\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"webhook\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"webhook\"\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"webhook\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-webhook\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-webhook:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --secure-port=10250\n          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca\n          - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.default,cert-manager-webhook.default.svc\n          ports:\n          - name: https\n            protocol: TCP\n            containerPort: 10250\n          livenessProbe:\n            httpGet:\n              path: /livez\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 60\n            periodSeconds: 10\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /healthz\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 77, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-webhook\n  namespace: \"default\"\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"webhook\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"webhook\"\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"webhook\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-webhook\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-webhook:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --secure-port=10250\n          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca\n          - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.default,cert-manager-webhook.default.svc\n          ports:\n          - name: https\n            protocol: TCP\n            containerPort: 10250\n          livenessProbe:\n            httpGet:\n              path: /livez\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 60\n            periodSeconds: 10\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /healthz\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 77, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-webhook\n  namespace: \"default\"\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"webhook\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"webhook\"\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"webhook\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-webhook\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-webhook:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --secure-port=10250\n          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca\n          - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.default,cert-manager-webhook.default.svc\n          ports:\n          - name: https\n            protocol: TCP\n            containerPort: 10250\n          livenessProbe:\n            httpGet:\n              path: /livez\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 60\n            periodSeconds: 10\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /healthz\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 77, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-webhook\n  namespace: \"default\"\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"webhook\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"webhook\"\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"webhook\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-webhook\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-webhook:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --secure-port=10250\n          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca\n          - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.default,cert-manager-webhook.default.svc\n          ports:\n          - name: https\n            protocol: TCP\n            containerPort: 10250\n          livenessProbe:\n            httpGet:\n              path: /livez\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 60\n            periodSeconds: 10\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /healthz\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 77, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-webhook\n  namespace: \"default\"\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"webhook\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"webhook\"\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"webhook\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-webhook\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-webhook:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --secure-port=10250\n          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca\n          - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.default,cert-manager-webhook.default.svc\n          ports:\n          - name: https\n            protocol: TCP\n            containerPort: 10250\n          livenessProbe:\n            httpGet:\n              path: /livez\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 60\n            periodSeconds: 10\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /healthz\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 77, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-webhook\n  namespace: \"default\"\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"webhook\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"webhook\"\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"webhook\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-webhook\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-webhook:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --secure-port=10250\n          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca\n          - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.default,cert-manager-webhook.default.svc\n          ports:\n          - name: https\n            protocol: TCP\n            containerPort: 10250\n          livenessProbe:\n            httpGet:\n              path: /livez\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 60\n            periodSeconds: 10\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /healthz\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 77, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-webhook\n  namespace: \"default\"\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"webhook\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"webhook\"\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"webhook\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-webhook\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-webhook:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --secure-port=10250\n          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca\n          - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.default,cert-manager-webhook.default.svc\n          ports:\n          - name: https\n            protocol: TCP\n            containerPort: 10250\n          livenessProbe:\n            httpGet:\n              path: /livez\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 60\n            periodSeconds: 10\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          readinessProbe:\n            httpGet:\n              path: /healthz\n              port: 6080\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/webhook-serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 16, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nautomountServiceAccountToken: true\nmetadata:\n  name: cert-manager-webhook\n  namespace: \"default\"\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"webhook\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/cainjector-deployment.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-cainjector\n  namespace: \"default\"\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"cainjector\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"cainjector\"\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"cainjector\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-cainjector\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-cainjector:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --leader-election-namespace=kube-system\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/cainjector-deployment.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-cainjector\n  namespace: \"default\"\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"cainjector\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"cainjector\"\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"cainjector\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-cainjector\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-cainjector:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --leader-election-namespace=kube-system\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/cainjector-deployment.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-cainjector\n  namespace: \"default\"\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"cainjector\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"cainjector\"\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"cainjector\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-cainjector\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-cainjector:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --leader-election-namespace=kube-system\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/cainjector-deployment.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-cainjector\n  namespace: \"default\"\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"cainjector\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"cainjector\"\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"cainjector\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-cainjector\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-cainjector:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --leader-election-namespace=kube-system\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/cainjector-deployment.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-cainjector\n  namespace: \"default\"\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"cainjector\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"cainjector\"\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"cainjector\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-cainjector\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-cainjector:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --leader-election-namespace=kube-system\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/cainjector-deployment.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-cainjector\n  namespace: \"default\"\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"cainjector\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"cainjector\"\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"cainjector\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-cainjector\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-cainjector:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --leader-election-namespace=kube-system\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/cainjector-deployment.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-cainjector\n  namespace: \"default\"\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"cainjector\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"cainjector\"\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"cainjector\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-cainjector\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-cainjector:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --leader-election-namespace=kube-system\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/cainjector-deployment.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-cainjector\n  namespace: \"default\"\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"cainjector\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"cainjector\"\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"cainjector\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-cainjector\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-cainjector:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --leader-election-namespace=kube-system\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/cainjector-deployment.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-cainjector\n  namespace: \"default\"\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"cainjector\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"cainjector\"\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"cainjector\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-cainjector\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-cainjector:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --leader-election-namespace=kube-system\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/cainjector-deployment.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-cainjector\n  namespace: \"default\"\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"cainjector\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"cainjector\"\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"cainjector\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-cainjector\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-cainjector:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --leader-election-namespace=kube-system\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/cainjector-deployment.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-cainjector\n  namespace: \"default\"\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"cainjector\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"cainjector\"\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"cainjector\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-cainjector\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-cainjector:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --leader-election-namespace=kube-system\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/cainjector-deployment.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-cainjector\n  namespace: \"default\"\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"cainjector\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"cainjector\"\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"cainjector\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-cainjector\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-cainjector:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --leader-election-namespace=kube-system\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/cainjector-deployment.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-cainjector\n  namespace: \"default\"\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"cainjector\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"cainjector\"\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"cainjector\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-cainjector\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-cainjector:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --leader-election-namespace=kube-system\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/cainjector-deployment.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-cainjector\n  namespace: \"default\"\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"cainjector\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"cainjector\"\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"cainjector\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-cainjector\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-cainjector:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --leader-election-namespace=kube-system\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/cainjector-deployment.yaml"}, "region": {"startLine": 3, "endLine": 50, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cert-manager-cainjector\n  namespace: \"default\"\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"cainjector\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: cert-manager\n      app.kubernetes.io/component: \"cainjector\"\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"cainjector\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      serviceAccountName: cert-manager-cainjector\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-cainjector:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - --v=2\n          - --leader-election-namespace=kube-system\n          env:\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/webhook-service.yaml"}, "region": {"startLine": 3, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: cert-manager-webhook\n  namespace: \"default\"\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"webhook\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  type: ClusterIP\n  ports:\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: 10250\n  selector:\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"webhook\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/startupapicheck-serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 20, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nautomountServiceAccountToken: true\nmetadata:\n  name: cert-manager-startupapicheck\n  namespace: \"default\"\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: \"-5\"\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"startupapicheck\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/startupapicheck-job.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cert-manager-startupapicheck\n  namespace: \"default\"\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"startupapicheck\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: \"1\"\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"startupapicheck\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: cert-manager-startupapicheck\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-ctl:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - check\n          - api\n          - --wait=1m\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/startupapicheck-job.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cert-manager-startupapicheck\n  namespace: \"default\"\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"startupapicheck\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: \"1\"\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"startupapicheck\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: cert-manager-startupapicheck\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-ctl:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - check\n          - api\n          - --wait=1m\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/startupapicheck-job.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cert-manager-startupapicheck\n  namespace: \"default\"\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"startupapicheck\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: \"1\"\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"startupapicheck\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: cert-manager-startupapicheck\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-ctl:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - check\n          - api\n          - --wait=1m\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/startupapicheck-job.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cert-manager-startupapicheck\n  namespace: \"default\"\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"startupapicheck\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: \"1\"\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"startupapicheck\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: cert-manager-startupapicheck\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-ctl:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - check\n          - api\n          - --wait=1m\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/startupapicheck-job.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cert-manager-startupapicheck\n  namespace: \"default\"\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"startupapicheck\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: \"1\"\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"startupapicheck\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: cert-manager-startupapicheck\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-ctl:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - check\n          - api\n          - --wait=1m\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/startupapicheck-job.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cert-manager-startupapicheck\n  namespace: \"default\"\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"startupapicheck\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: \"1\"\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"startupapicheck\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: cert-manager-startupapicheck\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-ctl:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - check\n          - api\n          - --wait=1m\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/startupapicheck-job.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cert-manager-startupapicheck\n  namespace: \"default\"\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"startupapicheck\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: \"1\"\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"startupapicheck\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: cert-manager-startupapicheck\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-ctl:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - check\n          - api\n          - --wait=1m\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/startupapicheck-job.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cert-manager-startupapicheck\n  namespace: \"default\"\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"startupapicheck\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: \"1\"\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"startupapicheck\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: cert-manager-startupapicheck\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-ctl:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - check\n          - api\n          - --wait=1m\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/startupapicheck-job.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cert-manager-startupapicheck\n  namespace: \"default\"\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"startupapicheck\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: \"1\"\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"startupapicheck\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: cert-manager-startupapicheck\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-ctl:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - check\n          - api\n          - --wait=1m\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/startupapicheck-job.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cert-manager-startupapicheck\n  namespace: \"default\"\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"startupapicheck\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: \"1\"\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"startupapicheck\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: cert-manager-startupapicheck\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-ctl:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - check\n          - api\n          - --wait=1m\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/startupapicheck-job.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cert-manager-startupapicheck\n  namespace: \"default\"\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"startupapicheck\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: \"1\"\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"startupapicheck\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: cert-manager-startupapicheck\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-ctl:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - check\n          - api\n          - --wait=1m\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/startupapicheck-job.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cert-manager-startupapicheck\n  namespace: \"default\"\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"startupapicheck\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: \"1\"\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"startupapicheck\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: cert-manager-startupapicheck\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-ctl:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - check\n          - api\n          - --wait=1m\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/startupapicheck-job.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cert-manager-startupapicheck\n  namespace: \"default\"\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"startupapicheck\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: \"1\"\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: cert-manager\n        app.kubernetes.io/component: \"startupapicheck\"\n        app.kubernetes.io/version: \"v1.5.4\"\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.5.4\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: cert-manager-startupapicheck\n      securityContext:\n        runAsNonRoot: true\n      containers:\n        - name: cert-manager\n          image: \"quay.io/jetstack/cert-manager-ctl:v1.5.4\"\n          imagePullPolicy: IfNotPresent\n          args:\n          - check\n          - api\n          - --wait=1m\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/service.yaml"}, "region": {"startLine": 3, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: cert-manager\n  namespace: \"default\"\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"controller\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nspec:\n  type: ClusterIP\n  ports:\n    - protocol: TCP\n      port: 9402\n      name: tcp-prometheus-servicemonitor\n      targetPort: 9402\n  selector:\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"controller\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/webhook-config.yaml"}, "region": {"startLine": 3, "endLine": 13, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cert-manager-webhook\n  namespace: \"default\"\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"webhook\"\ndata:\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/webhook-rbac.yaml"}, "region": {"startLine": 44, "endLine": 67, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: cert-manager-webhook:dynamic-serving\n  namespace: \"default\"\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"webhook\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nrules:\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  resourceNames:\n  - 'cert-manager-webhook-ca'\n  verbs: [\"get\", \"list\", \"watch\", \"update\"]\n# It's not possible to grant CREATE permission on a single resourceName.\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"create\"]\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/webhook-rbac.yaml"}, "region": {"startLine": 69, "endLine": 90, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: cert-manager-webhook:dynamic-serving\n  namespace: \"default\"\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"webhook\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: cert-manager-webhook:dynamic-serving\nsubjects:\n- apiGroup: \"\"\n  kind: ServiceAccount\n  name: cert-manager-webhook\n  namespace: default\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 16, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nautomountServiceAccountToken: true\nmetadata:\n  name: cert-manager\n  namespace: \"default\"\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"controller\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\n"}}}}]}, {"ruleId": "CKV_K8S_155", "ruleIndex": 25, "level": "error", "attachments": [], "message": {"text": "Minimize ClusterRoles that grant control over validating or mutating admission webhook configurations"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/cainjector-rbac.yaml"}, "region": {"startLine": 3, "endLine": 37, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: cert-manager-cainjector\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"cainjector\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\nrules:\n  - apiGroups: [\"cert-manager.io\"]\n    resources: [\"certificates\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"\"]\n    resources: [\"secrets\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"\"]\n    resources: [\"events\"]\n    verbs: [\"get\", \"create\", \"update\", \"patch\"]\n  - apiGroups: [\"admissionregistration.k8s.io\"]\n    resources: [\"validatingwebhookconfigurations\", \"mutatingwebhookconfigurations\"]\n    verbs: [\"get\", \"list\", \"watch\", \"update\"]\n  - apiGroups: [\"apiregistration.k8s.io\"]\n    resources: [\"apiservices\"]\n    verbs: [\"get\", \"list\", \"watch\", \"update\"]\n  - apiGroups: [\"apiextensions.k8s.io\"]\n    resources: [\"customresourcedefinitions\"]\n    verbs: [\"get\", \"list\", \"watch\", \"update\"]\n  - apiGroups: [\"auditregistration.k8s.io\"]\n    resources: [\"auditsinks\"]\n    verbs: [\"get\", \"list\", \"watch\", \"update\"]\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/startupapicheck-rbac.yaml"}, "region": {"startLine": 4, "endLine": 25, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: cert-manager-startupapicheck:create-cert\n  namespace: \"default\"\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"startupapicheck\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: \"-5\"\nrules:\n  - apiGroups: [\"cert-manager.io\"]\n    resources: [\"certificates\"]\n    verbs: [\"create\"]\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/startupapicheck-rbac.yaml"}, "region": {"startLine": 27, "endLine": 51, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: cert-manager-startupapicheck:create-cert\n  namespace: \"default\"\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"startupapicheck\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: \"-5\"\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: cert-manager-startupapicheck:create-cert\nsubjects:\n  - kind: ServiceAccount\n    name: cert-manager-startupapicheck\n    namespace: default\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/cert-manager/templates/cainjector-serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 16, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nautomountServiceAccountToken: true\nmetadata:\n  name: cert-manager-cainjector\n  namespace: \"default\"\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: cert-manager\n    app.kubernetes.io/component: \"cainjector\"\n    app.kubernetes.io/version: \"v1.5.4\"\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.5.4\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 95, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: grafana\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: grafana\n      annotations:\n        checksum/config: 07d2d063c6ef7769902b41e3901f5bbac3918fa4f0bdcfe5dc478a9a9eb62387\n        checksum/dashboards-json-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/sc-dashboard-provider-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 14d9828537e1b7ba166c79e641073a798db9abaded3c024b524c1d53a9a8657d\n    spec:\n      \n      serviceAccountName: grafana\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsUser: 472\n      containers:\n        - name: grafana\n          image: \"grafana/grafana:7.5.5\"\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n            - name: config\n              mountPath: \"/etc/grafana/grafana.ini\"\n              subPath: grafana.ini\n            - name: storage\n              mountPath: \"/var/lib/grafana\"\n          ports:\n            - name: service\n              containerPort: 80\n              protocol: TCP\n            - name: grafana\n              containerPort: 3000\n              protocol: TCP\n          env:\n            - name: GF_SECURITY_ADMIN_USER\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-user\n            - name: GF_SECURITY_ADMIN_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-password\n            \n            - name: GF_PATHS_DATA\n              value: /var/lib/grafana/\n            - name: GF_PATHS_LOGS\n              value: /var/log/grafana\n            - name: GF_PATHS_PLUGINS\n              value: /var/lib/grafana/plugins\n            - name: GF_PATHS_PROVISIONING\n              value: /etc/grafana/provisioning\n          livenessProbe:\n            failureThreshold: 10\n            httpGet:\n              path: /api/health\n              port: 3000\n            initialDelaySeconds: 60\n            timeoutSeconds: 30\n          readinessProbe:\n            httpGet:\n              path: /api/health\n              port: 3000\n          resources:\n            {}\n      volumes:\n        - name: config\n          configMap:\n            name: grafana\n        - name: storage\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 95, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: grafana\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: grafana\n      annotations:\n        checksum/config: 07d2d063c6ef7769902b41e3901f5bbac3918fa4f0bdcfe5dc478a9a9eb62387\n        checksum/dashboards-json-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/sc-dashboard-provider-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 14d9828537e1b7ba166c79e641073a798db9abaded3c024b524c1d53a9a8657d\n    spec:\n      \n      serviceAccountName: grafana\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsUser: 472\n      containers:\n        - name: grafana\n          image: \"grafana/grafana:7.5.5\"\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n            - name: config\n              mountPath: \"/etc/grafana/grafana.ini\"\n              subPath: grafana.ini\n            - name: storage\n              mountPath: \"/var/lib/grafana\"\n          ports:\n            - name: service\n              containerPort: 80\n              protocol: TCP\n            - name: grafana\n              containerPort: 3000\n              protocol: TCP\n          env:\n            - name: GF_SECURITY_ADMIN_USER\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-user\n            - name: GF_SECURITY_ADMIN_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-password\n            \n            - name: GF_PATHS_DATA\n              value: /var/lib/grafana/\n            - name: GF_PATHS_LOGS\n              value: /var/log/grafana\n            - name: GF_PATHS_PLUGINS\n              value: /var/lib/grafana/plugins\n            - name: GF_PATHS_PROVISIONING\n              value: /etc/grafana/provisioning\n          livenessProbe:\n            failureThreshold: 10\n            httpGet:\n              path: /api/health\n              port: 3000\n            initialDelaySeconds: 60\n            timeoutSeconds: 30\n          readinessProbe:\n            httpGet:\n              path: /api/health\n              port: 3000\n          resources:\n            {}\n      volumes:\n        - name: config\n          configMap:\n            name: grafana\n        - name: storage\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 95, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: grafana\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: grafana\n      annotations:\n        checksum/config: 07d2d063c6ef7769902b41e3901f5bbac3918fa4f0bdcfe5dc478a9a9eb62387\n        checksum/dashboards-json-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/sc-dashboard-provider-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 14d9828537e1b7ba166c79e641073a798db9abaded3c024b524c1d53a9a8657d\n    spec:\n      \n      serviceAccountName: grafana\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsUser: 472\n      containers:\n        - name: grafana\n          image: \"grafana/grafana:7.5.5\"\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n            - name: config\n              mountPath: \"/etc/grafana/grafana.ini\"\n              subPath: grafana.ini\n            - name: storage\n              mountPath: \"/var/lib/grafana\"\n          ports:\n            - name: service\n              containerPort: 80\n              protocol: TCP\n            - name: grafana\n              containerPort: 3000\n              protocol: TCP\n          env:\n            - name: GF_SECURITY_ADMIN_USER\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-user\n            - name: GF_SECURITY_ADMIN_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-password\n            \n            - name: GF_PATHS_DATA\n              value: /var/lib/grafana/\n            - name: GF_PATHS_LOGS\n              value: /var/log/grafana\n            - name: GF_PATHS_PLUGINS\n              value: /var/lib/grafana/plugins\n            - name: GF_PATHS_PROVISIONING\n              value: /etc/grafana/provisioning\n          livenessProbe:\n            failureThreshold: 10\n            httpGet:\n              path: /api/health\n              port: 3000\n            initialDelaySeconds: 60\n            timeoutSeconds: 30\n          readinessProbe:\n            httpGet:\n              path: /api/health\n              port: 3000\n          resources:\n            {}\n      volumes:\n        - name: config\n          configMap:\n            name: grafana\n        - name: storage\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 95, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: grafana\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: grafana\n      annotations:\n        checksum/config: 07d2d063c6ef7769902b41e3901f5bbac3918fa4f0bdcfe5dc478a9a9eb62387\n        checksum/dashboards-json-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/sc-dashboard-provider-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 14d9828537e1b7ba166c79e641073a798db9abaded3c024b524c1d53a9a8657d\n    spec:\n      \n      serviceAccountName: grafana\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsUser: 472\n      containers:\n        - name: grafana\n          image: \"grafana/grafana:7.5.5\"\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n            - name: config\n              mountPath: \"/etc/grafana/grafana.ini\"\n              subPath: grafana.ini\n            - name: storage\n              mountPath: \"/var/lib/grafana\"\n          ports:\n            - name: service\n              containerPort: 80\n              protocol: TCP\n            - name: grafana\n              containerPort: 3000\n              protocol: TCP\n          env:\n            - name: GF_SECURITY_ADMIN_USER\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-user\n            - name: GF_SECURITY_ADMIN_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-password\n            \n            - name: GF_PATHS_DATA\n              value: /var/lib/grafana/\n            - name: GF_PATHS_LOGS\n              value: /var/log/grafana\n            - name: GF_PATHS_PLUGINS\n              value: /var/lib/grafana/plugins\n            - name: GF_PATHS_PROVISIONING\n              value: /etc/grafana/provisioning\n          livenessProbe:\n            failureThreshold: 10\n            httpGet:\n              path: /api/health\n              port: 3000\n            initialDelaySeconds: 60\n            timeoutSeconds: 30\n          readinessProbe:\n            httpGet:\n              path: /api/health\n              port: 3000\n          resources:\n            {}\n      volumes:\n        - name: config\n          configMap:\n            name: grafana\n        - name: storage\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 95, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: grafana\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: grafana\n      annotations:\n        checksum/config: 07d2d063c6ef7769902b41e3901f5bbac3918fa4f0bdcfe5dc478a9a9eb62387\n        checksum/dashboards-json-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/sc-dashboard-provider-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 14d9828537e1b7ba166c79e641073a798db9abaded3c024b524c1d53a9a8657d\n    spec:\n      \n      serviceAccountName: grafana\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsUser: 472\n      containers:\n        - name: grafana\n          image: \"grafana/grafana:7.5.5\"\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n            - name: config\n              mountPath: \"/etc/grafana/grafana.ini\"\n              subPath: grafana.ini\n            - name: storage\n              mountPath: \"/var/lib/grafana\"\n          ports:\n            - name: service\n              containerPort: 80\n              protocol: TCP\n            - name: grafana\n              containerPort: 3000\n              protocol: TCP\n          env:\n            - name: GF_SECURITY_ADMIN_USER\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-user\n            - name: GF_SECURITY_ADMIN_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-password\n            \n            - name: GF_PATHS_DATA\n              value: /var/lib/grafana/\n            - name: GF_PATHS_LOGS\n              value: /var/log/grafana\n            - name: GF_PATHS_PLUGINS\n              value: /var/lib/grafana/plugins\n            - name: GF_PATHS_PROVISIONING\n              value: /etc/grafana/provisioning\n          livenessProbe:\n            failureThreshold: 10\n            httpGet:\n              path: /api/health\n              port: 3000\n            initialDelaySeconds: 60\n            timeoutSeconds: 30\n          readinessProbe:\n            httpGet:\n              path: /api/health\n              port: 3000\n          resources:\n            {}\n      volumes:\n        - name: config\n          configMap:\n            name: grafana\n        - name: storage\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 95, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: grafana\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: grafana\n      annotations:\n        checksum/config: 07d2d063c6ef7769902b41e3901f5bbac3918fa4f0bdcfe5dc478a9a9eb62387\n        checksum/dashboards-json-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/sc-dashboard-provider-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 14d9828537e1b7ba166c79e641073a798db9abaded3c024b524c1d53a9a8657d\n    spec:\n      \n      serviceAccountName: grafana\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsUser: 472\n      containers:\n        - name: grafana\n          image: \"grafana/grafana:7.5.5\"\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n            - name: config\n              mountPath: \"/etc/grafana/grafana.ini\"\n              subPath: grafana.ini\n            - name: storage\n              mountPath: \"/var/lib/grafana\"\n          ports:\n            - name: service\n              containerPort: 80\n              protocol: TCP\n            - name: grafana\n              containerPort: 3000\n              protocol: TCP\n          env:\n            - name: GF_SECURITY_ADMIN_USER\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-user\n            - name: GF_SECURITY_ADMIN_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-password\n            \n            - name: GF_PATHS_DATA\n              value: /var/lib/grafana/\n            - name: GF_PATHS_LOGS\n              value: /var/log/grafana\n            - name: GF_PATHS_PLUGINS\n              value: /var/lib/grafana/plugins\n            - name: GF_PATHS_PROVISIONING\n              value: /etc/grafana/provisioning\n          livenessProbe:\n            failureThreshold: 10\n            httpGet:\n              path: /api/health\n              port: 3000\n            initialDelaySeconds: 60\n            timeoutSeconds: 30\n          readinessProbe:\n            httpGet:\n              path: /api/health\n              port: 3000\n          resources:\n            {}\n      volumes:\n        - name: config\n          configMap:\n            name: grafana\n        - name: storage\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 95, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: grafana\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: grafana\n      annotations:\n        checksum/config: 07d2d063c6ef7769902b41e3901f5bbac3918fa4f0bdcfe5dc478a9a9eb62387\n        checksum/dashboards-json-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/sc-dashboard-provider-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 14d9828537e1b7ba166c79e641073a798db9abaded3c024b524c1d53a9a8657d\n    spec:\n      \n      serviceAccountName: grafana\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsUser: 472\n      containers:\n        - name: grafana\n          image: \"grafana/grafana:7.5.5\"\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n            - name: config\n              mountPath: \"/etc/grafana/grafana.ini\"\n              subPath: grafana.ini\n            - name: storage\n              mountPath: \"/var/lib/grafana\"\n          ports:\n            - name: service\n              containerPort: 80\n              protocol: TCP\n            - name: grafana\n              containerPort: 3000\n              protocol: TCP\n          env:\n            - name: GF_SECURITY_ADMIN_USER\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-user\n            - name: GF_SECURITY_ADMIN_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-password\n            \n            - name: GF_PATHS_DATA\n              value: /var/lib/grafana/\n            - name: GF_PATHS_LOGS\n              value: /var/log/grafana\n            - name: GF_PATHS_PLUGINS\n              value: /var/lib/grafana/plugins\n            - name: GF_PATHS_PROVISIONING\n              value: /etc/grafana/provisioning\n          livenessProbe:\n            failureThreshold: 10\n            httpGet:\n              path: /api/health\n              port: 3000\n            initialDelaySeconds: 60\n            timeoutSeconds: 30\n          readinessProbe:\n            httpGet:\n              path: /api/health\n              port: 3000\n          resources:\n            {}\n      volumes:\n        - name: config\n          configMap:\n            name: grafana\n        - name: storage\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_35", "ruleIndex": 6, "level": "error", "attachments": [], "message": {"text": "Prefer using secrets as files over secrets as environment variables"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 95, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: grafana\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: grafana\n      annotations:\n        checksum/config: 07d2d063c6ef7769902b41e3901f5bbac3918fa4f0bdcfe5dc478a9a9eb62387\n        checksum/dashboards-json-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/sc-dashboard-provider-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 14d9828537e1b7ba166c79e641073a798db9abaded3c024b524c1d53a9a8657d\n    spec:\n      \n      serviceAccountName: grafana\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsUser: 472\n      containers:\n        - name: grafana\n          image: \"grafana/grafana:7.5.5\"\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n            - name: config\n              mountPath: \"/etc/grafana/grafana.ini\"\n              subPath: grafana.ini\n            - name: storage\n              mountPath: \"/var/lib/grafana\"\n          ports:\n            - name: service\n              containerPort: 80\n              protocol: TCP\n            - name: grafana\n              containerPort: 3000\n              protocol: TCP\n          env:\n            - name: GF_SECURITY_ADMIN_USER\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-user\n            - name: GF_SECURITY_ADMIN_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-password\n            \n            - name: GF_PATHS_DATA\n              value: /var/lib/grafana/\n            - name: GF_PATHS_LOGS\n              value: /var/log/grafana\n            - name: GF_PATHS_PLUGINS\n              value: /var/lib/grafana/plugins\n            - name: GF_PATHS_PROVISIONING\n              value: /etc/grafana/provisioning\n          livenessProbe:\n            failureThreshold: 10\n            httpGet:\n              path: /api/health\n              port: 3000\n            initialDelaySeconds: 60\n            timeoutSeconds: 30\n          readinessProbe:\n            httpGet:\n              path: /api/health\n              port: 3000\n          resources:\n            {}\n      volumes:\n        - name: config\n          configMap:\n            name: grafana\n        - name: storage\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 95, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: grafana\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: grafana\n      annotations:\n        checksum/config: 07d2d063c6ef7769902b41e3901f5bbac3918fa4f0bdcfe5dc478a9a9eb62387\n        checksum/dashboards-json-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/sc-dashboard-provider-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 14d9828537e1b7ba166c79e641073a798db9abaded3c024b524c1d53a9a8657d\n    spec:\n      \n      serviceAccountName: grafana\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsUser: 472\n      containers:\n        - name: grafana\n          image: \"grafana/grafana:7.5.5\"\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n            - name: config\n              mountPath: \"/etc/grafana/grafana.ini\"\n              subPath: grafana.ini\n            - name: storage\n              mountPath: \"/var/lib/grafana\"\n          ports:\n            - name: service\n              containerPort: 80\n              protocol: TCP\n            - name: grafana\n              containerPort: 3000\n              protocol: TCP\n          env:\n            - name: GF_SECURITY_ADMIN_USER\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-user\n            - name: GF_SECURITY_ADMIN_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-password\n            \n            - name: GF_PATHS_DATA\n              value: /var/lib/grafana/\n            - name: GF_PATHS_LOGS\n              value: /var/log/grafana\n            - name: GF_PATHS_PLUGINS\n              value: /var/lib/grafana/plugins\n            - name: GF_PATHS_PROVISIONING\n              value: /etc/grafana/provisioning\n          livenessProbe:\n            failureThreshold: 10\n            httpGet:\n              path: /api/health\n              port: 3000\n            initialDelaySeconds: 60\n            timeoutSeconds: 30\n          readinessProbe:\n            httpGet:\n              path: /api/health\n              port: 3000\n          resources:\n            {}\n      volumes:\n        - name: config\n          configMap:\n            name: grafana\n        - name: storage\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 95, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: grafana\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: grafana\n      annotations:\n        checksum/config: 07d2d063c6ef7769902b41e3901f5bbac3918fa4f0bdcfe5dc478a9a9eb62387\n        checksum/dashboards-json-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/sc-dashboard-provider-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 14d9828537e1b7ba166c79e641073a798db9abaded3c024b524c1d53a9a8657d\n    spec:\n      \n      serviceAccountName: grafana\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsUser: 472\n      containers:\n        - name: grafana\n          image: \"grafana/grafana:7.5.5\"\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n            - name: config\n              mountPath: \"/etc/grafana/grafana.ini\"\n              subPath: grafana.ini\n            - name: storage\n              mountPath: \"/var/lib/grafana\"\n          ports:\n            - name: service\n              containerPort: 80\n              protocol: TCP\n            - name: grafana\n              containerPort: 3000\n              protocol: TCP\n          env:\n            - name: GF_SECURITY_ADMIN_USER\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-user\n            - name: GF_SECURITY_ADMIN_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-password\n            \n            - name: GF_PATHS_DATA\n              value: /var/lib/grafana/\n            - name: GF_PATHS_LOGS\n              value: /var/log/grafana\n            - name: GF_PATHS_PLUGINS\n              value: /var/lib/grafana/plugins\n            - name: GF_PATHS_PROVISIONING\n              value: /etc/grafana/provisioning\n          livenessProbe:\n            failureThreshold: 10\n            httpGet:\n              path: /api/health\n              port: 3000\n            initialDelaySeconds: 60\n            timeoutSeconds: 30\n          readinessProbe:\n            httpGet:\n              path: /api/health\n              port: 3000\n          resources:\n            {}\n      volumes:\n        - name: config\n          configMap:\n            name: grafana\n        - name: storage\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 95, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: grafana\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: grafana\n      annotations:\n        checksum/config: 07d2d063c6ef7769902b41e3901f5bbac3918fa4f0bdcfe5dc478a9a9eb62387\n        checksum/dashboards-json-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/sc-dashboard-provider-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 14d9828537e1b7ba166c79e641073a798db9abaded3c024b524c1d53a9a8657d\n    spec:\n      \n      serviceAccountName: grafana\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsUser: 472\n      containers:\n        - name: grafana\n          image: \"grafana/grafana:7.5.5\"\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n            - name: config\n              mountPath: \"/etc/grafana/grafana.ini\"\n              subPath: grafana.ini\n            - name: storage\n              mountPath: \"/var/lib/grafana\"\n          ports:\n            - name: service\n              containerPort: 80\n              protocol: TCP\n            - name: grafana\n              containerPort: 3000\n              protocol: TCP\n          env:\n            - name: GF_SECURITY_ADMIN_USER\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-user\n            - name: GF_SECURITY_ADMIN_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-password\n            \n            - name: GF_PATHS_DATA\n              value: /var/lib/grafana/\n            - name: GF_PATHS_LOGS\n              value: /var/log/grafana\n            - name: GF_PATHS_PLUGINS\n              value: /var/lib/grafana/plugins\n            - name: GF_PATHS_PROVISIONING\n              value: /etc/grafana/provisioning\n          livenessProbe:\n            failureThreshold: 10\n            httpGet:\n              path: /api/health\n              port: 3000\n            initialDelaySeconds: 60\n            timeoutSeconds: 30\n          readinessProbe:\n            httpGet:\n              path: /api/health\n              port: 3000\n          resources:\n            {}\n      volumes:\n        - name: config\n          configMap:\n            name: grafana\n        - name: storage\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 95, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: grafana\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: grafana\n      annotations:\n        checksum/config: 07d2d063c6ef7769902b41e3901f5bbac3918fa4f0bdcfe5dc478a9a9eb62387\n        checksum/dashboards-json-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/sc-dashboard-provider-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 14d9828537e1b7ba166c79e641073a798db9abaded3c024b524c1d53a9a8657d\n    spec:\n      \n      serviceAccountName: grafana\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsUser: 472\n      containers:\n        - name: grafana\n          image: \"grafana/grafana:7.5.5\"\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n            - name: config\n              mountPath: \"/etc/grafana/grafana.ini\"\n              subPath: grafana.ini\n            - name: storage\n              mountPath: \"/var/lib/grafana\"\n          ports:\n            - name: service\n              containerPort: 80\n              protocol: TCP\n            - name: grafana\n              containerPort: 3000\n              protocol: TCP\n          env:\n            - name: GF_SECURITY_ADMIN_USER\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-user\n            - name: GF_SECURITY_ADMIN_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-password\n            \n            - name: GF_PATHS_DATA\n              value: /var/lib/grafana/\n            - name: GF_PATHS_LOGS\n              value: /var/log/grafana\n            - name: GF_PATHS_PLUGINS\n              value: /var/lib/grafana/plugins\n            - name: GF_PATHS_PROVISIONING\n              value: /etc/grafana/provisioning\n          livenessProbe:\n            failureThreshold: 10\n            httpGet:\n              path: /api/health\n              port: 3000\n            initialDelaySeconds: 60\n            timeoutSeconds: 30\n          readinessProbe:\n            httpGet:\n              path: /api/health\n              port: 3000\n          resources:\n            {}\n      volumes:\n        - name: config\n          configMap:\n            name: grafana\n        - name: storage\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 95, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: grafana\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: grafana\n      annotations:\n        checksum/config: 07d2d063c6ef7769902b41e3901f5bbac3918fa4f0bdcfe5dc478a9a9eb62387\n        checksum/dashboards-json-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/sc-dashboard-provider-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 14d9828537e1b7ba166c79e641073a798db9abaded3c024b524c1d53a9a8657d\n    spec:\n      \n      serviceAccountName: grafana\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsUser: 472\n      containers:\n        - name: grafana\n          image: \"grafana/grafana:7.5.5\"\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n            - name: config\n              mountPath: \"/etc/grafana/grafana.ini\"\n              subPath: grafana.ini\n            - name: storage\n              mountPath: \"/var/lib/grafana\"\n          ports:\n            - name: service\n              containerPort: 80\n              protocol: TCP\n            - name: grafana\n              containerPort: 3000\n              protocol: TCP\n          env:\n            - name: GF_SECURITY_ADMIN_USER\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-user\n            - name: GF_SECURITY_ADMIN_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-password\n            \n            - name: GF_PATHS_DATA\n              value: /var/lib/grafana/\n            - name: GF_PATHS_LOGS\n              value: /var/log/grafana\n            - name: GF_PATHS_PLUGINS\n              value: /var/lib/grafana/plugins\n            - name: GF_PATHS_PROVISIONING\n              value: /etc/grafana/provisioning\n          livenessProbe:\n            failureThreshold: 10\n            httpGet:\n              path: /api/health\n              port: 3000\n            initialDelaySeconds: 60\n            timeoutSeconds: 30\n          readinessProbe:\n            httpGet:\n              path: /api/health\n              port: 3000\n          resources:\n            {}\n      volumes:\n        - name: config\n          configMap:\n            name: grafana\n        - name: storage\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 95, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: grafana\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: grafana\n      annotations:\n        checksum/config: 07d2d063c6ef7769902b41e3901f5bbac3918fa4f0bdcfe5dc478a9a9eb62387\n        checksum/dashboards-json-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/sc-dashboard-provider-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6\n        checksum/secret: 14d9828537e1b7ba166c79e641073a798db9abaded3c024b524c1d53a9a8657d\n    spec:\n      \n      serviceAccountName: grafana\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsUser: 472\n      containers:\n        - name: grafana\n          image: \"grafana/grafana:7.5.5\"\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n            - name: config\n              mountPath: \"/etc/grafana/grafana.ini\"\n              subPath: grafana.ini\n            - name: storage\n              mountPath: \"/var/lib/grafana\"\n          ports:\n            - name: service\n              containerPort: 80\n              protocol: TCP\n            - name: grafana\n              containerPort: 3000\n              protocol: TCP\n          env:\n            - name: GF_SECURITY_ADMIN_USER\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-user\n            - name: GF_SECURITY_ADMIN_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: grafana\n                  key: admin-password\n            \n            - name: GF_PATHS_DATA\n              value: /var/lib/grafana/\n            - name: GF_PATHS_LOGS\n              value: /var/log/grafana\n            - name: GF_PATHS_PLUGINS\n              value: /var/lib/grafana/plugins\n            - name: GF_PATHS_PROVISIONING\n              value: /etc/grafana/provisioning\n          livenessProbe:\n            failureThreshold: 10\n            httpGet:\n              path: /api/health\n              port: 3000\n            initialDelaySeconds: 60\n            timeoutSeconds: 30\n          readinessProbe:\n            httpGet:\n              path: /api/health\n              port: 3000\n          resources:\n            {}\n      volumes:\n        - name: config\n          configMap:\n            name: grafana\n        - name: storage\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/role.yaml"}, "region": {"startLine": 3, "endLine": 18, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nrules:\n- apiGroups:      ['extensions']\n  resources:      ['podsecuritypolicies']\n  verbs:          ['use']\n  resourceNames:  [grafana]\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/service.yaml"}, "region": {"startLine": 3, "endLine": 24, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ClusterIP\n  ports:\n    - name: service\n      port: 80\n      protocol: TCP\n      targetPort: 3000\n\n  selector:\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 13, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\n  name: grafana\n  namespace: default\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/configmap.yaml"}, "region": {"startLine": 3, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\ndata:\n  grafana.ini: |\n    [analytics]\n    check_for_updates = true\n    [grafana_net]\n    url = https://grafana.net\n    [log]\n    mode = console\n    [paths]\n    data = /var/lib/grafana/\n    logs = /var/log/grafana\n    plugins = /var/lib/grafana/plugins\n    provisioning = /etc/grafana/provisioning\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/rolebinding.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: grafana\nsubjects:\n- kind: ServiceAccount\n  name: grafana\n  namespace: default\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/secret.yaml"}, "region": {"startLine": 3, "endLine": 18, "snippet": {"text": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\ntype: Opaque\ndata:\n  admin-user: \"YWRtaW4=\"\n  admin-password: \"SzVhTWtuT0JXVkZzRURMMkMweU1oOHNuM0Fkd3RWZ3dvQkI5WWphSg==\"\n  ldap-toml: \"\"\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test.yaml"}, "region": {"startLine": 3, "endLine": 31, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: grafana-test\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\n  namespace: default\nspec:\n  serviceAccountName: grafana-test\n  containers:\n    - name: grafana-test\n      image: \"bats/bats:v1.1.0\"\n      imagePullPolicy: \"IfNotPresent\"\n      command: [\"/opt/bats/bin/bats\", \"-t\", \"/tests/run.sh\"]\n      volumeMounts:\n        - mountPath: /tests\n          name: tests\n          readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: grafana-test\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test.yaml"}, "region": {"startLine": 3, "endLine": 31, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: grafana-test\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\n  namespace: default\nspec:\n  serviceAccountName: grafana-test\n  containers:\n    - name: grafana-test\n      image: \"bats/bats:v1.1.0\"\n      imagePullPolicy: \"IfNotPresent\"\n      command: [\"/opt/bats/bin/bats\", \"-t\", \"/tests/run.sh\"]\n      volumeMounts:\n        - mountPath: /tests\n          name: tests\n          readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: grafana-test\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test.yaml"}, "region": {"startLine": 3, "endLine": 31, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: grafana-test\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\n  namespace: default\nspec:\n  serviceAccountName: grafana-test\n  containers:\n    - name: grafana-test\n      image: \"bats/bats:v1.1.0\"\n      imagePullPolicy: \"IfNotPresent\"\n      command: [\"/opt/bats/bin/bats\", \"-t\", \"/tests/run.sh\"]\n      volumeMounts:\n        - mountPath: /tests\n          name: tests\n          readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: grafana-test\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test.yaml"}, "region": {"startLine": 3, "endLine": 31, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: grafana-test\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\n  namespace: default\nspec:\n  serviceAccountName: grafana-test\n  containers:\n    - name: grafana-test\n      image: \"bats/bats:v1.1.0\"\n      imagePullPolicy: \"IfNotPresent\"\n      command: [\"/opt/bats/bin/bats\", \"-t\", \"/tests/run.sh\"]\n      volumeMounts:\n        - mountPath: /tests\n          name: tests\n          readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: grafana-test\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test.yaml"}, "region": {"startLine": 3, "endLine": 31, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: grafana-test\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\n  namespace: default\nspec:\n  serviceAccountName: grafana-test\n  containers:\n    - name: grafana-test\n      image: \"bats/bats:v1.1.0\"\n      imagePullPolicy: \"IfNotPresent\"\n      command: [\"/opt/bats/bin/bats\", \"-t\", \"/tests/run.sh\"]\n      volumeMounts:\n        - mountPath: /tests\n          name: tests\n          readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: grafana-test\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test.yaml"}, "region": {"startLine": 3, "endLine": 31, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: grafana-test\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\n  namespace: default\nspec:\n  serviceAccountName: grafana-test\n  containers:\n    - name: grafana-test\n      image: \"bats/bats:v1.1.0\"\n      imagePullPolicy: \"IfNotPresent\"\n      command: [\"/opt/bats/bin/bats\", \"-t\", \"/tests/run.sh\"]\n      volumeMounts:\n        - mountPath: /tests\n          name: tests\n          readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: grafana-test\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test.yaml"}, "region": {"startLine": 3, "endLine": 31, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: grafana-test\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\n  namespace: default\nspec:\n  serviceAccountName: grafana-test\n  containers:\n    - name: grafana-test\n      image: \"bats/bats:v1.1.0\"\n      imagePullPolicy: \"IfNotPresent\"\n      command: [\"/opt/bats/bin/bats\", \"-t\", \"/tests/run.sh\"]\n      volumeMounts:\n        - mountPath: /tests\n          name: tests\n          readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: grafana-test\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test.yaml"}, "region": {"startLine": 3, "endLine": 31, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: grafana-test\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\n  namespace: default\nspec:\n  serviceAccountName: grafana-test\n  containers:\n    - name: grafana-test\n      image: \"bats/bats:v1.1.0\"\n      imagePullPolicy: \"IfNotPresent\"\n      command: [\"/opt/bats/bin/bats\", \"-t\", \"/tests/run.sh\"]\n      volumeMounts:\n        - mountPath: /tests\n          name: tests\n          readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: grafana-test\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test.yaml"}, "region": {"startLine": 3, "endLine": 31, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: grafana-test\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\n  namespace: default\nspec:\n  serviceAccountName: grafana-test\n  containers:\n    - name: grafana-test\n      image: \"bats/bats:v1.1.0\"\n      imagePullPolicy: \"IfNotPresent\"\n      command: [\"/opt/bats/bin/bats\", \"-t\", \"/tests/run.sh\"]\n      volumeMounts:\n        - mountPath: /tests\n          name: tests\n          readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: grafana-test\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test.yaml"}, "region": {"startLine": 3, "endLine": 31, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: grafana-test\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\n  namespace: default\nspec:\n  serviceAccountName: grafana-test\n  containers:\n    - name: grafana-test\n      image: \"bats/bats:v1.1.0\"\n      imagePullPolicy: \"IfNotPresent\"\n      command: [\"/opt/bats/bin/bats\", \"-t\", \"/tests/run.sh\"]\n      volumeMounts:\n        - mountPath: /tests\n          name: tests\n          readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: grafana-test\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test.yaml"}, "region": {"startLine": 3, "endLine": 31, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: grafana-test\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\n  namespace: default\nspec:\n  serviceAccountName: grafana-test\n  containers:\n    - name: grafana-test\n      image: \"bats/bats:v1.1.0\"\n      imagePullPolicy: \"IfNotPresent\"\n      command: [\"/opt/bats/bin/bats\", \"-t\", \"/tests/run.sh\"]\n      volumeMounts:\n        - mountPath: /tests\n          name: tests\n          readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: grafana-test\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test.yaml"}, "region": {"startLine": 3, "endLine": 31, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: grafana-test\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\n  namespace: default\nspec:\n  serviceAccountName: grafana-test\n  containers:\n    - name: grafana-test\n      image: \"bats/bats:v1.1.0\"\n      imagePullPolicy: \"IfNotPresent\"\n      command: [\"/opt/bats/bin/bats\", \"-t\", \"/tests/run.sh\"]\n      volumeMounts:\n        - mountPath: /tests\n          name: tests\n          readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: grafana-test\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test.yaml"}, "region": {"startLine": 3, "endLine": 31, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: grafana-test\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\n  namespace: default\nspec:\n  serviceAccountName: grafana-test\n  containers:\n    - name: grafana-test\n      image: \"bats/bats:v1.1.0\"\n      imagePullPolicy: \"IfNotPresent\"\n      command: [\"/opt/bats/bin/bats\", \"-t\", \"/tests/run.sh\"]\n      volumeMounts:\n        - mountPath: /tests\n          name: tests\n          readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: grafana-test\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test.yaml"}, "region": {"startLine": 3, "endLine": 31, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: grafana-test\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\n  namespace: default\nspec:\n  serviceAccountName: grafana-test\n  containers:\n    - name: grafana-test\n      image: \"bats/bats:v1.1.0\"\n      imagePullPolicy: \"IfNotPresent\"\n      command: [\"/opt/bats/bin/bats\", \"-t\", \"/tests/run.sh\"]\n      volumeMounts:\n        - mountPath: /tests\n          name: tests\n          readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: grafana-test\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test.yaml"}, "region": {"startLine": 3, "endLine": 31, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: grafana-test\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\n  namespace: default\nspec:\n  serviceAccountName: grafana-test\n  containers:\n    - name: grafana-test\n      image: \"bats/bats:v1.1.0\"\n      imagePullPolicy: \"IfNotPresent\"\n      command: [\"/opt/bats/bin/bats\", \"-t\", \"/tests/run.sh\"]\n      volumeMounts:\n        - mountPath: /tests\n          name: tests\n          readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: grafana-test\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test.yaml"}, "region": {"startLine": 3, "endLine": 31, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: grafana-test\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\n  namespace: default\nspec:\n  serviceAccountName: grafana-test\n  containers:\n    - name: grafana-test\n      image: \"bats/bats:v1.1.0\"\n      imagePullPolicy: \"IfNotPresent\"\n      command: [\"/opt/bats/bin/bats\", \"-t\", \"/tests/run.sh\"]\n      volumeMounts:\n        - mountPath: /tests\n          name: tests\n          readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: grafana-test\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test.yaml"}, "region": {"startLine": 3, "endLine": 31, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: grafana-test\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\n  namespace: default\nspec:\n  serviceAccountName: grafana-test\n  containers:\n    - name: grafana-test\n      image: \"bats/bats:v1.1.0\"\n      imagePullPolicy: \"IfNotPresent\"\n      command: [\"/opt/bats/bin/bats\", \"-t\", \"/tests/run.sh\"]\n      volumeMounts:\n        - mountPath: /tests\n          name: tests\n          readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: grafana-test\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test.yaml"}, "region": {"startLine": 3, "endLine": 31, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: grafana-test\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\n  namespace: default\nspec:\n  serviceAccountName: grafana-test\n  containers:\n    - name: grafana-test\n      image: \"bats/bats:v1.1.0\"\n      imagePullPolicy: \"IfNotPresent\"\n      command: [\"/opt/bats/bin/bats\", \"-t\", \"/tests/run.sh\"]\n      volumeMounts:\n        - mountPath: /tests\n          name: tests\n          readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: grafana-test\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test.yaml"}, "region": {"startLine": 3, "endLine": 31, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: grafana-test\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\n  namespace: default\nspec:\n  serviceAccountName: grafana-test\n  containers:\n    - name: grafana-test\n      image: \"bats/bats:v1.1.0\"\n      imagePullPolicy: \"IfNotPresent\"\n      command: [\"/opt/bats/bin/bats\", \"-t\", \"/tests/run.sh\"]\n      volumeMounts:\n        - mountPath: /tests\n          name: tests\n          readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: grafana-test\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test-configmap.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: grafana-test\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\ndata:\n  run.sh: |-\n    @test \"Test Health\" {\n      url=\"http://grafana/api/health\"\n\n      code=$(wget --server-response --spider --timeout 10 --tries 1 ${url} 2>&1 | awk '/^  HTTP/{print $2}')\n      [ \"$code\" == \"200\" ]\n    }\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test-serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 13, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\n  name: grafana-test\n  namespace: default\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test-rolebinding.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: grafana-test\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: grafana-test\nsubjects:\n- kind: ServiceAccount\n  name: grafana-test\n  namespace: default\n"}}}}]}, {"ruleId": "CKV_K8S_6", "ruleIndex": 26, "level": "error", "attachments": [], "message": {"text": "Do not admit root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test-podsecuritypolicy.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "apiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: grafana-test\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  allowPrivilegeEscalation: true\n  privileged: false\n  hostNetwork: false\n  hostIPC: false\n  hostPID: false\n  fsGroup:\n    rule: RunAsAny\n  seLinux:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  runAsUser:\n    rule: RunAsAny\n  volumes:\n  - configMap\n  - downwardAPI\n  - emptyDir\n  - projected\n  - csi\n  - secret\n"}}}}]}, {"ruleId": "CKV_K8S_36", "ruleIndex": 27, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test-podsecuritypolicy.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "apiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: grafana-test\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  allowPrivilegeEscalation: true\n  privileged: false\n  hostNetwork: false\n  hostIPC: false\n  hostPID: false\n  fsGroup:\n    rule: RunAsAny\n  seLinux:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  runAsUser:\n    rule: RunAsAny\n  volumes:\n  - configMap\n  - downwardAPI\n  - emptyDir\n  - projected\n  - csi\n  - secret\n"}}}}]}, {"ruleId": "CKV_K8S_7", "ruleIndex": 28, "level": "error", "attachments": [], "message": {"text": "Do not admit containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test-podsecuritypolicy.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "apiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: grafana-test\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  allowPrivilegeEscalation: true\n  privileged: false\n  hostNetwork: false\n  hostIPC: false\n  hostPID: false\n  fsGroup:\n    rule: RunAsAny\n  seLinux:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  runAsUser:\n    rule: RunAsAny\n  volumes:\n  - configMap\n  - downwardAPI\n  - emptyDir\n  - projected\n  - csi\n  - secret\n"}}}}]}, {"ruleId": "CKV_K8S_32", "ruleIndex": 29, "level": "error", "attachments": [], "message": {"text": "Ensure default seccomp profile set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test-podsecuritypolicy.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "apiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: grafana-test\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  allowPrivilegeEscalation: true\n  privileged: false\n  hostNetwork: false\n  hostIPC: false\n  hostPID: false\n  fsGroup:\n    rule: RunAsAny\n  seLinux:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  runAsUser:\n    rule: RunAsAny\n  volumes:\n  - configMap\n  - downwardAPI\n  - emptyDir\n  - projected\n  - csi\n  - secret\n"}}}}]}, {"ruleId": "CKV_K8S_5", "ruleIndex": 30, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test-podsecuritypolicy.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "apiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: grafana-test\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  allowPrivilegeEscalation: true\n  privileged: false\n  hostNetwork: false\n  hostIPC: false\n  hostPID: false\n  fsGroup:\n    rule: RunAsAny\n  seLinux:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  runAsUser:\n    rule: RunAsAny\n  volumes:\n  - configMap\n  - downwardAPI\n  - emptyDir\n  - projected\n  - csi\n  - secret\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/grafana/templates/tests/test-role.yaml"}, "region": {"startLine": 3, "endLine": 18, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: grafana-test\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-6.11.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: grafana\n    app.kubernetes.io/version: \"7.5.5\"\n    app.kubernetes.io/managed-by: Helm\nrules:\n- apiGroups:      ['policy']\n  resources:      ['podsecuritypolicies']\n  verbs:          ['use']\n  resourceNames:  [grafana-test]\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/autoscaling.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-autoscaling\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-autoscaling\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-autoscaling\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-autoscaling\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-autoscaling\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-autoscaling\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.coreService\"\n            - name: ACTION\n              value: \"autoScaling\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-autoscaling\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/autoscaling.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-autoscaling\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-autoscaling\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-autoscaling\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-autoscaling\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-autoscaling\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-autoscaling\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.coreService\"\n            - name: ACTION\n              value: \"autoScaling\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-autoscaling\"\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/autoscaling.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-autoscaling\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-autoscaling\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-autoscaling\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-autoscaling\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-autoscaling\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-autoscaling\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.coreService\"\n            - name: ACTION\n              value: \"autoScaling\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-autoscaling\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/autoscaling.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-autoscaling\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-autoscaling\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-autoscaling\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-autoscaling\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-autoscaling\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-autoscaling\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.coreService\"\n            - name: ACTION\n              value: \"autoScaling\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-autoscaling\"\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/autoscaling.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-autoscaling\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-autoscaling\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-autoscaling\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-autoscaling\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-autoscaling\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-autoscaling\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.coreService\"\n            - name: ACTION\n              value: \"autoScaling\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-autoscaling\"\n"}}}}]}, {"ruleId": "CKV_K8S_35", "ruleIndex": 6, "level": "error", "attachments": [], "message": {"text": "Prefer using secrets as files over secrets as environment variables"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/autoscaling.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-autoscaling\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-autoscaling\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-autoscaling\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-autoscaling\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-autoscaling\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-autoscaling\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.coreService\"\n            - name: ACTION\n              value: \"autoScaling\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-autoscaling\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/autoscaling.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-autoscaling\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-autoscaling\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-autoscaling\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-autoscaling\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-autoscaling\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-autoscaling\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.coreService\"\n            - name: ACTION\n              value: \"autoScaling\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-autoscaling\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/autoscaling.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-autoscaling\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-autoscaling\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-autoscaling\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-autoscaling\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-autoscaling\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-autoscaling\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.coreService\"\n            - name: ACTION\n              value: \"autoScaling\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-autoscaling\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/autoscaling.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-autoscaling\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-autoscaling\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-autoscaling\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-autoscaling\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-autoscaling\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-autoscaling\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.coreService\"\n            - name: ACTION\n              value: \"autoScaling\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-autoscaling\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/autoscaling.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-autoscaling\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-autoscaling\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-autoscaling\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-autoscaling\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-autoscaling\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-autoscaling\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.coreService\"\n            - name: ACTION\n              value: \"autoScaling\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-autoscaling\"\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/rate-card-updater.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-rate-card-updater\"\nspec:\n  schedule: \"6 1 1 * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-rate-card-updater\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-rate-card-updater\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-rate-card-updater\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-rate-card-updater\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-rate-card-updater\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateRateCard\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-rate-card-updater\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/rate-card-updater.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-rate-card-updater\"\nspec:\n  schedule: \"6 1 1 * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-rate-card-updater\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-rate-card-updater\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-rate-card-updater\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-rate-card-updater\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-rate-card-updater\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateRateCard\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-rate-card-updater\"\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/rate-card-updater.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-rate-card-updater\"\nspec:\n  schedule: \"6 1 1 * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-rate-card-updater\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-rate-card-updater\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-rate-card-updater\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-rate-card-updater\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-rate-card-updater\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateRateCard\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-rate-card-updater\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/rate-card-updater.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-rate-card-updater\"\nspec:\n  schedule: \"6 1 1 * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-rate-card-updater\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-rate-card-updater\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-rate-card-updater\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-rate-card-updater\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-rate-card-updater\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateRateCard\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-rate-card-updater\"\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/rate-card-updater.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-rate-card-updater\"\nspec:\n  schedule: \"6 1 1 * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-rate-card-updater\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-rate-card-updater\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-rate-card-updater\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-rate-card-updater\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-rate-card-updater\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateRateCard\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-rate-card-updater\"\n"}}}}]}, {"ruleId": "CKV_K8S_35", "ruleIndex": 6, "level": "error", "attachments": [], "message": {"text": "Prefer using secrets as files over secrets as environment variables"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/rate-card-updater.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-rate-card-updater\"\nspec:\n  schedule: \"6 1 1 * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-rate-card-updater\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-rate-card-updater\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-rate-card-updater\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-rate-card-updater\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-rate-card-updater\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateRateCard\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-rate-card-updater\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/rate-card-updater.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-rate-card-updater\"\nspec:\n  schedule: \"6 1 1 * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-rate-card-updater\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-rate-card-updater\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-rate-card-updater\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-rate-card-updater\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-rate-card-updater\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateRateCard\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-rate-card-updater\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/rate-card-updater.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-rate-card-updater\"\nspec:\n  schedule: \"6 1 1 * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-rate-card-updater\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-rate-card-updater\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-rate-card-updater\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-rate-card-updater\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-rate-card-updater\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateRateCard\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-rate-card-updater\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/rate-card-updater.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-rate-card-updater\"\nspec:\n  schedule: \"6 1 1 * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-rate-card-updater\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-rate-card-updater\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-rate-card-updater\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-rate-card-updater\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-rate-card-updater\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateRateCard\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-rate-card-updater\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/rate-card-updater.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-rate-card-updater\"\nspec:\n  schedule: \"6 1 1 * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-rate-card-updater\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-rate-card-updater\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-rate-card-updater\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-rate-card-updater\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-rate-card-updater\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateRateCard\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-rate-card-updater\"\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/cost-monitoring.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-cost-monitoring\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-cost-monitoring\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-cost-monitoring\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-cost-monitoring\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-cost-monitoring\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-cost-monitoring\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateCostMonitor\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-cost-monitoring\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/cost-monitoring.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-cost-monitoring\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-cost-monitoring\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-cost-monitoring\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-cost-monitoring\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-cost-monitoring\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-cost-monitoring\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateCostMonitor\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-cost-monitoring\"\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/cost-monitoring.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-cost-monitoring\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-cost-monitoring\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-cost-monitoring\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-cost-monitoring\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-cost-monitoring\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-cost-monitoring\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateCostMonitor\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-cost-monitoring\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/cost-monitoring.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-cost-monitoring\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-cost-monitoring\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-cost-monitoring\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-cost-monitoring\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-cost-monitoring\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-cost-monitoring\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateCostMonitor\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-cost-monitoring\"\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/cost-monitoring.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-cost-monitoring\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-cost-monitoring\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-cost-monitoring\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-cost-monitoring\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-cost-monitoring\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-cost-monitoring\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateCostMonitor\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-cost-monitoring\"\n"}}}}]}, {"ruleId": "CKV_K8S_35", "ruleIndex": 6, "level": "error", "attachments": [], "message": {"text": "Prefer using secrets as files over secrets as environment variables"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/cost-monitoring.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-cost-monitoring\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-cost-monitoring\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-cost-monitoring\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-cost-monitoring\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-cost-monitoring\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-cost-monitoring\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateCostMonitor\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-cost-monitoring\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/cost-monitoring.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-cost-monitoring\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-cost-monitoring\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-cost-monitoring\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-cost-monitoring\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-cost-monitoring\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-cost-monitoring\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateCostMonitor\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-cost-monitoring\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/cost-monitoring.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-cost-monitoring\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-cost-monitoring\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-cost-monitoring\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-cost-monitoring\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-cost-monitoring\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-cost-monitoring\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateCostMonitor\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-cost-monitoring\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/cost-monitoring.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-cost-monitoring\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-cost-monitoring\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-cost-monitoring\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-cost-monitoring\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-cost-monitoring\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-cost-monitoring\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateCostMonitor\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-cost-monitoring\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/cost-monitoring.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-cost-monitoring\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-cost-monitoring\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-cost-monitoring\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-cost-monitoring\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-cost-monitoring\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-cost-monitoring\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateCostMonitor\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-cost-monitoring\"\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/history.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-history\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-history\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-history\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-history\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-history\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-history\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.coreService\"\n            - name: ACTION\n              value: \"regionMetricsSnapshot\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-history\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/history.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-history\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-history\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-history\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-history\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-history\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-history\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.coreService\"\n            - name: ACTION\n              value: \"regionMetricsSnapshot\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-history\"\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/history.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-history\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-history\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-history\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-history\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-history\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-history\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.coreService\"\n            - name: ACTION\n              value: \"regionMetricsSnapshot\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-history\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/history.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-history\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-history\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-history\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-history\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-history\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-history\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.coreService\"\n            - name: ACTION\n              value: \"regionMetricsSnapshot\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-history\"\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/history.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-history\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-history\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-history\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-history\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-history\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-history\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.coreService\"\n            - name: ACTION\n              value: \"regionMetricsSnapshot\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-history\"\n"}}}}]}, {"ruleId": "CKV_K8S_35", "ruleIndex": 6, "level": "error", "attachments": [], "message": {"text": "Prefer using secrets as files over secrets as environment variables"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/history.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-history\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-history\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-history\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-history\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-history\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-history\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.coreService\"\n            - name: ACTION\n              value: \"regionMetricsSnapshot\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-history\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/history.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-history\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-history\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-history\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-history\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-history\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-history\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.coreService\"\n            - name: ACTION\n              value: \"regionMetricsSnapshot\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-history\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/history.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-history\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-history\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-history\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-history\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-history\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-history\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.coreService\"\n            - name: ACTION\n              value: \"regionMetricsSnapshot\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-history\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/history.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-history\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-history\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-history\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-history\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-history\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-history\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.coreService\"\n            - name: ACTION\n              value: \"regionMetricsSnapshot\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-history\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/history.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-history\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-history\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-history\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-history\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-history\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-history\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.coreService\"\n            - name: ACTION\n              value: \"regionMetricsSnapshot\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-history\"\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/cost-history.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-cost-history\"\nspec:\n  schedule: \"8 3 * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-cost-history\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-cost-history\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-cost-history\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-cost-history\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-cost-history\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateRegionCostSnapshot\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-cost-history\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/cost-history.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-cost-history\"\nspec:\n  schedule: \"8 3 * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-cost-history\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-cost-history\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-cost-history\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-cost-history\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-cost-history\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateRegionCostSnapshot\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-cost-history\"\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/cost-history.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-cost-history\"\nspec:\n  schedule: \"8 3 * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-cost-history\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-cost-history\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-cost-history\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-cost-history\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-cost-history\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateRegionCostSnapshot\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-cost-history\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/cost-history.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-cost-history\"\nspec:\n  schedule: \"8 3 * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-cost-history\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-cost-history\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-cost-history\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-cost-history\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-cost-history\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateRegionCostSnapshot\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-cost-history\"\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/cost-history.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-cost-history\"\nspec:\n  schedule: \"8 3 * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-cost-history\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-cost-history\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-cost-history\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-cost-history\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-cost-history\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateRegionCostSnapshot\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-cost-history\"\n"}}}}]}, {"ruleId": "CKV_K8S_35", "ruleIndex": 6, "level": "error", "attachments": [], "message": {"text": "Prefer using secrets as files over secrets as environment variables"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/cost-history.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-cost-history\"\nspec:\n  schedule: \"8 3 * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-cost-history\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-cost-history\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-cost-history\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-cost-history\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-cost-history\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateRegionCostSnapshot\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-cost-history\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/cost-history.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-cost-history\"\nspec:\n  schedule: \"8 3 * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-cost-history\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-cost-history\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-cost-history\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-cost-history\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-cost-history\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateRegionCostSnapshot\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-cost-history\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/cost-history.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-cost-history\"\nspec:\n  schedule: \"8 3 * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-cost-history\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-cost-history\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-cost-history\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-cost-history\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-cost-history\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateRegionCostSnapshot\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-cost-history\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/cost-history.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-cost-history\"\nspec:\n  schedule: \"8 3 * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-cost-history\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-cost-history\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-cost-history\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-cost-history\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-cost-history\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateRegionCostSnapshot\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-cost-history\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/cost-history.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-cost-history\"\nspec:\n  schedule: \"8 3 * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-cost-history\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-cost-history\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-cost-history\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-cost-history\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-cost-history\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"updateRegionCostSnapshot\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-cost-history\"\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/set-estimated-cost.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-set-estimated-cost\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-set-estimated-cost\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-set-estimated-cost\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-set-estimated-cost\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-set-estimated-cost\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-set-estimated-cost\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"setEstimatedCost\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-set-estimated-cost\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/set-estimated-cost.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-set-estimated-cost\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-set-estimated-cost\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-set-estimated-cost\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-set-estimated-cost\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-set-estimated-cost\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-set-estimated-cost\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"setEstimatedCost\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-set-estimated-cost\"\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/set-estimated-cost.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-set-estimated-cost\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-set-estimated-cost\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-set-estimated-cost\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-set-estimated-cost\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-set-estimated-cost\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-set-estimated-cost\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"setEstimatedCost\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-set-estimated-cost\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/set-estimated-cost.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-set-estimated-cost\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-set-estimated-cost\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-set-estimated-cost\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-set-estimated-cost\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-set-estimated-cost\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-set-estimated-cost\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"setEstimatedCost\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-set-estimated-cost\"\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/set-estimated-cost.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-set-estimated-cost\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-set-estimated-cost\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-set-estimated-cost\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-set-estimated-cost\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-set-estimated-cost\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-set-estimated-cost\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"setEstimatedCost\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-set-estimated-cost\"\n"}}}}]}, {"ruleId": "CKV_K8S_35", "ruleIndex": 6, "level": "error", "attachments": [], "message": {"text": "Prefer using secrets as files over secrets as environment variables"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/set-estimated-cost.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-set-estimated-cost\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-set-estimated-cost\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-set-estimated-cost\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-set-estimated-cost\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-set-estimated-cost\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-set-estimated-cost\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"setEstimatedCost\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-set-estimated-cost\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/set-estimated-cost.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-set-estimated-cost\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-set-estimated-cost\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-set-estimated-cost\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-set-estimated-cost\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-set-estimated-cost\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-set-estimated-cost\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"setEstimatedCost\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-set-estimated-cost\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/set-estimated-cost.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-set-estimated-cost\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-set-estimated-cost\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-set-estimated-cost\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-set-estimated-cost\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-set-estimated-cost\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-set-estimated-cost\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"setEstimatedCost\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-set-estimated-cost\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/set-estimated-cost.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-set-estimated-cost\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-set-estimated-cost\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-set-estimated-cost\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-set-estimated-cost\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-set-estimated-cost\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-set-estimated-cost\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"setEstimatedCost\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-set-estimated-cost\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/cronjobs/set-estimated-cost.yaml"}, "region": {"startLine": 3, "endLine": 69, "snippet": {"text": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: \"mcp-cj-set-estimated-cost\"\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: Forbid\n  startingDeadlineSeconds: 15\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  suspend: false\n  jobTemplate:\n    spec:\n      backoffLimit: 0\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n          labels:\n            app: cron-batch-job\n        spec:\n          restartPolicy: OnFailure\n          securityContext:\n            fsGroup: 100\n            runAsGroup: 100\n            runAsUser: 1000\n          volumes:\n          - name: npm-logs-mcp-cj-set-estimated-cost\n            emptyDir: {}\n          - name: npm-tmp-mcp-cj-set-estimated-cost\n            emptyDir: {}\n          containers:\n          - name: mcp-cj-set-estimated-cost\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: false\n              runAsGroup: 100\n              runAsUser: 1000\n            image: \"wr-studio-product/wrai-usp/mcp-cronjob:0.12.0-22.09\"\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 100m\n                memory: 200Mi\n              requests:\n                cpu: 50m\n                memory: 100Mi\n            volumeMounts:\n            - name: npm-logs-mcp-cj-set-estimated-cost\n              mountPath: /home/node/.npm\n            - name: npm-tmp-mcp-cj-set-estimated-cost\n              mountPath: /tmp\n            command: [\"npm\",\"run\",\"start\"]\n            env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cronjob\n                  key: mcpconfigCronjob\n            - name: SERVICE\n              value: \"mcp.cronjob.costService\"\n            - name: ACTION\n              value: \"setEstimatedCost\"\n            - name: PARAM1\n              value: us-west-1\n            - name: TASK\n              value: \"mcp-cj-set-estimated-cost\"\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/gateway.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-gateway-test-connection\"\n  labels:\n     app: \"mcp-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-gateway\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/gateway.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-gateway-test-connection\"\n  labels:\n     app: \"mcp-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-gateway\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/gateway.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-gateway-test-connection\"\n  labels:\n     app: \"mcp-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-gateway\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/gateway.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-gateway-test-connection\"\n  labels:\n     app: \"mcp-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-gateway\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/gateway.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-gateway-test-connection\"\n  labels:\n     app: \"mcp-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-gateway\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/gateway.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-gateway-test-connection\"\n  labels:\n     app: \"mcp-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-gateway\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/gateway.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-gateway-test-connection\"\n  labels:\n     app: \"mcp-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-gateway\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/gateway.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-gateway-test-connection\"\n  labels:\n     app: \"mcp-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-gateway\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/gateway.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-gateway-test-connection\"\n  labels:\n     app: \"mcp-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-gateway\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/gateway.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-gateway-test-connection\"\n  labels:\n     app: \"mcp-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-gateway\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/gateway.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-gateway-test-connection\"\n  labels:\n     app: \"mcp-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-gateway\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/gateway.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-gateway-test-connection\"\n  labels:\n     app: \"mcp-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-gateway\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/gateway.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-gateway-test-connection\"\n  labels:\n     app: \"mcp-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-gateway\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/gateway.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-gateway-test-connection\"\n  labels:\n     app: \"mcp-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-gateway\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/gateway.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-gateway-test-connection\"\n  labels:\n     app: \"mcp-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-gateway\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/gateway.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-gateway-test-connection\"\n  labels:\n     app: \"mcp-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-gateway\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/gateway.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-gateway-test-connection\"\n  labels:\n     app: \"mcp-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-gateway\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/gateway.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-gateway-test-connection\"\n  labels:\n     app: \"mcp-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-gateway\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/gateway.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-gateway-test-connection\"\n  labels:\n     app: \"mcp-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-gateway\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/ui.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-ui-test-connection\"\n  labels:\n     app: \"mcp-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-ui\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-ui:8888']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/ui.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-ui-test-connection\"\n  labels:\n     app: \"mcp-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-ui\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-ui:8888']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/ui.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-ui-test-connection\"\n  labels:\n     app: \"mcp-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-ui\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-ui:8888']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/ui.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-ui-test-connection\"\n  labels:\n     app: \"mcp-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-ui\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-ui:8888']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/ui.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-ui-test-connection\"\n  labels:\n     app: \"mcp-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-ui\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-ui:8888']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/ui.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-ui-test-connection\"\n  labels:\n     app: \"mcp-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-ui\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-ui:8888']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/ui.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-ui-test-connection\"\n  labels:\n     app: \"mcp-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-ui\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-ui:8888']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/ui.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-ui-test-connection\"\n  labels:\n     app: \"mcp-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-ui\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-ui:8888']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/ui.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-ui-test-connection\"\n  labels:\n     app: \"mcp-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-ui\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-ui:8888']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/ui.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-ui-test-connection\"\n  labels:\n     app: \"mcp-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-ui\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-ui:8888']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/ui.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-ui-test-connection\"\n  labels:\n     app: \"mcp-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-ui\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-ui:8888']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/ui.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-ui-test-connection\"\n  labels:\n     app: \"mcp-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-ui\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-ui:8888']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/ui.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-ui-test-connection\"\n  labels:\n     app: \"mcp-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-ui\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-ui:8888']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/ui.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-ui-test-connection\"\n  labels:\n     app: \"mcp-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-ui\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-ui:8888']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/ui.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-ui-test-connection\"\n  labels:\n     app: \"mcp-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-ui\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-ui:8888']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/ui.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-ui-test-connection\"\n  labels:\n     app: \"mcp-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-ui\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-ui:8888']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/ui.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-ui-test-connection\"\n  labels:\n     app: \"mcp-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-ui\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-ui:8888']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/ui.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-ui-test-connection\"\n  labels:\n     app: \"mcp-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-ui\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-ui:8888']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/tests/ui.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"mcp-ui-test-connection\"\n  labels:\n     app: \"mcp-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: \"test-for-mcp-ui\"\n      image: system.registry.dev.wrstudio1.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['mcp-ui:8888']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/serviceaccount/3ptool.yaml"}, "region": {"startLine": 3, "endLine": 8, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  namespace: \"default\"\n  name: \"mcp-3ptool-installer\"\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/serviceaccount/3ptool.yaml"}, "region": {"startLine": 10, "endLine": 27, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: \"mcp-3ptool-installer\"\n  namespace: \"default\"\nrules:\n  - apiGroups:\n      - \"\"\n      - \"batch\"\n    resources:\n      - jobs\n    verbs:\n      - get\n      - list\n      - watch\n      - create\n      - delete\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/serviceaccount/3ptool.yaml"}, "region": {"startLine": 29, "endLine": 41, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: \"mcp-3ptool-installer\"\n  namespace: \"default\"\nsubjects:\n- kind: ServiceAccount\n  name: \"mcp-3ptool-installer\"\n  namespace: \"default\"\nroleRef:\n  kind: Role\n  name: \"mcp-3ptool-installer\"\n  apiGroup: rbac.authorization.k8s.io\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/serviceaccount/componentInstaller.yaml"}, "region": {"startLine": 3, "endLine": 8, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: \"mcp-content-installer\"\n  namespace: default\nautomountServiceAccountToken: true\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/serviceaccount/init-db.yaml"}, "region": {"startLine": 3, "endLine": 12, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: \"mcp-init\"\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation\nautomountServiceAccountToken: true\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/serviceaccount/cost.yaml"}, "region": {"startLine": 3, "endLine": 8, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: \"mcp-cost\"\n  namespace: default\nautomountServiceAccountToken: true\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/serviceaccount/gateway.yaml"}, "region": {"startLine": 3, "endLine": 8, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: \"mcp-gateway\"\n  namespace: default\nautomountServiceAccountToken: false\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/serviceaccount/ui.yaml"}, "region": {"startLine": 3, "endLine": 8, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: \"mcp-ui\"\n  namespace: default\nautomountServiceAccountToken: false\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/serviceaccount/core.yaml"}, "region": {"startLine": 3, "endLine": 9, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: \"mcp-core\"\n  namespace: default\nautomountServiceAccountToken: true\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/serviceaccount/install.yaml"}, "region": {"startLine": 3, "endLine": 9, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  namespace: default \n  name: \"mcp-install\"\nautomountServiceAccountToken: true\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/serviceaccount/install.yaml"}, "region": {"startLine": 35, "endLine": 50, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: default\n  name: \"mcp-install\"\nrules:\n - apiGroups: [\"\"]\n   resources: [\"pods\"]\n   verbs: [\"get\", \"watch\", \"list\", \"create\", \"delete\"]\n - apiGroups: [\"\"]\n   resources: [\"pod/logs\"]\n   verbs: [\"get\"]\n - apiGroups: [\"\"]\n   resources: [\"configmaps\"]\n   verbs: [\"get\", \"watch\", \"list\", \"create\", \"delete\"]\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/serviceaccount/install.yaml"}, "region": {"startLine": 52, "endLine": 64, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: \"mcp-install\"\n  namespace: default\nsubjects:\n- kind: ServiceAccount\n  name: mcp-install\n  apiGroup: \"\"\nroleRef:\n  kind: Role\n  name: \"mcp-install\"\n  apiGroup: rbac.authorization.k8s.io\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/tp-tool-installer.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-3ptool-installer\n  labels:\n    helm.sh/chart: mcp-3ptool-installer-1.0.0\n    app.kubernetes.io/name: mcp-3ptool-installer\n    app.kubernetes.io/instance: mcp-3ptool-installer\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-3ptool-installer\n      app.kubernetes.io/instance: mcp-3ptool-installer\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-3ptool-installer\n        app.kubernetes.io/instance: mcp-3ptool-installer\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-tptoolinstaller-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-3ptool-installer\n      containers:\n        - name: mcp-3ptool-installer\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-3ptool-installer:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 120Mi\n          volumeMounts:\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-tp-tools\n                  key: mcpconfig3pTools\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/tp-tool-installer.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-3ptool-installer\n  labels:\n    helm.sh/chart: mcp-3ptool-installer-1.0.0\n    app.kubernetes.io/name: mcp-3ptool-installer\n    app.kubernetes.io/instance: mcp-3ptool-installer\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-3ptool-installer\n      app.kubernetes.io/instance: mcp-3ptool-installer\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-3ptool-installer\n        app.kubernetes.io/instance: mcp-3ptool-installer\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-tptoolinstaller-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-3ptool-installer\n      containers:\n        - name: mcp-3ptool-installer\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-3ptool-installer:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 120Mi\n          volumeMounts:\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-tp-tools\n                  key: mcpconfig3pTools\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/tp-tool-installer.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-3ptool-installer\n  labels:\n    helm.sh/chart: mcp-3ptool-installer-1.0.0\n    app.kubernetes.io/name: mcp-3ptool-installer\n    app.kubernetes.io/instance: mcp-3ptool-installer\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-3ptool-installer\n      app.kubernetes.io/instance: mcp-3ptool-installer\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-3ptool-installer\n        app.kubernetes.io/instance: mcp-3ptool-installer\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-tptoolinstaller-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-3ptool-installer\n      containers:\n        - name: mcp-3ptool-installer\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-3ptool-installer:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 120Mi\n          volumeMounts:\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-tp-tools\n                  key: mcpconfig3pTools\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/tp-tool-installer.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-3ptool-installer\n  labels:\n    helm.sh/chart: mcp-3ptool-installer-1.0.0\n    app.kubernetes.io/name: mcp-3ptool-installer\n    app.kubernetes.io/instance: mcp-3ptool-installer\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-3ptool-installer\n      app.kubernetes.io/instance: mcp-3ptool-installer\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-3ptool-installer\n        app.kubernetes.io/instance: mcp-3ptool-installer\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-tptoolinstaller-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-3ptool-installer\n      containers:\n        - name: mcp-3ptool-installer\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-3ptool-installer:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 120Mi\n          volumeMounts:\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-tp-tools\n                  key: mcpconfig3pTools\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/tp-tool-installer.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-3ptool-installer\n  labels:\n    helm.sh/chart: mcp-3ptool-installer-1.0.0\n    app.kubernetes.io/name: mcp-3ptool-installer\n    app.kubernetes.io/instance: mcp-3ptool-installer\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-3ptool-installer\n      app.kubernetes.io/instance: mcp-3ptool-installer\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-3ptool-installer\n        app.kubernetes.io/instance: mcp-3ptool-installer\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-tptoolinstaller-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-3ptool-installer\n      containers:\n        - name: mcp-3ptool-installer\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-3ptool-installer:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 120Mi\n          volumeMounts:\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-tp-tools\n                  key: mcpconfig3pTools\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/tp-tool-installer.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-3ptool-installer\n  labels:\n    helm.sh/chart: mcp-3ptool-installer-1.0.0\n    app.kubernetes.io/name: mcp-3ptool-installer\n    app.kubernetes.io/instance: mcp-3ptool-installer\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-3ptool-installer\n      app.kubernetes.io/instance: mcp-3ptool-installer\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-3ptool-installer\n        app.kubernetes.io/instance: mcp-3ptool-installer\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-tptoolinstaller-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-3ptool-installer\n      containers:\n        - name: mcp-3ptool-installer\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-3ptool-installer:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 120Mi\n          volumeMounts:\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-tp-tools\n                  key: mcpconfig3pTools\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/tp-tool-installer.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-3ptool-installer\n  labels:\n    helm.sh/chart: mcp-3ptool-installer-1.0.0\n    app.kubernetes.io/name: mcp-3ptool-installer\n    app.kubernetes.io/instance: mcp-3ptool-installer\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-3ptool-installer\n      app.kubernetes.io/instance: mcp-3ptool-installer\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-3ptool-installer\n        app.kubernetes.io/instance: mcp-3ptool-installer\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-tptoolinstaller-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-3ptool-installer\n      containers:\n        - name: mcp-3ptool-installer\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-3ptool-installer:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 120Mi\n          volumeMounts:\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-tp-tools\n                  key: mcpconfig3pTools\n"}}}}]}, {"ruleId": "CKV_K8S_35", "ruleIndex": 6, "level": "error", "attachments": [], "message": {"text": "Prefer using secrets as files over secrets as environment variables"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/tp-tool-installer.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-3ptool-installer\n  labels:\n    helm.sh/chart: mcp-3ptool-installer-1.0.0\n    app.kubernetes.io/name: mcp-3ptool-installer\n    app.kubernetes.io/instance: mcp-3ptool-installer\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-3ptool-installer\n      app.kubernetes.io/instance: mcp-3ptool-installer\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-3ptool-installer\n        app.kubernetes.io/instance: mcp-3ptool-installer\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-tptoolinstaller-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-3ptool-installer\n      containers:\n        - name: mcp-3ptool-installer\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-3ptool-installer:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 120Mi\n          volumeMounts:\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-tp-tools\n                  key: mcpconfig3pTools\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/tp-tool-installer.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-3ptool-installer\n  labels:\n    helm.sh/chart: mcp-3ptool-installer-1.0.0\n    app.kubernetes.io/name: mcp-3ptool-installer\n    app.kubernetes.io/instance: mcp-3ptool-installer\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-3ptool-installer\n      app.kubernetes.io/instance: mcp-3ptool-installer\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-3ptool-installer\n        app.kubernetes.io/instance: mcp-3ptool-installer\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-tptoolinstaller-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-3ptool-installer\n      containers:\n        - name: mcp-3ptool-installer\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-3ptool-installer:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 120Mi\n          volumeMounts:\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-tp-tools\n                  key: mcpconfig3pTools\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/tp-tool-installer.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-3ptool-installer\n  labels:\n    helm.sh/chart: mcp-3ptool-installer-1.0.0\n    app.kubernetes.io/name: mcp-3ptool-installer\n    app.kubernetes.io/instance: mcp-3ptool-installer\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-3ptool-installer\n      app.kubernetes.io/instance: mcp-3ptool-installer\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-3ptool-installer\n        app.kubernetes.io/instance: mcp-3ptool-installer\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-tptoolinstaller-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-3ptool-installer\n      containers:\n        - name: mcp-3ptool-installer\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-3ptool-installer:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 120Mi\n          volumeMounts:\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-tp-tools\n                  key: mcpconfig3pTools\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/tp-tool-installer.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-3ptool-installer\n  labels:\n    helm.sh/chart: mcp-3ptool-installer-1.0.0\n    app.kubernetes.io/name: mcp-3ptool-installer\n    app.kubernetes.io/instance: mcp-3ptool-installer\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-3ptool-installer\n      app.kubernetes.io/instance: mcp-3ptool-installer\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-3ptool-installer\n        app.kubernetes.io/instance: mcp-3ptool-installer\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-tptoolinstaller-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-3ptool-installer\n      containers:\n        - name: mcp-3ptool-installer\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-3ptool-installer:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 120Mi\n          volumeMounts:\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-tp-tools\n                  key: mcpconfig3pTools\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/tp-tool-installer.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-3ptool-installer\n  labels:\n    helm.sh/chart: mcp-3ptool-installer-1.0.0\n    app.kubernetes.io/name: mcp-3ptool-installer\n    app.kubernetes.io/instance: mcp-3ptool-installer\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-3ptool-installer\n      app.kubernetes.io/instance: mcp-3ptool-installer\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-3ptool-installer\n        app.kubernetes.io/instance: mcp-3ptool-installer\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-tptoolinstaller-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-3ptool-installer\n      containers:\n        - name: mcp-3ptool-installer\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-3ptool-installer:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 120Mi\n          volumeMounts:\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-tp-tools\n                  key: mcpconfig3pTools\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/tp-tool-installer.yaml"}, "region": {"startLine": 3, "endLine": 65, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-3ptool-installer\n  labels:\n    helm.sh/chart: mcp-3ptool-installer-1.0.0\n    app.kubernetes.io/name: mcp-3ptool-installer\n    app.kubernetes.io/instance: mcp-3ptool-installer\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-3ptool-installer\n      app.kubernetes.io/instance: mcp-3ptool-installer\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-3ptool-installer\n        app.kubernetes.io/instance: mcp-3ptool-installer\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-tptoolinstaller-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-3ptool-installer\n      containers:\n        - name: mcp-3ptool-installer\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-3ptool-installer:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 120Mi\n          volumeMounts:\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-tptoolinstaller-node-fs\n              mountPath: /home/node/.cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-tp-tools\n                  key: mcpconfig3pTools\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/cost.yaml"}, "region": {"startLine": 3, "endLine": 66, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-cost\n  labels:\n    helm.sh/chart: mcp-cost-1.0.0\n    app.kubernetes.io/name: mcp-cost\n    app.kubernetes.io/instance: mcp-cost\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-cost\n      app.kubernetes.io/instance: mcp-cost\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-cost\n        app.kubernetes.io/instance: mcp-cost\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-cost-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-cost\n      containers:\n        - name: mcp-cost\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-cost:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 300Mi\n            requests:\n              cpu: 50m\n              memory: 150Mi\n          volumeMounts:\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-cost-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.cloudify\n              subPath: cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cost\n                  key: mcpconfigCost\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/cost.yaml"}, "region": {"startLine": 3, "endLine": 66, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-cost\n  labels:\n    helm.sh/chart: mcp-cost-1.0.0\n    app.kubernetes.io/name: mcp-cost\n    app.kubernetes.io/instance: mcp-cost\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-cost\n      app.kubernetes.io/instance: mcp-cost\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-cost\n        app.kubernetes.io/instance: mcp-cost\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-cost-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-cost\n      containers:\n        - name: mcp-cost\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-cost:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 300Mi\n            requests:\n              cpu: 50m\n              memory: 150Mi\n          volumeMounts:\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-cost-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.cloudify\n              subPath: cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cost\n                  key: mcpconfigCost\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/cost.yaml"}, "region": {"startLine": 3, "endLine": 66, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-cost\n  labels:\n    helm.sh/chart: mcp-cost-1.0.0\n    app.kubernetes.io/name: mcp-cost\n    app.kubernetes.io/instance: mcp-cost\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-cost\n      app.kubernetes.io/instance: mcp-cost\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-cost\n        app.kubernetes.io/instance: mcp-cost\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-cost-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-cost\n      containers:\n        - name: mcp-cost\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-cost:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 300Mi\n            requests:\n              cpu: 50m\n              memory: 150Mi\n          volumeMounts:\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-cost-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.cloudify\n              subPath: cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cost\n                  key: mcpconfigCost\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/cost.yaml"}, "region": {"startLine": 3, "endLine": 66, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-cost\n  labels:\n    helm.sh/chart: mcp-cost-1.0.0\n    app.kubernetes.io/name: mcp-cost\n    app.kubernetes.io/instance: mcp-cost\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-cost\n      app.kubernetes.io/instance: mcp-cost\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-cost\n        app.kubernetes.io/instance: mcp-cost\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-cost-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-cost\n      containers:\n        - name: mcp-cost\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-cost:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 300Mi\n            requests:\n              cpu: 50m\n              memory: 150Mi\n          volumeMounts:\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-cost-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.cloudify\n              subPath: cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cost\n                  key: mcpconfigCost\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/cost.yaml"}, "region": {"startLine": 3, "endLine": 66, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-cost\n  labels:\n    helm.sh/chart: mcp-cost-1.0.0\n    app.kubernetes.io/name: mcp-cost\n    app.kubernetes.io/instance: mcp-cost\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-cost\n      app.kubernetes.io/instance: mcp-cost\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-cost\n        app.kubernetes.io/instance: mcp-cost\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-cost-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-cost\n      containers:\n        - name: mcp-cost\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-cost:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 300Mi\n            requests:\n              cpu: 50m\n              memory: 150Mi\n          volumeMounts:\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-cost-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.cloudify\n              subPath: cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cost\n                  key: mcpconfigCost\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/cost.yaml"}, "region": {"startLine": 3, "endLine": 66, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-cost\n  labels:\n    helm.sh/chart: mcp-cost-1.0.0\n    app.kubernetes.io/name: mcp-cost\n    app.kubernetes.io/instance: mcp-cost\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-cost\n      app.kubernetes.io/instance: mcp-cost\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-cost\n        app.kubernetes.io/instance: mcp-cost\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-cost-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-cost\n      containers:\n        - name: mcp-cost\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-cost:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 300Mi\n            requests:\n              cpu: 50m\n              memory: 150Mi\n          volumeMounts:\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-cost-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.cloudify\n              subPath: cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cost\n                  key: mcpconfigCost\n"}}}}]}, {"ruleId": "CKV_K8S_35", "ruleIndex": 6, "level": "error", "attachments": [], "message": {"text": "Prefer using secrets as files over secrets as environment variables"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/cost.yaml"}, "region": {"startLine": 3, "endLine": 66, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-cost\n  labels:\n    helm.sh/chart: mcp-cost-1.0.0\n    app.kubernetes.io/name: mcp-cost\n    app.kubernetes.io/instance: mcp-cost\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-cost\n      app.kubernetes.io/instance: mcp-cost\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-cost\n        app.kubernetes.io/instance: mcp-cost\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-cost-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-cost\n      containers:\n        - name: mcp-cost\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-cost:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 300Mi\n            requests:\n              cpu: 50m\n              memory: 150Mi\n          volumeMounts:\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-cost-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.cloudify\n              subPath: cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cost\n                  key: mcpconfigCost\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/cost.yaml"}, "region": {"startLine": 3, "endLine": 66, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-cost\n  labels:\n    helm.sh/chart: mcp-cost-1.0.0\n    app.kubernetes.io/name: mcp-cost\n    app.kubernetes.io/instance: mcp-cost\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-cost\n      app.kubernetes.io/instance: mcp-cost\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-cost\n        app.kubernetes.io/instance: mcp-cost\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-cost-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-cost\n      containers:\n        - name: mcp-cost\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-cost:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 300Mi\n            requests:\n              cpu: 50m\n              memory: 150Mi\n          volumeMounts:\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-cost-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.cloudify\n              subPath: cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cost\n                  key: mcpconfigCost\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/cost.yaml"}, "region": {"startLine": 3, "endLine": 66, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-cost\n  labels:\n    helm.sh/chart: mcp-cost-1.0.0\n    app.kubernetes.io/name: mcp-cost\n    app.kubernetes.io/instance: mcp-cost\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-cost\n      app.kubernetes.io/instance: mcp-cost\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-cost\n        app.kubernetes.io/instance: mcp-cost\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-cost-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-cost\n      containers:\n        - name: mcp-cost\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-cost:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 300Mi\n            requests:\n              cpu: 50m\n              memory: 150Mi\n          volumeMounts:\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-cost-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.cloudify\n              subPath: cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cost\n                  key: mcpconfigCost\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/cost.yaml"}, "region": {"startLine": 3, "endLine": 66, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-cost\n  labels:\n    helm.sh/chart: mcp-cost-1.0.0\n    app.kubernetes.io/name: mcp-cost\n    app.kubernetes.io/instance: mcp-cost\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-cost\n      app.kubernetes.io/instance: mcp-cost\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-cost\n        app.kubernetes.io/instance: mcp-cost\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-cost-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-cost\n      containers:\n        - name: mcp-cost\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-cost:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 300Mi\n            requests:\n              cpu: 50m\n              memory: 150Mi\n          volumeMounts:\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-cost-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.cloudify\n              subPath: cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cost\n                  key: mcpconfigCost\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/cost.yaml"}, "region": {"startLine": 3, "endLine": 66, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-cost\n  labels:\n    helm.sh/chart: mcp-cost-1.0.0\n    app.kubernetes.io/name: mcp-cost\n    app.kubernetes.io/instance: mcp-cost\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-cost\n      app.kubernetes.io/instance: mcp-cost\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-cost\n        app.kubernetes.io/instance: mcp-cost\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-cost-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-cost\n      containers:\n        - name: mcp-cost\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-cost:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 300Mi\n            requests:\n              cpu: 50m\n              memory: 150Mi\n          volumeMounts:\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-cost-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.cloudify\n              subPath: cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cost\n                  key: mcpconfigCost\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/cost.yaml"}, "region": {"startLine": 3, "endLine": 66, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-cost\n  labels:\n    helm.sh/chart: mcp-cost-1.0.0\n    app.kubernetes.io/name: mcp-cost\n    app.kubernetes.io/instance: mcp-cost\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-cost\n      app.kubernetes.io/instance: mcp-cost\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-cost\n        app.kubernetes.io/instance: mcp-cost\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-cost-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-cost\n      containers:\n        - name: mcp-cost\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-cost:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 300Mi\n            requests:\n              cpu: 50m\n              memory: 150Mi\n          volumeMounts:\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-cost-node-fs\n              mountPath: /tmp\n              subPath: tmp\n            - name: mcp-cost-node-fs\n              mountPath: /home/node/.cloudify\n              subPath: cloudify\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-cost\n                  key: mcpconfigCost\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/gateway.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-gateway\n  labels:\n    helm.sh/chart: mcp-gateway-1.0.0\n    app.kubernetes.io/name: mcp-gateway\n    app.kubernetes.io/instance: mcp-gateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-gateway\n      app.kubernetes.io/instance: mcp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: mcp-gateway\n        app.kubernetes.io/instance: mcp-gateway\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-gateway-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-gateway\n      containers:\n        - name: mcp-gateway\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-gateway:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: http\n          resources:\n            limits:\n              cpu: 100m\n              memory: 320Mi\n            requests:\n              cpu: 50m\n              memory: 160Mi\n          volumeMounts:\n            - name: mcp-gateway-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-gateway-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-gateway\n                  key: mcpconfigGateway\n            - name: ATTEMPTS\n              value: \"15\"\n            - name: DELAY_RETRY\n              value: \"30000\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/gateway.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-gateway\n  labels:\n    helm.sh/chart: mcp-gateway-1.0.0\n    app.kubernetes.io/name: mcp-gateway\n    app.kubernetes.io/instance: mcp-gateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-gateway\n      app.kubernetes.io/instance: mcp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: mcp-gateway\n        app.kubernetes.io/instance: mcp-gateway\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-gateway-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-gateway\n      containers:\n        - name: mcp-gateway\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-gateway:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: http\n          resources:\n            limits:\n              cpu: 100m\n              memory: 320Mi\n            requests:\n              cpu: 50m\n              memory: 160Mi\n          volumeMounts:\n            - name: mcp-gateway-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-gateway-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-gateway\n                  key: mcpconfigGateway\n            - name: ATTEMPTS\n              value: \"15\"\n            - name: DELAY_RETRY\n              value: \"30000\"\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/gateway.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-gateway\n  labels:\n    helm.sh/chart: mcp-gateway-1.0.0\n    app.kubernetes.io/name: mcp-gateway\n    app.kubernetes.io/instance: mcp-gateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-gateway\n      app.kubernetes.io/instance: mcp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: mcp-gateway\n        app.kubernetes.io/instance: mcp-gateway\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-gateway-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-gateway\n      containers:\n        - name: mcp-gateway\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-gateway:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: http\n          resources:\n            limits:\n              cpu: 100m\n              memory: 320Mi\n            requests:\n              cpu: 50m\n              memory: 160Mi\n          volumeMounts:\n            - name: mcp-gateway-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-gateway-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-gateway\n                  key: mcpconfigGateway\n            - name: ATTEMPTS\n              value: \"15\"\n            - name: DELAY_RETRY\n              value: \"30000\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/gateway.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-gateway\n  labels:\n    helm.sh/chart: mcp-gateway-1.0.0\n    app.kubernetes.io/name: mcp-gateway\n    app.kubernetes.io/instance: mcp-gateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-gateway\n      app.kubernetes.io/instance: mcp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: mcp-gateway\n        app.kubernetes.io/instance: mcp-gateway\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-gateway-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-gateway\n      containers:\n        - name: mcp-gateway\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-gateway:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: http\n          resources:\n            limits:\n              cpu: 100m\n              memory: 320Mi\n            requests:\n              cpu: 50m\n              memory: 160Mi\n          volumeMounts:\n            - name: mcp-gateway-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-gateway-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-gateway\n                  key: mcpconfigGateway\n            - name: ATTEMPTS\n              value: \"15\"\n            - name: DELAY_RETRY\n              value: \"30000\"\n"}}}}]}, {"ruleId": "CKV_K8S_35", "ruleIndex": 6, "level": "error", "attachments": [], "message": {"text": "Prefer using secrets as files over secrets as environment variables"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/gateway.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-gateway\n  labels:\n    helm.sh/chart: mcp-gateway-1.0.0\n    app.kubernetes.io/name: mcp-gateway\n    app.kubernetes.io/instance: mcp-gateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-gateway\n      app.kubernetes.io/instance: mcp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: mcp-gateway\n        app.kubernetes.io/instance: mcp-gateway\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-gateway-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-gateway\n      containers:\n        - name: mcp-gateway\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-gateway:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: http\n          resources:\n            limits:\n              cpu: 100m\n              memory: 320Mi\n            requests:\n              cpu: 50m\n              memory: 160Mi\n          volumeMounts:\n            - name: mcp-gateway-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-gateway-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-gateway\n                  key: mcpconfigGateway\n            - name: ATTEMPTS\n              value: \"15\"\n            - name: DELAY_RETRY\n              value: \"30000\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/gateway.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-gateway\n  labels:\n    helm.sh/chart: mcp-gateway-1.0.0\n    app.kubernetes.io/name: mcp-gateway\n    app.kubernetes.io/instance: mcp-gateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-gateway\n      app.kubernetes.io/instance: mcp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: mcp-gateway\n        app.kubernetes.io/instance: mcp-gateway\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-gateway-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-gateway\n      containers:\n        - name: mcp-gateway\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-gateway:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: http\n          resources:\n            limits:\n              cpu: 100m\n              memory: 320Mi\n            requests:\n              cpu: 50m\n              memory: 160Mi\n          volumeMounts:\n            - name: mcp-gateway-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-gateway-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-gateway\n                  key: mcpconfigGateway\n            - name: ATTEMPTS\n              value: \"15\"\n            - name: DELAY_RETRY\n              value: \"30000\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/gateway.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-gateway\n  labels:\n    helm.sh/chart: mcp-gateway-1.0.0\n    app.kubernetes.io/name: mcp-gateway\n    app.kubernetes.io/instance: mcp-gateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-gateway\n      app.kubernetes.io/instance: mcp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: mcp-gateway\n        app.kubernetes.io/instance: mcp-gateway\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-gateway-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-gateway\n      containers:\n        - name: mcp-gateway\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-gateway:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: http\n          resources:\n            limits:\n              cpu: 100m\n              memory: 320Mi\n            requests:\n              cpu: 50m\n              memory: 160Mi\n          volumeMounts:\n            - name: mcp-gateway-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-gateway-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-gateway\n                  key: mcpconfigGateway\n            - name: ATTEMPTS\n              value: \"15\"\n            - name: DELAY_RETRY\n              value: \"30000\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/gateway.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-gateway\n  labels:\n    helm.sh/chart: mcp-gateway-1.0.0\n    app.kubernetes.io/name: mcp-gateway\n    app.kubernetes.io/instance: mcp-gateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-gateway\n      app.kubernetes.io/instance: mcp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: mcp-gateway\n        app.kubernetes.io/instance: mcp-gateway\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-gateway-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-gateway\n      containers:\n        - name: mcp-gateway\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-gateway:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: http\n          resources:\n            limits:\n              cpu: 100m\n              memory: 320Mi\n            requests:\n              cpu: 50m\n              memory: 160Mi\n          volumeMounts:\n            - name: mcp-gateway-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-gateway-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-gateway\n                  key: mcpconfigGateway\n            - name: ATTEMPTS\n              value: \"15\"\n            - name: DELAY_RETRY\n              value: \"30000\"\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/gateway.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-gateway\n  labels:\n    helm.sh/chart: mcp-gateway-1.0.0\n    app.kubernetes.io/name: mcp-gateway\n    app.kubernetes.io/instance: mcp-gateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-gateway\n      app.kubernetes.io/instance: mcp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: mcp-gateway\n        app.kubernetes.io/instance: mcp-gateway\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-gateway-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-gateway\n      containers:\n        - name: mcp-gateway\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-gateway:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: http\n          resources:\n            limits:\n              cpu: 100m\n              memory: 320Mi\n            requests:\n              cpu: 50m\n              memory: 160Mi\n          volumeMounts:\n            - name: mcp-gateway-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-gateway-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-gateway\n                  key: mcpconfigGateway\n            - name: ATTEMPTS\n              value: \"15\"\n            - name: DELAY_RETRY\n              value: \"30000\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/gateway.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-gateway\n  labels:\n    helm.sh/chart: mcp-gateway-1.0.0\n    app.kubernetes.io/name: mcp-gateway\n    app.kubernetes.io/instance: mcp-gateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-gateway\n      app.kubernetes.io/instance: mcp-gateway\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: mcp-gateway\n        app.kubernetes.io/instance: mcp-gateway\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-gateway-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-gateway\n      containers:\n        - name: mcp-gateway\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-gateway:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: http\n          resources:\n            limits:\n              cpu: 100m\n              memory: 320Mi\n            requests:\n              cpu: 50m\n              memory: 160Mi\n          volumeMounts:\n            - name: mcp-gateway-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-gateway-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-gateway\n                  key: mcpconfigGateway\n            - name: ATTEMPTS\n              value: \"15\"\n            - name: DELAY_RETRY\n              value: \"30000\"\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/ui.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-ui\n  labels:\n    helm.sh/chart: mcp-ui-1.0.0\n    app.kubernetes.io/name: mcp-ui\n    app.kubernetes.io/instance: mcp-ui\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-ui\n      app.kubernetes.io/instance: mcp-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-ui\n        app.kubernetes.io/instance: mcp-ui\n    spec:\n      volumes:\n        - name: mcp-ui-node-fs\n          emptyDir: {}\n      securityContext:\n        {}\n      serviceAccountName: mcp-ui\n      containers:\n        - name: mcp-ui\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-ui:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 4200\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 8888\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8888\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 115Mi\n          volumeMounts:\n            - name: mcp-ui-node-fs\n              mountPath: /usr/share/nginx/html/assets/config\n              subPath: assets\n            - name: mcp-ui-node-fs\n              mountPath: /run/nginx/\n              subPath: run\n            - name: mcp-ui-node-fs\n              mountPath: /var/log/nginx/\n              subPath: logs\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://mcp.gateway.devstar.cloud\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"https://um.gateway.dev.wrstudio1.cloud\\\",\\\"repoApiUrl\\\":\\\"https://gitlab.dev.wrstudio1.cloud/glms\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.api.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/ui.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-ui\n  labels:\n    helm.sh/chart: mcp-ui-1.0.0\n    app.kubernetes.io/name: mcp-ui\n    app.kubernetes.io/instance: mcp-ui\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-ui\n      app.kubernetes.io/instance: mcp-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-ui\n        app.kubernetes.io/instance: mcp-ui\n    spec:\n      volumes:\n        - name: mcp-ui-node-fs\n          emptyDir: {}\n      securityContext:\n        {}\n      serviceAccountName: mcp-ui\n      containers:\n        - name: mcp-ui\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-ui:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 4200\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 8888\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8888\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 115Mi\n          volumeMounts:\n            - name: mcp-ui-node-fs\n              mountPath: /usr/share/nginx/html/assets/config\n              subPath: assets\n            - name: mcp-ui-node-fs\n              mountPath: /run/nginx/\n              subPath: run\n            - name: mcp-ui-node-fs\n              mountPath: /var/log/nginx/\n              subPath: logs\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://mcp.gateway.devstar.cloud\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"https://um.gateway.dev.wrstudio1.cloud\\\",\\\"repoApiUrl\\\":\\\"https://gitlab.dev.wrstudio1.cloud/glms\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.api.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/ui.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-ui\n  labels:\n    helm.sh/chart: mcp-ui-1.0.0\n    app.kubernetes.io/name: mcp-ui\n    app.kubernetes.io/instance: mcp-ui\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-ui\n      app.kubernetes.io/instance: mcp-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-ui\n        app.kubernetes.io/instance: mcp-ui\n    spec:\n      volumes:\n        - name: mcp-ui-node-fs\n          emptyDir: {}\n      securityContext:\n        {}\n      serviceAccountName: mcp-ui\n      containers:\n        - name: mcp-ui\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-ui:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 4200\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 8888\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8888\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 115Mi\n          volumeMounts:\n            - name: mcp-ui-node-fs\n              mountPath: /usr/share/nginx/html/assets/config\n              subPath: assets\n            - name: mcp-ui-node-fs\n              mountPath: /run/nginx/\n              subPath: run\n            - name: mcp-ui-node-fs\n              mountPath: /var/log/nginx/\n              subPath: logs\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://mcp.gateway.devstar.cloud\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"https://um.gateway.dev.wrstudio1.cloud\\\",\\\"repoApiUrl\\\":\\\"https://gitlab.dev.wrstudio1.cloud/glms\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.api.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/ui.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-ui\n  labels:\n    helm.sh/chart: mcp-ui-1.0.0\n    app.kubernetes.io/name: mcp-ui\n    app.kubernetes.io/instance: mcp-ui\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-ui\n      app.kubernetes.io/instance: mcp-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-ui\n        app.kubernetes.io/instance: mcp-ui\n    spec:\n      volumes:\n        - name: mcp-ui-node-fs\n          emptyDir: {}\n      securityContext:\n        {}\n      serviceAccountName: mcp-ui\n      containers:\n        - name: mcp-ui\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-ui:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 4200\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 8888\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8888\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 115Mi\n          volumeMounts:\n            - name: mcp-ui-node-fs\n              mountPath: /usr/share/nginx/html/assets/config\n              subPath: assets\n            - name: mcp-ui-node-fs\n              mountPath: /run/nginx/\n              subPath: run\n            - name: mcp-ui-node-fs\n              mountPath: /var/log/nginx/\n              subPath: logs\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://mcp.gateway.devstar.cloud\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"https://um.gateway.dev.wrstudio1.cloud\\\",\\\"repoApiUrl\\\":\\\"https://gitlab.dev.wrstudio1.cloud/glms\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.api.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/ui.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-ui\n  labels:\n    helm.sh/chart: mcp-ui-1.0.0\n    app.kubernetes.io/name: mcp-ui\n    app.kubernetes.io/instance: mcp-ui\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-ui\n      app.kubernetes.io/instance: mcp-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-ui\n        app.kubernetes.io/instance: mcp-ui\n    spec:\n      volumes:\n        - name: mcp-ui-node-fs\n          emptyDir: {}\n      securityContext:\n        {}\n      serviceAccountName: mcp-ui\n      containers:\n        - name: mcp-ui\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-ui:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 4200\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 8888\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8888\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 115Mi\n          volumeMounts:\n            - name: mcp-ui-node-fs\n              mountPath: /usr/share/nginx/html/assets/config\n              subPath: assets\n            - name: mcp-ui-node-fs\n              mountPath: /run/nginx/\n              subPath: run\n            - name: mcp-ui-node-fs\n              mountPath: /var/log/nginx/\n              subPath: logs\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://mcp.gateway.devstar.cloud\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"https://um.gateway.dev.wrstudio1.cloud\\\",\\\"repoApiUrl\\\":\\\"https://gitlab.dev.wrstudio1.cloud/glms\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.api.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/ui.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-ui\n  labels:\n    helm.sh/chart: mcp-ui-1.0.0\n    app.kubernetes.io/name: mcp-ui\n    app.kubernetes.io/instance: mcp-ui\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-ui\n      app.kubernetes.io/instance: mcp-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-ui\n        app.kubernetes.io/instance: mcp-ui\n    spec:\n      volumes:\n        - name: mcp-ui-node-fs\n          emptyDir: {}\n      securityContext:\n        {}\n      serviceAccountName: mcp-ui\n      containers:\n        - name: mcp-ui\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-ui:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 4200\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 8888\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8888\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 115Mi\n          volumeMounts:\n            - name: mcp-ui-node-fs\n              mountPath: /usr/share/nginx/html/assets/config\n              subPath: assets\n            - name: mcp-ui-node-fs\n              mountPath: /run/nginx/\n              subPath: run\n            - name: mcp-ui-node-fs\n              mountPath: /var/log/nginx/\n              subPath: logs\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://mcp.gateway.devstar.cloud\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"https://um.gateway.dev.wrstudio1.cloud\\\",\\\"repoApiUrl\\\":\\\"https://gitlab.dev.wrstudio1.cloud/glms\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.api.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/ui.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-ui\n  labels:\n    helm.sh/chart: mcp-ui-1.0.0\n    app.kubernetes.io/name: mcp-ui\n    app.kubernetes.io/instance: mcp-ui\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-ui\n      app.kubernetes.io/instance: mcp-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-ui\n        app.kubernetes.io/instance: mcp-ui\n    spec:\n      volumes:\n        - name: mcp-ui-node-fs\n          emptyDir: {}\n      securityContext:\n        {}\n      serviceAccountName: mcp-ui\n      containers:\n        - name: mcp-ui\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-ui:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 4200\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 8888\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8888\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 115Mi\n          volumeMounts:\n            - name: mcp-ui-node-fs\n              mountPath: /usr/share/nginx/html/assets/config\n              subPath: assets\n            - name: mcp-ui-node-fs\n              mountPath: /run/nginx/\n              subPath: run\n            - name: mcp-ui-node-fs\n              mountPath: /var/log/nginx/\n              subPath: logs\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://mcp.gateway.devstar.cloud\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"https://um.gateway.dev.wrstudio1.cloud\\\",\\\"repoApiUrl\\\":\\\"https://gitlab.dev.wrstudio1.cloud/glms\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.api.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/ui.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-ui\n  labels:\n    helm.sh/chart: mcp-ui-1.0.0\n    app.kubernetes.io/name: mcp-ui\n    app.kubernetes.io/instance: mcp-ui\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-ui\n      app.kubernetes.io/instance: mcp-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-ui\n        app.kubernetes.io/instance: mcp-ui\n    spec:\n      volumes:\n        - name: mcp-ui-node-fs\n          emptyDir: {}\n      securityContext:\n        {}\n      serviceAccountName: mcp-ui\n      containers:\n        - name: mcp-ui\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-ui:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 4200\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 8888\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8888\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 115Mi\n          volumeMounts:\n            - name: mcp-ui-node-fs\n              mountPath: /usr/share/nginx/html/assets/config\n              subPath: assets\n            - name: mcp-ui-node-fs\n              mountPath: /run/nginx/\n              subPath: run\n            - name: mcp-ui-node-fs\n              mountPath: /var/log/nginx/\n              subPath: logs\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://mcp.gateway.devstar.cloud\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"https://um.gateway.dev.wrstudio1.cloud\\\",\\\"repoApiUrl\\\":\\\"https://gitlab.dev.wrstudio1.cloud/glms\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.api.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/ui.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-ui\n  labels:\n    helm.sh/chart: mcp-ui-1.0.0\n    app.kubernetes.io/name: mcp-ui\n    app.kubernetes.io/instance: mcp-ui\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-ui\n      app.kubernetes.io/instance: mcp-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-ui\n        app.kubernetes.io/instance: mcp-ui\n    spec:\n      volumes:\n        - name: mcp-ui-node-fs\n          emptyDir: {}\n      securityContext:\n        {}\n      serviceAccountName: mcp-ui\n      containers:\n        - name: mcp-ui\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-ui:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 4200\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 8888\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8888\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 115Mi\n          volumeMounts:\n            - name: mcp-ui-node-fs\n              mountPath: /usr/share/nginx/html/assets/config\n              subPath: assets\n            - name: mcp-ui-node-fs\n              mountPath: /run/nginx/\n              subPath: run\n            - name: mcp-ui-node-fs\n              mountPath: /var/log/nginx/\n              subPath: logs\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://mcp.gateway.devstar.cloud\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"https://um.gateway.dev.wrstudio1.cloud\\\",\\\"repoApiUrl\\\":\\\"https://gitlab.dev.wrstudio1.cloud/glms\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.api.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/ui.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-ui\n  labels:\n    helm.sh/chart: mcp-ui-1.0.0\n    app.kubernetes.io/name: mcp-ui\n    app.kubernetes.io/instance: mcp-ui\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-ui\n      app.kubernetes.io/instance: mcp-ui\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-ui\n        app.kubernetes.io/instance: mcp-ui\n    spec:\n      volumes:\n        - name: mcp-ui-node-fs\n          emptyDir: {}\n      securityContext:\n        {}\n      serviceAccountName: mcp-ui\n      containers:\n        - name: mcp-ui\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: false\n          image: \"wr-studio-product/wrai-usp/mcp-ui:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 4200\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 8888\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8888\n          resources:\n            limits:\n              cpu: 100m\n              memory: 240Mi\n            requests:\n              cpu: 50m\n              memory: 115Mi\n          volumeMounts:\n            - name: mcp-ui-node-fs\n              mountPath: /usr/share/nginx/html/assets/config\n              subPath: assets\n            - name: mcp-ui-node-fs\n              mountPath: /run/nginx/\n              subPath: run\n            - name: mcp-ui-node-fs\n              mountPath: /var/log/nginx/\n              subPath: logs\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"https://mcp.gateway.devstar.cloud\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"https://um.gateway.dev.wrstudio1.cloud\\\",\\\"repoApiUrl\\\":\\\"https://gitlab.dev.wrstudio1.cloud/glms\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.api.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/core.yaml"}, "region": {"startLine": 3, "endLine": 73, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-core\n  labels:\n    helm.sh/chart: mcp-core-1.0.0\n    app.kubernetes.io/name: mcp-core\n    app.kubernetes.io/instance: mcp-core\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-core\n      app.kubernetes.io/instance: mcp-core\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-core\n        app.kubernetes.io/instance: mcp-core\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: npm-logs-mcp-core\n          emptyDir: {}\n        - name: npm-tmp-core\n          emptyDir: {}\n        - name: kubernetes-config\n          emptyDir: {}\n        - name: aws-config-core\n          emptyDir: {}\n      serviceAccountName: mcp-core\n      containers:\n        - name: mcp-core\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-core:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 480Mi\n            requests:\n              cpu: 50m\n              memory: 240Mi\n          volumeMounts:\n            - name: npm-logs-mcp-core\n              mountPath: /home/node/.npm\n            - name: kubernetes-config\n              mountPath: /home/node/.kube\n            - name: npm-tmp-core\n              mountPath: /tmp\n            - name: aws-config-core\n              mountPath: /home/node/.aws\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-core\n                  key: mcpconfigCore\n            - name: AWS_PROFILE\n              value: svc-usp-mcp\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/core.yaml"}, "region": {"startLine": 3, "endLine": 73, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-core\n  labels:\n    helm.sh/chart: mcp-core-1.0.0\n    app.kubernetes.io/name: mcp-core\n    app.kubernetes.io/instance: mcp-core\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-core\n      app.kubernetes.io/instance: mcp-core\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-core\n        app.kubernetes.io/instance: mcp-core\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: npm-logs-mcp-core\n          emptyDir: {}\n        - name: npm-tmp-core\n          emptyDir: {}\n        - name: kubernetes-config\n          emptyDir: {}\n        - name: aws-config-core\n          emptyDir: {}\n      serviceAccountName: mcp-core\n      containers:\n        - name: mcp-core\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-core:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 480Mi\n            requests:\n              cpu: 50m\n              memory: 240Mi\n          volumeMounts:\n            - name: npm-logs-mcp-core\n              mountPath: /home/node/.npm\n            - name: kubernetes-config\n              mountPath: /home/node/.kube\n            - name: npm-tmp-core\n              mountPath: /tmp\n            - name: aws-config-core\n              mountPath: /home/node/.aws\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-core\n                  key: mcpconfigCore\n            - name: AWS_PROFILE\n              value: svc-usp-mcp\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/core.yaml"}, "region": {"startLine": 3, "endLine": 73, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-core\n  labels:\n    helm.sh/chart: mcp-core-1.0.0\n    app.kubernetes.io/name: mcp-core\n    app.kubernetes.io/instance: mcp-core\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-core\n      app.kubernetes.io/instance: mcp-core\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-core\n        app.kubernetes.io/instance: mcp-core\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: npm-logs-mcp-core\n          emptyDir: {}\n        - name: npm-tmp-core\n          emptyDir: {}\n        - name: kubernetes-config\n          emptyDir: {}\n        - name: aws-config-core\n          emptyDir: {}\n      serviceAccountName: mcp-core\n      containers:\n        - name: mcp-core\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-core:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 480Mi\n            requests:\n              cpu: 50m\n              memory: 240Mi\n          volumeMounts:\n            - name: npm-logs-mcp-core\n              mountPath: /home/node/.npm\n            - name: kubernetes-config\n              mountPath: /home/node/.kube\n            - name: npm-tmp-core\n              mountPath: /tmp\n            - name: aws-config-core\n              mountPath: /home/node/.aws\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-core\n                  key: mcpconfigCore\n            - name: AWS_PROFILE\n              value: svc-usp-mcp\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/core.yaml"}, "region": {"startLine": 3, "endLine": 73, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-core\n  labels:\n    helm.sh/chart: mcp-core-1.0.0\n    app.kubernetes.io/name: mcp-core\n    app.kubernetes.io/instance: mcp-core\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-core\n      app.kubernetes.io/instance: mcp-core\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-core\n        app.kubernetes.io/instance: mcp-core\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: npm-logs-mcp-core\n          emptyDir: {}\n        - name: npm-tmp-core\n          emptyDir: {}\n        - name: kubernetes-config\n          emptyDir: {}\n        - name: aws-config-core\n          emptyDir: {}\n      serviceAccountName: mcp-core\n      containers:\n        - name: mcp-core\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-core:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 480Mi\n            requests:\n              cpu: 50m\n              memory: 240Mi\n          volumeMounts:\n            - name: npm-logs-mcp-core\n              mountPath: /home/node/.npm\n            - name: kubernetes-config\n              mountPath: /home/node/.kube\n            - name: npm-tmp-core\n              mountPath: /tmp\n            - name: aws-config-core\n              mountPath: /home/node/.aws\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-core\n                  key: mcpconfigCore\n            - name: AWS_PROFILE\n              value: svc-usp-mcp\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/core.yaml"}, "region": {"startLine": 3, "endLine": 73, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-core\n  labels:\n    helm.sh/chart: mcp-core-1.0.0\n    app.kubernetes.io/name: mcp-core\n    app.kubernetes.io/instance: mcp-core\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-core\n      app.kubernetes.io/instance: mcp-core\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-core\n        app.kubernetes.io/instance: mcp-core\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: npm-logs-mcp-core\n          emptyDir: {}\n        - name: npm-tmp-core\n          emptyDir: {}\n        - name: kubernetes-config\n          emptyDir: {}\n        - name: aws-config-core\n          emptyDir: {}\n      serviceAccountName: mcp-core\n      containers:\n        - name: mcp-core\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-core:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 480Mi\n            requests:\n              cpu: 50m\n              memory: 240Mi\n          volumeMounts:\n            - name: npm-logs-mcp-core\n              mountPath: /home/node/.npm\n            - name: kubernetes-config\n              mountPath: /home/node/.kube\n            - name: npm-tmp-core\n              mountPath: /tmp\n            - name: aws-config-core\n              mountPath: /home/node/.aws\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-core\n                  key: mcpconfigCore\n            - name: AWS_PROFILE\n              value: svc-usp-mcp\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/core.yaml"}, "region": {"startLine": 3, "endLine": 73, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-core\n  labels:\n    helm.sh/chart: mcp-core-1.0.0\n    app.kubernetes.io/name: mcp-core\n    app.kubernetes.io/instance: mcp-core\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-core\n      app.kubernetes.io/instance: mcp-core\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-core\n        app.kubernetes.io/instance: mcp-core\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: npm-logs-mcp-core\n          emptyDir: {}\n        - name: npm-tmp-core\n          emptyDir: {}\n        - name: kubernetes-config\n          emptyDir: {}\n        - name: aws-config-core\n          emptyDir: {}\n      serviceAccountName: mcp-core\n      containers:\n        - name: mcp-core\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-core:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 480Mi\n            requests:\n              cpu: 50m\n              memory: 240Mi\n          volumeMounts:\n            - name: npm-logs-mcp-core\n              mountPath: /home/node/.npm\n            - name: kubernetes-config\n              mountPath: /home/node/.kube\n            - name: npm-tmp-core\n              mountPath: /tmp\n            - name: aws-config-core\n              mountPath: /home/node/.aws\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-core\n                  key: mcpconfigCore\n            - name: AWS_PROFILE\n              value: svc-usp-mcp\n"}}}}]}, {"ruleId": "CKV_K8S_35", "ruleIndex": 6, "level": "error", "attachments": [], "message": {"text": "Prefer using secrets as files over secrets as environment variables"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/core.yaml"}, "region": {"startLine": 3, "endLine": 73, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-core\n  labels:\n    helm.sh/chart: mcp-core-1.0.0\n    app.kubernetes.io/name: mcp-core\n    app.kubernetes.io/instance: mcp-core\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-core\n      app.kubernetes.io/instance: mcp-core\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-core\n        app.kubernetes.io/instance: mcp-core\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: npm-logs-mcp-core\n          emptyDir: {}\n        - name: npm-tmp-core\n          emptyDir: {}\n        - name: kubernetes-config\n          emptyDir: {}\n        - name: aws-config-core\n          emptyDir: {}\n      serviceAccountName: mcp-core\n      containers:\n        - name: mcp-core\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-core:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 480Mi\n            requests:\n              cpu: 50m\n              memory: 240Mi\n          volumeMounts:\n            - name: npm-logs-mcp-core\n              mountPath: /home/node/.npm\n            - name: kubernetes-config\n              mountPath: /home/node/.kube\n            - name: npm-tmp-core\n              mountPath: /tmp\n            - name: aws-config-core\n              mountPath: /home/node/.aws\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-core\n                  key: mcpconfigCore\n            - name: AWS_PROFILE\n              value: svc-usp-mcp\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/core.yaml"}, "region": {"startLine": 3, "endLine": 73, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-core\n  labels:\n    helm.sh/chart: mcp-core-1.0.0\n    app.kubernetes.io/name: mcp-core\n    app.kubernetes.io/instance: mcp-core\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-core\n      app.kubernetes.io/instance: mcp-core\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-core\n        app.kubernetes.io/instance: mcp-core\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: npm-logs-mcp-core\n          emptyDir: {}\n        - name: npm-tmp-core\n          emptyDir: {}\n        - name: kubernetes-config\n          emptyDir: {}\n        - name: aws-config-core\n          emptyDir: {}\n      serviceAccountName: mcp-core\n      containers:\n        - name: mcp-core\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-core:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 480Mi\n            requests:\n              cpu: 50m\n              memory: 240Mi\n          volumeMounts:\n            - name: npm-logs-mcp-core\n              mountPath: /home/node/.npm\n            - name: kubernetes-config\n              mountPath: /home/node/.kube\n            - name: npm-tmp-core\n              mountPath: /tmp\n            - name: aws-config-core\n              mountPath: /home/node/.aws\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-core\n                  key: mcpconfigCore\n            - name: AWS_PROFILE\n              value: svc-usp-mcp\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/core.yaml"}, "region": {"startLine": 3, "endLine": 73, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-core\n  labels:\n    helm.sh/chart: mcp-core-1.0.0\n    app.kubernetes.io/name: mcp-core\n    app.kubernetes.io/instance: mcp-core\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-core\n      app.kubernetes.io/instance: mcp-core\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-core\n        app.kubernetes.io/instance: mcp-core\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: npm-logs-mcp-core\n          emptyDir: {}\n        - name: npm-tmp-core\n          emptyDir: {}\n        - name: kubernetes-config\n          emptyDir: {}\n        - name: aws-config-core\n          emptyDir: {}\n      serviceAccountName: mcp-core\n      containers:\n        - name: mcp-core\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-core:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 480Mi\n            requests:\n              cpu: 50m\n              memory: 240Mi\n          volumeMounts:\n            - name: npm-logs-mcp-core\n              mountPath: /home/node/.npm\n            - name: kubernetes-config\n              mountPath: /home/node/.kube\n            - name: npm-tmp-core\n              mountPath: /tmp\n            - name: aws-config-core\n              mountPath: /home/node/.aws\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-core\n                  key: mcpconfigCore\n            - name: AWS_PROFILE\n              value: svc-usp-mcp\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/core.yaml"}, "region": {"startLine": 3, "endLine": 73, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-core\n  labels:\n    helm.sh/chart: mcp-core-1.0.0\n    app.kubernetes.io/name: mcp-core\n    app.kubernetes.io/instance: mcp-core\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-core\n      app.kubernetes.io/instance: mcp-core\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-core\n        app.kubernetes.io/instance: mcp-core\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: npm-logs-mcp-core\n          emptyDir: {}\n        - name: npm-tmp-core\n          emptyDir: {}\n        - name: kubernetes-config\n          emptyDir: {}\n        - name: aws-config-core\n          emptyDir: {}\n      serviceAccountName: mcp-core\n      containers:\n        - name: mcp-core\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-core:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 480Mi\n            requests:\n              cpu: 50m\n              memory: 240Mi\n          volumeMounts:\n            - name: npm-logs-mcp-core\n              mountPath: /home/node/.npm\n            - name: kubernetes-config\n              mountPath: /home/node/.kube\n            - name: npm-tmp-core\n              mountPath: /tmp\n            - name: aws-config-core\n              mountPath: /home/node/.aws\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-core\n                  key: mcpconfigCore\n            - name: AWS_PROFILE\n              value: svc-usp-mcp\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/core.yaml"}, "region": {"startLine": 3, "endLine": 73, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-core\n  labels:\n    helm.sh/chart: mcp-core-1.0.0\n    app.kubernetes.io/name: mcp-core\n    app.kubernetes.io/instance: mcp-core\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-core\n      app.kubernetes.io/instance: mcp-core\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-core\n        app.kubernetes.io/instance: mcp-core\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: npm-logs-mcp-core\n          emptyDir: {}\n        - name: npm-tmp-core\n          emptyDir: {}\n        - name: kubernetes-config\n          emptyDir: {}\n        - name: aws-config-core\n          emptyDir: {}\n      serviceAccountName: mcp-core\n      containers:\n        - name: mcp-core\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-core:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 480Mi\n            requests:\n              cpu: 50m\n              memory: 240Mi\n          volumeMounts:\n            - name: npm-logs-mcp-core\n              mountPath: /home/node/.npm\n            - name: kubernetes-config\n              mountPath: /home/node/.kube\n            - name: npm-tmp-core\n              mountPath: /tmp\n            - name: aws-config-core\n              mountPath: /home/node/.aws\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-core\n                  key: mcpconfigCore\n            - name: AWS_PROFILE\n              value: svc-usp-mcp\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/core.yaml"}, "region": {"startLine": 3, "endLine": 73, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-core\n  labels:\n    helm.sh/chart: mcp-core-1.0.0\n    app.kubernetes.io/name: mcp-core\n    app.kubernetes.io/instance: mcp-core\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-core\n      app.kubernetes.io/instance: mcp-core\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-core\n        app.kubernetes.io/instance: mcp-core\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: npm-logs-mcp-core\n          emptyDir: {}\n        - name: npm-tmp-core\n          emptyDir: {}\n        - name: kubernetes-config\n          emptyDir: {}\n        - name: aws-config-core\n          emptyDir: {}\n      serviceAccountName: mcp-core\n      containers:\n        - name: mcp-core\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n          image: \"wr-studio-product/wrai-usp/mcp-core:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: 100m\n              memory: 480Mi\n            requests:\n              cpu: 50m\n              memory: 240Mi\n          volumeMounts:\n            - name: npm-logs-mcp-core\n              mountPath: /home/node/.npm\n            - name: kubernetes-config\n              mountPath: /home/node/.kube\n            - name: npm-tmp-core\n              mountPath: /tmp\n            - name: aws-config-core\n              mountPath: /home/node/.aws\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-core\n                  key: mcpconfigCore\n            - name: AWS_PROFILE\n              value: svc-usp-mcp\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/install.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-install\n  labels:\n    helm.sh/chart: mcp-install-1.0.0\n    app.kubernetes.io/name: mcp-install\n    app.kubernetes.io/instance: mcp-install\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-install\n      app.kubernetes.io/instance: mcp-install\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-install\n        app.kubernetes.io/instance: mcp-install\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-install-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-install\n      containers:\n        - name: mcp-install\n          securityContext:\n            {}\n          image: \"wr-studio-product/wrai-usp/mcp-install:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n            - name: mcp-install-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-install-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-install\n                  key: mcpconfigInstall\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/install.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-install\n  labels:\n    helm.sh/chart: mcp-install-1.0.0\n    app.kubernetes.io/name: mcp-install\n    app.kubernetes.io/instance: mcp-install\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-install\n      app.kubernetes.io/instance: mcp-install\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-install\n        app.kubernetes.io/instance: mcp-install\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-install-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-install\n      containers:\n        - name: mcp-install\n          securityContext:\n            {}\n          image: \"wr-studio-product/wrai-usp/mcp-install:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n            - name: mcp-install-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-install-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-install\n                  key: mcpconfigInstall\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/install.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-install\n  labels:\n    helm.sh/chart: mcp-install-1.0.0\n    app.kubernetes.io/name: mcp-install\n    app.kubernetes.io/instance: mcp-install\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-install\n      app.kubernetes.io/instance: mcp-install\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-install\n        app.kubernetes.io/instance: mcp-install\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-install-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-install\n      containers:\n        - name: mcp-install\n          securityContext:\n            {}\n          image: \"wr-studio-product/wrai-usp/mcp-install:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n            - name: mcp-install-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-install-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-install\n                  key: mcpconfigInstall\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/install.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-install\n  labels:\n    helm.sh/chart: mcp-install-1.0.0\n    app.kubernetes.io/name: mcp-install\n    app.kubernetes.io/instance: mcp-install\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-install\n      app.kubernetes.io/instance: mcp-install\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-install\n        app.kubernetes.io/instance: mcp-install\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-install-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-install\n      containers:\n        - name: mcp-install\n          securityContext:\n            {}\n          image: \"wr-studio-product/wrai-usp/mcp-install:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n            - name: mcp-install-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-install-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-install\n                  key: mcpconfigInstall\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/install.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-install\n  labels:\n    helm.sh/chart: mcp-install-1.0.0\n    app.kubernetes.io/name: mcp-install\n    app.kubernetes.io/instance: mcp-install\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-install\n      app.kubernetes.io/instance: mcp-install\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-install\n        app.kubernetes.io/instance: mcp-install\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-install-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-install\n      containers:\n        - name: mcp-install\n          securityContext:\n            {}\n          image: \"wr-studio-product/wrai-usp/mcp-install:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n            - name: mcp-install-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-install-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-install\n                  key: mcpconfigInstall\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/install.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-install\n  labels:\n    helm.sh/chart: mcp-install-1.0.0\n    app.kubernetes.io/name: mcp-install\n    app.kubernetes.io/instance: mcp-install\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-install\n      app.kubernetes.io/instance: mcp-install\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-install\n        app.kubernetes.io/instance: mcp-install\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-install-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-install\n      containers:\n        - name: mcp-install\n          securityContext:\n            {}\n          image: \"wr-studio-product/wrai-usp/mcp-install:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n            - name: mcp-install-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-install-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-install\n                  key: mcpconfigInstall\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/install.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-install\n  labels:\n    helm.sh/chart: mcp-install-1.0.0\n    app.kubernetes.io/name: mcp-install\n    app.kubernetes.io/instance: mcp-install\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-install\n      app.kubernetes.io/instance: mcp-install\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-install\n        app.kubernetes.io/instance: mcp-install\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-install-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-install\n      containers:\n        - name: mcp-install\n          securityContext:\n            {}\n          image: \"wr-studio-product/wrai-usp/mcp-install:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n            - name: mcp-install-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-install-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-install\n                  key: mcpconfigInstall\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/install.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-install\n  labels:\n    helm.sh/chart: mcp-install-1.0.0\n    app.kubernetes.io/name: mcp-install\n    app.kubernetes.io/instance: mcp-install\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-install\n      app.kubernetes.io/instance: mcp-install\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-install\n        app.kubernetes.io/instance: mcp-install\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-install-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-install\n      containers:\n        - name: mcp-install\n          securityContext:\n            {}\n          image: \"wr-studio-product/wrai-usp/mcp-install:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n            - name: mcp-install-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-install-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-install\n                  key: mcpconfigInstall\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/install.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-install\n  labels:\n    helm.sh/chart: mcp-install-1.0.0\n    app.kubernetes.io/name: mcp-install\n    app.kubernetes.io/instance: mcp-install\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-install\n      app.kubernetes.io/instance: mcp-install\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-install\n        app.kubernetes.io/instance: mcp-install\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-install-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-install\n      containers:\n        - name: mcp-install\n          securityContext:\n            {}\n          image: \"wr-studio-product/wrai-usp/mcp-install:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n            - name: mcp-install-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-install-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-install\n                  key: mcpconfigInstall\n"}}}}]}, {"ruleId": "CKV_K8S_35", "ruleIndex": 6, "level": "error", "attachments": [], "message": {"text": "Prefer using secrets as files over secrets as environment variables"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/install.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-install\n  labels:\n    helm.sh/chart: mcp-install-1.0.0\n    app.kubernetes.io/name: mcp-install\n    app.kubernetes.io/instance: mcp-install\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-install\n      app.kubernetes.io/instance: mcp-install\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-install\n        app.kubernetes.io/instance: mcp-install\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-install-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-install\n      containers:\n        - name: mcp-install\n          securityContext:\n            {}\n          image: \"wr-studio-product/wrai-usp/mcp-install:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n            - name: mcp-install-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-install-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-install\n                  key: mcpconfigInstall\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/install.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-install\n  labels:\n    helm.sh/chart: mcp-install-1.0.0\n    app.kubernetes.io/name: mcp-install\n    app.kubernetes.io/instance: mcp-install\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-install\n      app.kubernetes.io/instance: mcp-install\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-install\n        app.kubernetes.io/instance: mcp-install\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-install-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-install\n      containers:\n        - name: mcp-install\n          securityContext:\n            {}\n          image: \"wr-studio-product/wrai-usp/mcp-install:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n            - name: mcp-install-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-install-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-install\n                  key: mcpconfigInstall\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/install.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-install\n  labels:\n    helm.sh/chart: mcp-install-1.0.0\n    app.kubernetes.io/name: mcp-install\n    app.kubernetes.io/instance: mcp-install\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-install\n      app.kubernetes.io/instance: mcp-install\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-install\n        app.kubernetes.io/instance: mcp-install\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-install-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-install\n      containers:\n        - name: mcp-install\n          securityContext:\n            {}\n          image: \"wr-studio-product/wrai-usp/mcp-install:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n            - name: mcp-install-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-install-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-install\n                  key: mcpconfigInstall\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/install.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-install\n  labels:\n    helm.sh/chart: mcp-install-1.0.0\n    app.kubernetes.io/name: mcp-install\n    app.kubernetes.io/instance: mcp-install\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-install\n      app.kubernetes.io/instance: mcp-install\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-install\n        app.kubernetes.io/instance: mcp-install\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-install-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-install\n      containers:\n        - name: mcp-install\n          securityContext:\n            {}\n          image: \"wr-studio-product/wrai-usp/mcp-install:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n            - name: mcp-install-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-install-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-install\n                  key: mcpconfigInstall\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/install.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-install\n  labels:\n    helm.sh/chart: mcp-install-1.0.0\n    app.kubernetes.io/name: mcp-install\n    app.kubernetes.io/instance: mcp-install\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-install\n      app.kubernetes.io/instance: mcp-install\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-install\n        app.kubernetes.io/instance: mcp-install\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-install-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-install\n      containers:\n        - name: mcp-install\n          securityContext:\n            {}\n          image: \"wr-studio-product/wrai-usp/mcp-install:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n            - name: mcp-install-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-install-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-install\n                  key: mcpconfigInstall\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/install.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-install\n  labels:\n    helm.sh/chart: mcp-install-1.0.0\n    app.kubernetes.io/name: mcp-install\n    app.kubernetes.io/instance: mcp-install\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-install\n      app.kubernetes.io/instance: mcp-install\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-install\n        app.kubernetes.io/instance: mcp-install\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-install-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-install\n      containers:\n        - name: mcp-install\n          securityContext:\n            {}\n          image: \"wr-studio-product/wrai-usp/mcp-install:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n            - name: mcp-install-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-install-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-install\n                  key: mcpconfigInstall\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/deployment/install.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mcp-install\n  labels:\n    helm.sh/chart: mcp-install-1.0.0\n    app.kubernetes.io/name: mcp-install\n    app.kubernetes.io/instance: mcp-install\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mcp-install\n      app.kubernetes.io/instance: mcp-install\n  template:\n    metadata:\n      annotations:\n      labels:\n        app.kubernetes.io/name: mcp-install\n        app.kubernetes.io/instance: mcp-install\n    spec:\n      securityContext:\n        fsGroup: 100\n      volumes:\n        - name: mcp-install-node-fs\n          emptyDir: {}\n      serviceAccountName: mcp-install\n      containers:\n        - name: mcp-install\n          securityContext:\n            {}\n          image: \"wr-studio-product/wrai-usp/mcp-install:0.12.0-22.09\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          resources:\n            {}\n          volumeMounts:\n            - name: mcp-install-node-fs\n              mountPath: /home/node/.npm\n              subPath: logs\n            - name: mcp-install-node-fs\n              mountPath: /tmp\n              subPath: tmp\n          command: [\"npm\",\"run\", \"start\"]\n          env:\n            - name: USP_ENV_CONFIG\n              valueFrom:\n                secretKeyRef:\n                  name: mcp-config-secret-install\n                  key: mcpconfigInstall\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/hooks/pre-delete.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mcp-pre-delete-hook-pod\n  annotations:\n    sidecar.istio.io/inject: \"false\"\n    \"helm.sh/hook\": pre-delete\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\nspec:\n  containers:\n    - name: mcp-pre-delete-hook\n      securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n      image: \"wr-studio-product/wrai-usp/curl-hook-call:0.12.0-22.09\"\n      imagePullPolicy: \"Always\"\n      command: [\"/bin/sh\", \"-c\"]\n      args:\n      - ./hook.sh -t=PUT -d=\"\" -u=$URL -r=$RETRY -m=$RETRY_MAX_TIME -x=$MAX_TIME;\n      env:\n        - name: URL\n          value: \"mcp-gateway.prod-wrai-usp-mcp.svc.cluster.local/mcp/regions/us-east-2/preDelete\"\n        - name: REQUEST_TYPE\n          value: \"PUT\"\n        - name: RETRY_MAX_TIME\n          value: \"1\"\n        - name: MAX_TIME\n          value: \"20\"\n      resources:\n            limits:\n              cpu: 100m\n              memory: 200Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n  restartPolicy: Never\n  terminationGracePeriodSeconds: 0\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/hooks/pre-delete.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mcp-pre-delete-hook-pod\n  annotations:\n    sidecar.istio.io/inject: \"false\"\n    \"helm.sh/hook\": pre-delete\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\nspec:\n  containers:\n    - name: mcp-pre-delete-hook\n      securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n      image: \"wr-studio-product/wrai-usp/curl-hook-call:0.12.0-22.09\"\n      imagePullPolicy: \"Always\"\n      command: [\"/bin/sh\", \"-c\"]\n      args:\n      - ./hook.sh -t=PUT -d=\"\" -u=$URL -r=$RETRY -m=$RETRY_MAX_TIME -x=$MAX_TIME;\n      env:\n        - name: URL\n          value: \"mcp-gateway.prod-wrai-usp-mcp.svc.cluster.local/mcp/regions/us-east-2/preDelete\"\n        - name: REQUEST_TYPE\n          value: \"PUT\"\n        - name: RETRY_MAX_TIME\n          value: \"1\"\n        - name: MAX_TIME\n          value: \"20\"\n      resources:\n            limits:\n              cpu: 100m\n              memory: 200Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n  restartPolicy: Never\n  terminationGracePeriodSeconds: 0\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/hooks/pre-delete.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mcp-pre-delete-hook-pod\n  annotations:\n    sidecar.istio.io/inject: \"false\"\n    \"helm.sh/hook\": pre-delete\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\nspec:\n  containers:\n    - name: mcp-pre-delete-hook\n      securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n      image: \"wr-studio-product/wrai-usp/curl-hook-call:0.12.0-22.09\"\n      imagePullPolicy: \"Always\"\n      command: [\"/bin/sh\", \"-c\"]\n      args:\n      - ./hook.sh -t=PUT -d=\"\" -u=$URL -r=$RETRY -m=$RETRY_MAX_TIME -x=$MAX_TIME;\n      env:\n        - name: URL\n          value: \"mcp-gateway.prod-wrai-usp-mcp.svc.cluster.local/mcp/regions/us-east-2/preDelete\"\n        - name: REQUEST_TYPE\n          value: \"PUT\"\n        - name: RETRY_MAX_TIME\n          value: \"1\"\n        - name: MAX_TIME\n          value: \"20\"\n      resources:\n            limits:\n              cpu: 100m\n              memory: 200Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n  restartPolicy: Never\n  terminationGracePeriodSeconds: 0\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/hooks/pre-delete.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mcp-pre-delete-hook-pod\n  annotations:\n    sidecar.istio.io/inject: \"false\"\n    \"helm.sh/hook\": pre-delete\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\nspec:\n  containers:\n    - name: mcp-pre-delete-hook\n      securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n      image: \"wr-studio-product/wrai-usp/curl-hook-call:0.12.0-22.09\"\n      imagePullPolicy: \"Always\"\n      command: [\"/bin/sh\", \"-c\"]\n      args:\n      - ./hook.sh -t=PUT -d=\"\" -u=$URL -r=$RETRY -m=$RETRY_MAX_TIME -x=$MAX_TIME;\n      env:\n        - name: URL\n          value: \"mcp-gateway.prod-wrai-usp-mcp.svc.cluster.local/mcp/regions/us-east-2/preDelete\"\n        - name: REQUEST_TYPE\n          value: \"PUT\"\n        - name: RETRY_MAX_TIME\n          value: \"1\"\n        - name: MAX_TIME\n          value: \"20\"\n      resources:\n            limits:\n              cpu: 100m\n              memory: 200Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n  restartPolicy: Never\n  terminationGracePeriodSeconds: 0\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/hooks/pre-delete.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mcp-pre-delete-hook-pod\n  annotations:\n    sidecar.istio.io/inject: \"false\"\n    \"helm.sh/hook\": pre-delete\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\nspec:\n  containers:\n    - name: mcp-pre-delete-hook\n      securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n      image: \"wr-studio-product/wrai-usp/curl-hook-call:0.12.0-22.09\"\n      imagePullPolicy: \"Always\"\n      command: [\"/bin/sh\", \"-c\"]\n      args:\n      - ./hook.sh -t=PUT -d=\"\" -u=$URL -r=$RETRY -m=$RETRY_MAX_TIME -x=$MAX_TIME;\n      env:\n        - name: URL\n          value: \"mcp-gateway.prod-wrai-usp-mcp.svc.cluster.local/mcp/regions/us-east-2/preDelete\"\n        - name: REQUEST_TYPE\n          value: \"PUT\"\n        - name: RETRY_MAX_TIME\n          value: \"1\"\n        - name: MAX_TIME\n          value: \"20\"\n      resources:\n            limits:\n              cpu: 100m\n              memory: 200Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n  restartPolicy: Never\n  terminationGracePeriodSeconds: 0\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/hooks/pre-delete.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mcp-pre-delete-hook-pod\n  annotations:\n    sidecar.istio.io/inject: \"false\"\n    \"helm.sh/hook\": pre-delete\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\nspec:\n  containers:\n    - name: mcp-pre-delete-hook\n      securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n      image: \"wr-studio-product/wrai-usp/curl-hook-call:0.12.0-22.09\"\n      imagePullPolicy: \"Always\"\n      command: [\"/bin/sh\", \"-c\"]\n      args:\n      - ./hook.sh -t=PUT -d=\"\" -u=$URL -r=$RETRY -m=$RETRY_MAX_TIME -x=$MAX_TIME;\n      env:\n        - name: URL\n          value: \"mcp-gateway.prod-wrai-usp-mcp.svc.cluster.local/mcp/regions/us-east-2/preDelete\"\n        - name: REQUEST_TYPE\n          value: \"PUT\"\n        - name: RETRY_MAX_TIME\n          value: \"1\"\n        - name: MAX_TIME\n          value: \"20\"\n      resources:\n            limits:\n              cpu: 100m\n              memory: 200Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n  restartPolicy: Never\n  terminationGracePeriodSeconds: 0\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/hooks/pre-delete.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mcp-pre-delete-hook-pod\n  annotations:\n    sidecar.istio.io/inject: \"false\"\n    \"helm.sh/hook\": pre-delete\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\nspec:\n  containers:\n    - name: mcp-pre-delete-hook\n      securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n      image: \"wr-studio-product/wrai-usp/curl-hook-call:0.12.0-22.09\"\n      imagePullPolicy: \"Always\"\n      command: [\"/bin/sh\", \"-c\"]\n      args:\n      - ./hook.sh -t=PUT -d=\"\" -u=$URL -r=$RETRY -m=$RETRY_MAX_TIME -x=$MAX_TIME;\n      env:\n        - name: URL\n          value: \"mcp-gateway.prod-wrai-usp-mcp.svc.cluster.local/mcp/regions/us-east-2/preDelete\"\n        - name: REQUEST_TYPE\n          value: \"PUT\"\n        - name: RETRY_MAX_TIME\n          value: \"1\"\n        - name: MAX_TIME\n          value: \"20\"\n      resources:\n            limits:\n              cpu: 100m\n              memory: 200Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n  restartPolicy: Never\n  terminationGracePeriodSeconds: 0\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/hooks/pre-delete.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mcp-pre-delete-hook-pod\n  annotations:\n    sidecar.istio.io/inject: \"false\"\n    \"helm.sh/hook\": pre-delete\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\nspec:\n  containers:\n    - name: mcp-pre-delete-hook\n      securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n      image: \"wr-studio-product/wrai-usp/curl-hook-call:0.12.0-22.09\"\n      imagePullPolicy: \"Always\"\n      command: [\"/bin/sh\", \"-c\"]\n      args:\n      - ./hook.sh -t=PUT -d=\"\" -u=$URL -r=$RETRY -m=$RETRY_MAX_TIME -x=$MAX_TIME;\n      env:\n        - name: URL\n          value: \"mcp-gateway.prod-wrai-usp-mcp.svc.cluster.local/mcp/regions/us-east-2/preDelete\"\n        - name: REQUEST_TYPE\n          value: \"PUT\"\n        - name: RETRY_MAX_TIME\n          value: \"1\"\n        - name: MAX_TIME\n          value: \"20\"\n      resources:\n            limits:\n              cpu: 100m\n              memory: 200Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n  restartPolicy: Never\n  terminationGracePeriodSeconds: 0\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/hooks/pre-delete.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mcp-pre-delete-hook-pod\n  annotations:\n    sidecar.istio.io/inject: \"false\"\n    \"helm.sh/hook\": pre-delete\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\nspec:\n  containers:\n    - name: mcp-pre-delete-hook\n      securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n      image: \"wr-studio-product/wrai-usp/curl-hook-call:0.12.0-22.09\"\n      imagePullPolicy: \"Always\"\n      command: [\"/bin/sh\", \"-c\"]\n      args:\n      - ./hook.sh -t=PUT -d=\"\" -u=$URL -r=$RETRY -m=$RETRY_MAX_TIME -x=$MAX_TIME;\n      env:\n        - name: URL\n          value: \"mcp-gateway.prod-wrai-usp-mcp.svc.cluster.local/mcp/regions/us-east-2/preDelete\"\n        - name: REQUEST_TYPE\n          value: \"PUT\"\n        - name: RETRY_MAX_TIME\n          value: \"1\"\n        - name: MAX_TIME\n          value: \"20\"\n      resources:\n            limits:\n              cpu: 100m\n              memory: 200Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n  restartPolicy: Never\n  terminationGracePeriodSeconds: 0\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/hooks/pre-delete.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mcp-pre-delete-hook-pod\n  annotations:\n    sidecar.istio.io/inject: \"false\"\n    \"helm.sh/hook\": pre-delete\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\nspec:\n  containers:\n    - name: mcp-pre-delete-hook\n      securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n      image: \"wr-studio-product/wrai-usp/curl-hook-call:0.12.0-22.09\"\n      imagePullPolicy: \"Always\"\n      command: [\"/bin/sh\", \"-c\"]\n      args:\n      - ./hook.sh -t=PUT -d=\"\" -u=$URL -r=$RETRY -m=$RETRY_MAX_TIME -x=$MAX_TIME;\n      env:\n        - name: URL\n          value: \"mcp-gateway.prod-wrai-usp-mcp.svc.cluster.local/mcp/regions/us-east-2/preDelete\"\n        - name: REQUEST_TYPE\n          value: \"PUT\"\n        - name: RETRY_MAX_TIME\n          value: \"1\"\n        - name: MAX_TIME\n          value: \"20\"\n      resources:\n            limits:\n              cpu: 100m\n              memory: 200Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n  restartPolicy: Never\n  terminationGracePeriodSeconds: 0\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/hooks/pre-delete.yaml"}, "region": {"startLine": 3, "endLine": 39, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: mcp-pre-delete-hook-pod\n  annotations:\n    sidecar.istio.io/inject: \"false\"\n    \"helm.sh/hook\": pre-delete\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\nspec:\n  containers:\n    - name: mcp-pre-delete-hook\n      securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n      image: \"wr-studio-product/wrai-usp/curl-hook-call:0.12.0-22.09\"\n      imagePullPolicy: \"Always\"\n      command: [\"/bin/sh\", \"-c\"]\n      args:\n      - ./hook.sh -t=PUT -d=\"\" -u=$URL -r=$RETRY -m=$RETRY_MAX_TIME -x=$MAX_TIME;\n      env:\n        - name: URL\n          value: \"mcp-gateway.prod-wrai-usp-mcp.svc.cluster.local/mcp/regions/us-east-2/preDelete\"\n        - name: REQUEST_TYPE\n          value: \"PUT\"\n        - name: RETRY_MAX_TIME\n          value: \"1\"\n        - name: MAX_TIME\n          value: \"20\"\n      resources:\n            limits:\n              cpu: 100m\n              memory: 200Mi\n            requests:\n              cpu: 50m\n              memory: 100Mi\n  restartPolicy: Never\n  terminationGracePeriodSeconds: 0\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/job/init-db.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: mcp-init-db\n  namespace: default\n  labels:\n    app: mcp-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-weight\": \"3\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation\n    \"sidecar.istio.io/inject\": \"false\"\nspec:\n  template:\n    metadata:\n      name: mcp-init-db\n      namespace: default\n      labels:\n        app: mcp-init-db\n      annotations:\n    spec:\n      restartPolicy: OnFailure\n      automountServiceAccountToken: true\n      serviceAccountName: mcp-init\n      containers:\n        - name: mcp-init-db\n          image: \"wr-studio-product/wrai-usp/mcp-init-db:tag\"\n          workingDir: /home/node/app/db/\n          command:\n            - node\n          args:\n            - init.js\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n            runAsGroup: 100\n            runAsUser: 1000\n          resources:\n            limits:\n              cpu: 100m\n              memory: 320Mi\n            requests:\n              cpu: 50m\n              memory: 160Mi\n          imagePullPolicy: Always\n          env:\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"localhost\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"user\"\n            - name: DB_PASSWORD\n              value: \"password\"\n            - name: DB_DEFAULT_TENANT\n              value: \"wrstudio\"\n            - name: ROOT_DNS\n              value: \n            - name: ATTEMPTS\n              value: \"15\"\n            - name: DELAY_RETRY\n              value: \"30000\"\n            - name: TOZNY_ACCOUNT_USERNAME\n              value: \"tozny-admin@windriver.com\"\n            - name: HIVE_ADMIN_ACCOUNT_USERNAME\n              value:\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/job/init-db.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: mcp-init-db\n  namespace: default\n  labels:\n    app: mcp-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-weight\": \"3\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation\n    \"sidecar.istio.io/inject\": \"false\"\nspec:\n  template:\n    metadata:\n      name: mcp-init-db\n      namespace: default\n      labels:\n        app: mcp-init-db\n      annotations:\n    spec:\n      restartPolicy: OnFailure\n      automountServiceAccountToken: true\n      serviceAccountName: mcp-init\n      containers:\n        - name: mcp-init-db\n          image: \"wr-studio-product/wrai-usp/mcp-init-db:tag\"\n          workingDir: /home/node/app/db/\n          command:\n            - node\n          args:\n            - init.js\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n            runAsGroup: 100\n            runAsUser: 1000\n          resources:\n            limits:\n              cpu: 100m\n              memory: 320Mi\n            requests:\n              cpu: 50m\n              memory: 160Mi\n          imagePullPolicy: Always\n          env:\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"localhost\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"user\"\n            - name: DB_PASSWORD\n              value: \"password\"\n            - name: DB_DEFAULT_TENANT\n              value: \"wrstudio\"\n            - name: ROOT_DNS\n              value: \n            - name: ATTEMPTS\n              value: \"15\"\n            - name: DELAY_RETRY\n              value: \"30000\"\n            - name: TOZNY_ACCOUNT_USERNAME\n              value: \"tozny-admin@windriver.com\"\n            - name: HIVE_ADMIN_ACCOUNT_USERNAME\n              value:\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/job/init-db.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: mcp-init-db\n  namespace: default\n  labels:\n    app: mcp-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-weight\": \"3\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation\n    \"sidecar.istio.io/inject\": \"false\"\nspec:\n  template:\n    metadata:\n      name: mcp-init-db\n      namespace: default\n      labels:\n        app: mcp-init-db\n      annotations:\n    spec:\n      restartPolicy: OnFailure\n      automountServiceAccountToken: true\n      serviceAccountName: mcp-init\n      containers:\n        - name: mcp-init-db\n          image: \"wr-studio-product/wrai-usp/mcp-init-db:tag\"\n          workingDir: /home/node/app/db/\n          command:\n            - node\n          args:\n            - init.js\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n            runAsGroup: 100\n            runAsUser: 1000\n          resources:\n            limits:\n              cpu: 100m\n              memory: 320Mi\n            requests:\n              cpu: 50m\n              memory: 160Mi\n          imagePullPolicy: Always\n          env:\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"localhost\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"user\"\n            - name: DB_PASSWORD\n              value: \"password\"\n            - name: DB_DEFAULT_TENANT\n              value: \"wrstudio\"\n            - name: ROOT_DNS\n              value: \n            - name: ATTEMPTS\n              value: \"15\"\n            - name: DELAY_RETRY\n              value: \"30000\"\n            - name: TOZNY_ACCOUNT_USERNAME\n              value: \"tozny-admin@windriver.com\"\n            - name: HIVE_ADMIN_ACCOUNT_USERNAME\n              value:\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/job/init-db.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: mcp-init-db\n  namespace: default\n  labels:\n    app: mcp-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-weight\": \"3\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation\n    \"sidecar.istio.io/inject\": \"false\"\nspec:\n  template:\n    metadata:\n      name: mcp-init-db\n      namespace: default\n      labels:\n        app: mcp-init-db\n      annotations:\n    spec:\n      restartPolicy: OnFailure\n      automountServiceAccountToken: true\n      serviceAccountName: mcp-init\n      containers:\n        - name: mcp-init-db\n          image: \"wr-studio-product/wrai-usp/mcp-init-db:tag\"\n          workingDir: /home/node/app/db/\n          command:\n            - node\n          args:\n            - init.js\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n            runAsGroup: 100\n            runAsUser: 1000\n          resources:\n            limits:\n              cpu: 100m\n              memory: 320Mi\n            requests:\n              cpu: 50m\n              memory: 160Mi\n          imagePullPolicy: Always\n          env:\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"localhost\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"user\"\n            - name: DB_PASSWORD\n              value: \"password\"\n            - name: DB_DEFAULT_TENANT\n              value: \"wrstudio\"\n            - name: ROOT_DNS\n              value: \n            - name: ATTEMPTS\n              value: \"15\"\n            - name: DELAY_RETRY\n              value: \"30000\"\n            - name: TOZNY_ACCOUNT_USERNAME\n              value: \"tozny-admin@windriver.com\"\n            - name: HIVE_ADMIN_ACCOUNT_USERNAME\n              value:\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/job/init-db.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: mcp-init-db\n  namespace: default\n  labels:\n    app: mcp-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-weight\": \"3\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation\n    \"sidecar.istio.io/inject\": \"false\"\nspec:\n  template:\n    metadata:\n      name: mcp-init-db\n      namespace: default\n      labels:\n        app: mcp-init-db\n      annotations:\n    spec:\n      restartPolicy: OnFailure\n      automountServiceAccountToken: true\n      serviceAccountName: mcp-init\n      containers:\n        - name: mcp-init-db\n          image: \"wr-studio-product/wrai-usp/mcp-init-db:tag\"\n          workingDir: /home/node/app/db/\n          command:\n            - node\n          args:\n            - init.js\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n            runAsGroup: 100\n            runAsUser: 1000\n          resources:\n            limits:\n              cpu: 100m\n              memory: 320Mi\n            requests:\n              cpu: 50m\n              memory: 160Mi\n          imagePullPolicy: Always\n          env:\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"localhost\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"user\"\n            - name: DB_PASSWORD\n              value: \"password\"\n            - name: DB_DEFAULT_TENANT\n              value: \"wrstudio\"\n            - name: ROOT_DNS\n              value: \n            - name: ATTEMPTS\n              value: \"15\"\n            - name: DELAY_RETRY\n              value: \"30000\"\n            - name: TOZNY_ACCOUNT_USERNAME\n              value: \"tozny-admin@windriver.com\"\n            - name: HIVE_ADMIN_ACCOUNT_USERNAME\n              value:\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/job/init-db.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: mcp-init-db\n  namespace: default\n  labels:\n    app: mcp-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-weight\": \"3\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation\n    \"sidecar.istio.io/inject\": \"false\"\nspec:\n  template:\n    metadata:\n      name: mcp-init-db\n      namespace: default\n      labels:\n        app: mcp-init-db\n      annotations:\n    spec:\n      restartPolicy: OnFailure\n      automountServiceAccountToken: true\n      serviceAccountName: mcp-init\n      containers:\n        - name: mcp-init-db\n          image: \"wr-studio-product/wrai-usp/mcp-init-db:tag\"\n          workingDir: /home/node/app/db/\n          command:\n            - node\n          args:\n            - init.js\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n            runAsGroup: 100\n            runAsUser: 1000\n          resources:\n            limits:\n              cpu: 100m\n              memory: 320Mi\n            requests:\n              cpu: 50m\n              memory: 160Mi\n          imagePullPolicy: Always\n          env:\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"localhost\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"user\"\n            - name: DB_PASSWORD\n              value: \"password\"\n            - name: DB_DEFAULT_TENANT\n              value: \"wrstudio\"\n            - name: ROOT_DNS\n              value: \n            - name: ATTEMPTS\n              value: \"15\"\n            - name: DELAY_RETRY\n              value: \"30000\"\n            - name: TOZNY_ACCOUNT_USERNAME\n              value: \"tozny-admin@windriver.com\"\n            - name: HIVE_ADMIN_ACCOUNT_USERNAME\n              value:\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/job/init-db.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: mcp-init-db\n  namespace: default\n  labels:\n    app: mcp-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-weight\": \"3\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation\n    \"sidecar.istio.io/inject\": \"false\"\nspec:\n  template:\n    metadata:\n      name: mcp-init-db\n      namespace: default\n      labels:\n        app: mcp-init-db\n      annotations:\n    spec:\n      restartPolicy: OnFailure\n      automountServiceAccountToken: true\n      serviceAccountName: mcp-init\n      containers:\n        - name: mcp-init-db\n          image: \"wr-studio-product/wrai-usp/mcp-init-db:tag\"\n          workingDir: /home/node/app/db/\n          command:\n            - node\n          args:\n            - init.js\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n            runAsGroup: 100\n            runAsUser: 1000\n          resources:\n            limits:\n              cpu: 100m\n              memory: 320Mi\n            requests:\n              cpu: 50m\n              memory: 160Mi\n          imagePullPolicy: Always\n          env:\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"localhost\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"user\"\n            - name: DB_PASSWORD\n              value: \"password\"\n            - name: DB_DEFAULT_TENANT\n              value: \"wrstudio\"\n            - name: ROOT_DNS\n              value: \n            - name: ATTEMPTS\n              value: \"15\"\n            - name: DELAY_RETRY\n              value: \"30000\"\n            - name: TOZNY_ACCOUNT_USERNAME\n              value: \"tozny-admin@windriver.com\"\n            - name: HIVE_ADMIN_ACCOUNT_USERNAME\n              value:\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/job/init-db.yaml"}, "region": {"startLine": 3, "endLine": 70, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: mcp-init-db\n  namespace: default\n  labels:\n    app: mcp-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-weight\": \"3\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation\n    \"sidecar.istio.io/inject\": \"false\"\nspec:\n  template:\n    metadata:\n      name: mcp-init-db\n      namespace: default\n      labels:\n        app: mcp-init-db\n      annotations:\n    spec:\n      restartPolicy: OnFailure\n      automountServiceAccountToken: true\n      serviceAccountName: mcp-init\n      containers:\n        - name: mcp-init-db\n          image: \"wr-studio-product/wrai-usp/mcp-init-db:tag\"\n          workingDir: /home/node/app/db/\n          command:\n            - node\n          args:\n            - init.js\n          securityContext:\n            allowPrivilegeEscalation: false\n            readOnlyRootFilesystem: true\n            runAsGroup: 100\n            runAsUser: 1000\n          resources:\n            limits:\n              cpu: 100m\n              memory: 320Mi\n            requests:\n              cpu: 50m\n              memory: 160Mi\n          imagePullPolicy: Always\n          env:\n            - name: DB_NAME\n              value: \"usp\"\n            - name: DB_HOST\n              value: \"localhost\"\n            - name: DB_PORT\n              value: \"5432\"\n            - name: DB_USER\n              value: \"user\"\n            - name: DB_PASSWORD\n              value: \"password\"\n            - name: DB_DEFAULT_TENANT\n              value: \"wrstudio\"\n            - name: ROOT_DNS\n              value: \n            - name: ATTEMPTS\n              value: \"15\"\n            - name: DELAY_RETRY\n              value: \"30000\"\n            - name: TOZNY_ACCOUNT_USERNAME\n              value: \"tozny-admin@windriver.com\"\n            - name: HIVE_ADMIN_ACCOUNT_USERNAME\n              value:\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/secret/secret.yaml"}, "region": {"startLine": 3, "endLine": 10, "snippet": {"text": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mcp-config-secret\ntype: Opaque\ndata:\n  mcpconfig: eyJjb21tb24iOnsiZGVmYXVsdE1heEF1dG9TY2FsaW5nUmVzb3VyY2VOb2RlcyI6MTAsImRlZmF1bHRNaW5BdXRvU2NhbGluZ1Jlc291cmNlTm9kZXMiOjAsImluaXRpYWxpemUiOnRydWUsInNjYWxpbmdBY3Rpdml0eUF0dGVtcHRzIjo1LCJzY2FsaW5nQWN0aXZpdHlEZWxheU1pbGxpc2Vjb25kcyI6MTIwMDAwLCJzdWRvRGV2TW9kZSI6ZmFsc2UsInRhZ1RvQmFja3VwIjoiYmFzZSIsInRlc3RNb2RlRGJPbmx5IjpmYWxzZX0sImNvbmR1Y3RvciI6eyJob3N0IjoiSE9TVCIsInBhc3MiOiJQQVNTIiwidGVuYW50IjoiVEVOQU5UIiwidXNlciI6IlVTRVIifSwia3ViZXJuZXRlcyI6eyJhdXRvc2NhbGluZ0tleSI6Im1jcC1hdXRvc2NhbGluZyIsImF1dG9zY2FsaW5nVmFsdWUiOiJ0b2JlZGVsZXRlZCIsImNsaWVudFZlcnNpb24iOiIxLjEzIiwiZWZmZWN0IjoiTm9TY2hlZHVsZSIsImluc3RhbGxSZWdpb25Bcmd1bWVudCI6ImFwcGx5IiwibWluc1RvV2FpdEJlZm9yZVJlbW92ZU5vZGUiOjEsInJlZ2lvbkluc3RhbGxlck5hbWVzcGFjZSI6InByb2Qtd3JhaS11c3AtbWNwLWluc3RhbGxlciIsInRpbWVzdGFtcEtleSI6Im1jcC10YWludGVkZGF0ZSIsInVuaW5zdGFsbFJlZ2lvbkFyZ3VtZW50IjoiZGVzdHJveSJ9LCJyZWRpcyI6eyJob3N0IjoiMTI3LjAuMC4xIiwicG9ydCI6NjM3OX0sInJlZ2V4Ijp7InZhbHVlIjoid3JhaSJ9LCJ0aGlyZFBhcnR5VG9vbHMiOnsiZGVsaXZlcnNBcnRpZmFjdHNHYXRld2F5IjoiaHR0cHM6Ly9nYXRld2F5LmRlbGl2ZXJzLndpbmRyaXZlci5jb20vdjEvdG9vbHMvbWluaW8vYXJ0aWZhY3RzIiwiaW5zdGFsbGF0aW9uTmFtZXNwYWNlIjoicHJvZC13cmFpLXVzcC1tY3AiLCJpbnN0YWxsZXJEb2NrZXJJbWFnZSI6IndyLXN0dWRpby1wcm9kdWN0LzNwdG9vbHMvM3B0b29scy1zaGFyZWQvY29udGVudC1pbnN0YWxsZXI6MS4wLjAtMjIuMDYifX0=\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/secret/secret.yaml"}, "region": {"startLine": 12, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mcp-config-secret-conductor-cronjob\ntype: Opaque\ndata:\n  config: eyJjb21tb24iOnsiZGVmYXVsdE1heEF1dG9TY2FsaW5nUmVzb3VyY2VOb2RlcyI6MTAsImRlZmF1bHRNaW5BdXRvU2NhbGluZ1Jlc291cmNlTm9kZXMiOjAsImluaXRpYWxpemUiOnRydWUsInNjYWxpbmdBY3Rpdml0eUF0dGVtcHRzIjo1LCJzY2FsaW5nQWN0aXZpdHlEZWxheU1pbGxpc2Vjb25kcyI6MTIwMDAwLCJzdWRvRGV2TW9kZSI6ZmFsc2UsInRhZ1RvQmFja3VwIjoiYmFzZSIsInRlc3RNb2RlRGJPbmx5IjpmYWxzZX0sImNvbmR1Y3RvciI6eyJob3N0IjoiSE9TVCIsInBhc3MiOiJQQVNTIiwidGVuYW50IjoiVEVOQU5UIiwidXNlciI6IlVTRVIifSwicmVkaXMiOnsiaG9zdCI6IjEyNy4wLjAuMSIsInBvcnQiOjYzNzl9fQ==\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/secret/secret.yaml"}, "region": {"startLine": 21, "endLine": 28, "snippet": {"text": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mcp-config-secret-core\ntype: Opaque\ndata:\n  mcpconfigCore: eyJjb21tb24iOnsiZGVmYXVsdE1heEF1dG9TY2FsaW5nUmVzb3VyY2VOb2RlcyI6MTAsImRlZmF1bHRNaW5BdXRvU2NhbGluZ1Jlc291cmNlTm9kZXMiOjAsImluaXRpYWxpemUiOnRydWUsInNjYWxpbmdBY3Rpdml0eUF0dGVtcHRzIjo1LCJzY2FsaW5nQWN0aXZpdHlEZWxheU1pbGxpc2Vjb25kcyI6MTIwMDAwLCJzdWRvRGV2TW9kZSI6ZmFsc2UsInRhZ1RvQmFja3VwIjoiYmFzZSIsInRlc3RNb2RlRGJPbmx5IjpmYWxzZX0sImt1YmVybmV0ZXMiOnsiYXV0b3NjYWxpbmdLZXkiOiJtY3AtYXV0b3NjYWxpbmciLCJhdXRvc2NhbGluZ1ZhbHVlIjoidG9iZWRlbGV0ZWQiLCJjbGllbnRWZXJzaW9uIjoiMS4xMyIsImVmZmVjdCI6Ik5vU2NoZWR1bGUiLCJpbnN0YWxsUmVnaW9uQXJndW1lbnQiOiJhcHBseSIsIm1pbnNUb1dhaXRCZWZvcmVSZW1vdmVOb2RlIjoxLCJyZWdpb25JbnN0YWxsZXJOYW1lc3BhY2UiOiJwcm9kLXdyYWktdXNwLW1jcC1pbnN0YWxsZXIiLCJ0aW1lc3RhbXBLZXkiOiJtY3AtdGFpbnRlZGRhdGUiLCJ1bmluc3RhbGxSZWdpb25Bcmd1bWVudCI6ImRlc3Ryb3kifSwicmVnZXgiOnsidmFsdWUiOiJ3cmFpIn19\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/secret/secret.yaml"}, "region": {"startLine": 30, "endLine": 37, "snippet": {"text": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mcp-config-secret-cost\ntype: Opaque\ndata:\n  mcpconfigCost: eyJjb21tb24iOnsiZGVmYXVsdE1heEF1dG9TY2FsaW5nUmVzb3VyY2VOb2RlcyI6MTAsImRlZmF1bHRNaW5BdXRvU2NhbGluZ1Jlc291cmNlTm9kZXMiOjAsImluaXRpYWxpemUiOnRydWUsInNjYWxpbmdBY3Rpdml0eUF0dGVtcHRzIjo1LCJzY2FsaW5nQWN0aXZpdHlEZWxheU1pbGxpc2Vjb25kcyI6MTIwMDAwLCJzdWRvRGV2TW9kZSI6ZmFsc2UsInRhZ1RvQmFja3VwIjoiYmFzZSIsInRlc3RNb2RlRGJPbmx5IjpmYWxzZX19\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/secret/secret.yaml"}, "region": {"startLine": 39, "endLine": 46, "snippet": {"text": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mcp-config-secret-gateway\ntype: Opaque\ndata:\n  mcpconfigGateway: eyJjb21tb24iOnsiZGVmYXVsdE1heEF1dG9TY2FsaW5nUmVzb3VyY2VOb2RlcyI6MTAsImRlZmF1bHRNaW5BdXRvU2NhbGluZ1Jlc291cmNlTm9kZXMiOjAsImluaXRpYWxpemUiOnRydWUsInNjYWxpbmdBY3Rpdml0eUF0dGVtcHRzIjo1LCJzY2FsaW5nQWN0aXZpdHlEZWxheU1pbGxpc2Vjb25kcyI6MTIwMDAwLCJzdWRvRGV2TW9kZSI6ZmFsc2UsInRhZ1RvQmFja3VwIjoiYmFzZSIsInRlc3RNb2RlRGJPbmx5IjpmYWxzZX19\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/secret/secret.yaml"}, "region": {"startLine": 48, "endLine": 55, "snippet": {"text": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mcp-config-secret-install\ntype: Opaque\ndata:\n  mcpconfigInstall: eyJrdWJlcm5ldGVzIjp7ImF1dG9zY2FsaW5nS2V5IjoibWNwLWF1dG9zY2FsaW5nIiwiYXV0b3NjYWxpbmdWYWx1ZSI6InRvYmVkZWxldGVkIiwiY2xpZW50VmVyc2lvbiI6IjEuMTMiLCJlZmZlY3QiOiJOb1NjaGVkdWxlIiwiaW5zdGFsbFJlZ2lvbkFyZ3VtZW50IjoiYXBwbHkiLCJtaW5zVG9XYWl0QmVmb3JlUmVtb3ZlTm9kZSI6MSwicmVnaW9uSW5zdGFsbGVyTmFtZXNwYWNlIjoicHJvZC13cmFpLXVzcC1tY3AtaW5zdGFsbGVyIiwidGltZXN0YW1wS2V5IjoibWNwLXRhaW50ZWRkYXRlIiwidW5pbnN0YWxsUmVnaW9uQXJndW1lbnQiOiJkZXN0cm95In19\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/secret/secret.yaml"}, "region": {"startLine": 57, "endLine": 64, "snippet": {"text": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mcp-config-secret-tp-tools\ntype: Opaque\ndata:\n  mcpconfig3pTools: eyJjb21tb24iOnsiZGVmYXVsdE1heEF1dG9TY2FsaW5nUmVzb3VyY2VOb2RlcyI6MTAsImRlZmF1bHRNaW5BdXRvU2NhbGluZ1Jlc291cmNlTm9kZXMiOjAsImluaXRpYWxpemUiOnRydWUsInNjYWxpbmdBY3Rpdml0eUF0dGVtcHRzIjo1LCJzY2FsaW5nQWN0aXZpdHlEZWxheU1pbGxpc2Vjb25kcyI6MTIwMDAwLCJzdWRvRGV2TW9kZSI6ZmFsc2UsInRhZ1RvQmFja3VwIjoiYmFzZSIsInRlc3RNb2RlRGJPbmx5IjpmYWxzZX0sImNvbmR1Y3RvciI6eyJob3N0IjoiSE9TVCIsInBhc3MiOiJQQVNTIiwidGVuYW50IjoiVEVOQU5UIiwidXNlciI6IlVTRVIifSwidGhpcmRQYXJ0eVRvb2xzIjp7ImRlbGl2ZXJzQXJ0aWZhY3RzR2F0ZXdheSI6Imh0dHBzOi8vZ2F0ZXdheS5kZWxpdmVycy53aW5kcml2ZXIuY29tL3YxL3Rvb2xzL21pbmlvL2FydGlmYWN0cyIsImluc3RhbGxhdGlvbk5hbWVzcGFjZSI6InByb2Qtd3JhaS11c3AtbWNwIiwiaW5zdGFsbGVyRG9ja2VySW1hZ2UiOiJ3ci1zdHVkaW8tcHJvZHVjdC8zcHRvb2xzLzNwdG9vbHMtc2hhcmVkL2NvbnRlbnQtaW5zdGFsbGVyOjEuMC4wLTIyLjA2In19\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/secret/secret.yaml"}, "region": {"startLine": 66, "endLine": 72, "snippet": {"text": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mcp-config-secret-cronjob\ntype: Opaque\ndata:\n  mcpconfigCronjob: e30=\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/service/gateway.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: mcp-gateway\n  labels:\n    helm.sh/chart: mcp-gateway-1.0.0\n    app.kubernetes.io/name: mcp-gateway\n    app.kubernetes.io/instance: mcp-gateway\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: NodePort\n  ports:\n    - port: 80\n      targetPort: 3000\n      protocol: TCP\n      name: https-gateway\n  selector:\n    app.kubernetes.io/name: mcp-gateway\n    app.kubernetes.io/instance: mcp-gateway\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/mcp/templates/service/ui.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: mcp-ui\n  labels:\n    helm.sh/chart: mcp-ui-1.0.0\n    app.kubernetes.io/name: mcp-ui\n    app.kubernetes.io/instance: mcp-ui\n    app.kubernetes.io/version: \"1.0.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: NodePort\n  ports:\n    - port: 8888\n      targetPort: 8888\n      protocol: TCP\n      name: https-ui\n  selector:\n    app.kubernetes.io/name: mcp-ui\n    app.kubernetes.io/instance: mcp-ui\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smarter-device-manager/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: smarter-device-manager\n  labels:\n    name: smarter-device-manager\n    role: agent\nspec:\n  selector:\n    matchLabels:\n      name: smarter-device-manager\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        name: smarter-device-manager\n      annotations:\n        node.kubernetes.io/bootstrap-checkpoint: \"true\"\n    spec:\n      affinity: \n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: purpose\n                operator: In\n                values:\n                - spot\n                - scalable\n      tolerations: \n      - effect: NoSchedule\n        key: nestvirt\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: spot\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: spotInstance\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: purpose\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: team\n        operator: Equal\n        value: linux\n      hostname: smarter-device-management\n      hostNetwork: true\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n      - name: smarter-device-manager\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        image: \":\"\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 15Mi\n          requests:\n            cpu: 10m\n            memory: 15Mi\n        volumeMounts:\n          - name: device-plugin\n            mountPath: /var/lib/kubelet/device-plugins\n          - name: dev-dir\n            mountPath: /dev\n          - name: sys-dir\n            mountPath: /sys\n          - name: config\n            mountPath: /root/config\n      volumes:\n        - name: device-plugin\n          hostPath:\n            path: /var/lib/kubelet/device-plugins\n        - name: dev-dir\n          hostPath:\n            path: /dev\n        - name: sys-dir\n          hostPath:\n            path: /sys\n        - name: config\n          configMap:\n            name: smarter-device-manager\n---\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smarter-device-manager/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: smarter-device-manager\n  labels:\n    name: smarter-device-manager\n    role: agent\nspec:\n  selector:\n    matchLabels:\n      name: smarter-device-manager\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        name: smarter-device-manager\n      annotations:\n        node.kubernetes.io/bootstrap-checkpoint: \"true\"\n    spec:\n      affinity: \n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: purpose\n                operator: In\n                values:\n                - spot\n                - scalable\n      tolerations: \n      - effect: NoSchedule\n        key: nestvirt\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: spot\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: spotInstance\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: purpose\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: team\n        operator: Equal\n        value: linux\n      hostname: smarter-device-management\n      hostNetwork: true\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n      - name: smarter-device-manager\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        image: \":\"\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 15Mi\n          requests:\n            cpu: 10m\n            memory: 15Mi\n        volumeMounts:\n          - name: device-plugin\n            mountPath: /var/lib/kubelet/device-plugins\n          - name: dev-dir\n            mountPath: /dev\n          - name: sys-dir\n            mountPath: /sys\n          - name: config\n            mountPath: /root/config\n      volumes:\n        - name: device-plugin\n          hostPath:\n            path: /var/lib/kubelet/device-plugins\n        - name: dev-dir\n          hostPath:\n            path: /dev\n        - name: sys-dir\n          hostPath:\n            path: /sys\n        - name: config\n          configMap:\n            name: smarter-device-manager\n---\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smarter-device-manager/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: smarter-device-manager\n  labels:\n    name: smarter-device-manager\n    role: agent\nspec:\n  selector:\n    matchLabels:\n      name: smarter-device-manager\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        name: smarter-device-manager\n      annotations:\n        node.kubernetes.io/bootstrap-checkpoint: \"true\"\n    spec:\n      affinity: \n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: purpose\n                operator: In\n                values:\n                - spot\n                - scalable\n      tolerations: \n      - effect: NoSchedule\n        key: nestvirt\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: spot\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: spotInstance\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: purpose\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: team\n        operator: Equal\n        value: linux\n      hostname: smarter-device-management\n      hostNetwork: true\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n      - name: smarter-device-manager\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        image: \":\"\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 15Mi\n          requests:\n            cpu: 10m\n            memory: 15Mi\n        volumeMounts:\n          - name: device-plugin\n            mountPath: /var/lib/kubelet/device-plugins\n          - name: dev-dir\n            mountPath: /dev\n          - name: sys-dir\n            mountPath: /sys\n          - name: config\n            mountPath: /root/config\n      volumes:\n        - name: device-plugin\n          hostPath:\n            path: /var/lib/kubelet/device-plugins\n        - name: dev-dir\n          hostPath:\n            path: /dev\n        - name: sys-dir\n          hostPath:\n            path: /sys\n        - name: config\n          configMap:\n            name: smarter-device-manager\n---\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smarter-device-manager/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: smarter-device-manager\n  labels:\n    name: smarter-device-manager\n    role: agent\nspec:\n  selector:\n    matchLabels:\n      name: smarter-device-manager\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        name: smarter-device-manager\n      annotations:\n        node.kubernetes.io/bootstrap-checkpoint: \"true\"\n    spec:\n      affinity: \n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: purpose\n                operator: In\n                values:\n                - spot\n                - scalable\n      tolerations: \n      - effect: NoSchedule\n        key: nestvirt\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: spot\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: spotInstance\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: purpose\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: team\n        operator: Equal\n        value: linux\n      hostname: smarter-device-management\n      hostNetwork: true\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n      - name: smarter-device-manager\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        image: \":\"\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 15Mi\n          requests:\n            cpu: 10m\n            memory: 15Mi\n        volumeMounts:\n          - name: device-plugin\n            mountPath: /var/lib/kubelet/device-plugins\n          - name: dev-dir\n            mountPath: /dev\n          - name: sys-dir\n            mountPath: /sys\n          - name: config\n            mountPath: /root/config\n      volumes:\n        - name: device-plugin\n          hostPath:\n            path: /var/lib/kubelet/device-plugins\n        - name: dev-dir\n          hostPath:\n            path: /dev\n        - name: sys-dir\n          hostPath:\n            path: /sys\n        - name: config\n          configMap:\n            name: smarter-device-manager\n---\n"}}}}]}, {"ruleId": "CKV_K8S_19", "ruleIndex": 31, "level": "error", "attachments": [], "message": {"text": "Containers should not share the host network namespace"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smarter-device-manager/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: smarter-device-manager\n  labels:\n    name: smarter-device-manager\n    role: agent\nspec:\n  selector:\n    matchLabels:\n      name: smarter-device-manager\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        name: smarter-device-manager\n      annotations:\n        node.kubernetes.io/bootstrap-checkpoint: \"true\"\n    spec:\n      affinity: \n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: purpose\n                operator: In\n                values:\n                - spot\n                - scalable\n      tolerations: \n      - effect: NoSchedule\n        key: nestvirt\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: spot\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: spotInstance\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: purpose\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: team\n        operator: Equal\n        value: linux\n      hostname: smarter-device-management\n      hostNetwork: true\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n      - name: smarter-device-manager\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        image: \":\"\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 15Mi\n          requests:\n            cpu: 10m\n            memory: 15Mi\n        volumeMounts:\n          - name: device-plugin\n            mountPath: /var/lib/kubelet/device-plugins\n          - name: dev-dir\n            mountPath: /dev\n          - name: sys-dir\n            mountPath: /sys\n          - name: config\n            mountPath: /root/config\n      volumes:\n        - name: device-plugin\n          hostPath:\n            path: /var/lib/kubelet/device-plugins\n        - name: dev-dir\n          hostPath:\n            path: /dev\n        - name: sys-dir\n          hostPath:\n            path: /sys\n        - name: config\n          configMap:\n            name: smarter-device-manager\n---\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smarter-device-manager/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: smarter-device-manager\n  labels:\n    name: smarter-device-manager\n    role: agent\nspec:\n  selector:\n    matchLabels:\n      name: smarter-device-manager\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        name: smarter-device-manager\n      annotations:\n        node.kubernetes.io/bootstrap-checkpoint: \"true\"\n    spec:\n      affinity: \n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: purpose\n                operator: In\n                values:\n                - spot\n                - scalable\n      tolerations: \n      - effect: NoSchedule\n        key: nestvirt\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: spot\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: spotInstance\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: purpose\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: team\n        operator: Equal\n        value: linux\n      hostname: smarter-device-management\n      hostNetwork: true\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n      - name: smarter-device-manager\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        image: \":\"\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 15Mi\n          requests:\n            cpu: 10m\n            memory: 15Mi\n        volumeMounts:\n          - name: device-plugin\n            mountPath: /var/lib/kubelet/device-plugins\n          - name: dev-dir\n            mountPath: /dev\n          - name: sys-dir\n            mountPath: /sys\n          - name: config\n            mountPath: /root/config\n      volumes:\n        - name: device-plugin\n          hostPath:\n            path: /var/lib/kubelet/device-plugins\n        - name: dev-dir\n          hostPath:\n            path: /dev\n        - name: sys-dir\n          hostPath:\n            path: /sys\n        - name: config\n          configMap:\n            name: smarter-device-manager\n---\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smarter-device-manager/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: smarter-device-manager\n  labels:\n    name: smarter-device-manager\n    role: agent\nspec:\n  selector:\n    matchLabels:\n      name: smarter-device-manager\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        name: smarter-device-manager\n      annotations:\n        node.kubernetes.io/bootstrap-checkpoint: \"true\"\n    spec:\n      affinity: \n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: purpose\n                operator: In\n                values:\n                - spot\n                - scalable\n      tolerations: \n      - effect: NoSchedule\n        key: nestvirt\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: spot\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: spotInstance\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: purpose\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: team\n        operator: Equal\n        value: linux\n      hostname: smarter-device-management\n      hostNetwork: true\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n      - name: smarter-device-manager\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        image: \":\"\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 15Mi\n          requests:\n            cpu: 10m\n            memory: 15Mi\n        volumeMounts:\n          - name: device-plugin\n            mountPath: /var/lib/kubelet/device-plugins\n          - name: dev-dir\n            mountPath: /dev\n          - name: sys-dir\n            mountPath: /sys\n          - name: config\n            mountPath: /root/config\n      volumes:\n        - name: device-plugin\n          hostPath:\n            path: /var/lib/kubelet/device-plugins\n        - name: dev-dir\n          hostPath:\n            path: /dev\n        - name: sys-dir\n          hostPath:\n            path: /sys\n        - name: config\n          configMap:\n            name: smarter-device-manager\n---\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smarter-device-manager/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: smarter-device-manager\n  labels:\n    name: smarter-device-manager\n    role: agent\nspec:\n  selector:\n    matchLabels:\n      name: smarter-device-manager\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        name: smarter-device-manager\n      annotations:\n        node.kubernetes.io/bootstrap-checkpoint: \"true\"\n    spec:\n      affinity: \n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: purpose\n                operator: In\n                values:\n                - spot\n                - scalable\n      tolerations: \n      - effect: NoSchedule\n        key: nestvirt\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: spot\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: spotInstance\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: purpose\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: team\n        operator: Equal\n        value: linux\n      hostname: smarter-device-management\n      hostNetwork: true\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n      - name: smarter-device-manager\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        image: \":\"\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 15Mi\n          requests:\n            cpu: 10m\n            memory: 15Mi\n        volumeMounts:\n          - name: device-plugin\n            mountPath: /var/lib/kubelet/device-plugins\n          - name: dev-dir\n            mountPath: /dev\n          - name: sys-dir\n            mountPath: /sys\n          - name: config\n            mountPath: /root/config\n      volumes:\n        - name: device-plugin\n          hostPath:\n            path: /var/lib/kubelet/device-plugins\n        - name: dev-dir\n          hostPath:\n            path: /dev\n        - name: sys-dir\n          hostPath:\n            path: /sys\n        - name: config\n          configMap:\n            name: smarter-device-manager\n---\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smarter-device-manager/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: smarter-device-manager\n  labels:\n    name: smarter-device-manager\n    role: agent\nspec:\n  selector:\n    matchLabels:\n      name: smarter-device-manager\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        name: smarter-device-manager\n      annotations:\n        node.kubernetes.io/bootstrap-checkpoint: \"true\"\n    spec:\n      affinity: \n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: purpose\n                operator: In\n                values:\n                - spot\n                - scalable\n      tolerations: \n      - effect: NoSchedule\n        key: nestvirt\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: spot\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: spotInstance\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: purpose\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: team\n        operator: Equal\n        value: linux\n      hostname: smarter-device-management\n      hostNetwork: true\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n      - name: smarter-device-manager\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        image: \":\"\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 15Mi\n          requests:\n            cpu: 10m\n            memory: 15Mi\n        volumeMounts:\n          - name: device-plugin\n            mountPath: /var/lib/kubelet/device-plugins\n          - name: dev-dir\n            mountPath: /dev\n          - name: sys-dir\n            mountPath: /sys\n          - name: config\n            mountPath: /root/config\n      volumes:\n        - name: device-plugin\n          hostPath:\n            path: /var/lib/kubelet/device-plugins\n        - name: dev-dir\n          hostPath:\n            path: /dev\n        - name: sys-dir\n          hostPath:\n            path: /sys\n        - name: config\n          configMap:\n            name: smarter-device-manager\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smarter-device-manager/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: smarter-device-manager\n  labels:\n    name: smarter-device-manager\n    role: agent\nspec:\n  selector:\n    matchLabels:\n      name: smarter-device-manager\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        name: smarter-device-manager\n      annotations:\n        node.kubernetes.io/bootstrap-checkpoint: \"true\"\n    spec:\n      affinity: \n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: purpose\n                operator: In\n                values:\n                - spot\n                - scalable\n      tolerations: \n      - effect: NoSchedule\n        key: nestvirt\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: spot\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: spotInstance\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: purpose\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: team\n        operator: Equal\n        value: linux\n      hostname: smarter-device-management\n      hostNetwork: true\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n      - name: smarter-device-manager\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        image: \":\"\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 15Mi\n          requests:\n            cpu: 10m\n            memory: 15Mi\n        volumeMounts:\n          - name: device-plugin\n            mountPath: /var/lib/kubelet/device-plugins\n          - name: dev-dir\n            mountPath: /dev\n          - name: sys-dir\n            mountPath: /sys\n          - name: config\n            mountPath: /root/config\n      volumes:\n        - name: device-plugin\n          hostPath:\n            path: /var/lib/kubelet/device-plugins\n        - name: dev-dir\n          hostPath:\n            path: /dev\n        - name: sys-dir\n          hostPath:\n            path: /sys\n        - name: config\n          configMap:\n            name: smarter-device-manager\n---\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smarter-device-manager/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: smarter-device-manager\n  labels:\n    name: smarter-device-manager\n    role: agent\nspec:\n  selector:\n    matchLabels:\n      name: smarter-device-manager\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        name: smarter-device-manager\n      annotations:\n        node.kubernetes.io/bootstrap-checkpoint: \"true\"\n    spec:\n      affinity: \n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: purpose\n                operator: In\n                values:\n                - spot\n                - scalable\n      tolerations: \n      - effect: NoSchedule\n        key: nestvirt\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: spot\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: spotInstance\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: purpose\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: team\n        operator: Equal\n        value: linux\n      hostname: smarter-device-management\n      hostNetwork: true\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n      - name: smarter-device-manager\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        image: \":\"\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 15Mi\n          requests:\n            cpu: 10m\n            memory: 15Mi\n        volumeMounts:\n          - name: device-plugin\n            mountPath: /var/lib/kubelet/device-plugins\n          - name: dev-dir\n            mountPath: /dev\n          - name: sys-dir\n            mountPath: /sys\n          - name: config\n            mountPath: /root/config\n      volumes:\n        - name: device-plugin\n          hostPath:\n            path: /var/lib/kubelet/device-plugins\n        - name: dev-dir\n          hostPath:\n            path: /dev\n        - name: sys-dir\n          hostPath:\n            path: /sys\n        - name: config\n          configMap:\n            name: smarter-device-manager\n---\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smarter-device-manager/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 99, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: smarter-device-manager\n  labels:\n    name: smarter-device-manager\n    role: agent\nspec:\n  selector:\n    matchLabels:\n      name: smarter-device-manager\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        name: smarter-device-manager\n      annotations:\n        node.kubernetes.io/bootstrap-checkpoint: \"true\"\n    spec:\n      affinity: \n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: purpose\n                operator: In\n                values:\n                - spot\n                - scalable\n      tolerations: \n      - effect: NoSchedule\n        key: nestvirt\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: spot\n      - effect: NoSchedule\n        key: kubernetes.azure.com/scalesetpriority\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: spotInstance\n        operator: Equal\n        value: \"true\"\n      - effect: NoSchedule\n        key: purpose\n        operator: Equal\n        value: scalable\n      - effect: NoSchedule\n        key: team\n        operator: Equal\n        value: linux\n      hostname: smarter-device-management\n      hostNetwork: true\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n      - name: smarter-device-manager\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        image: \":\"\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 15Mi\n          requests:\n            cpu: 10m\n            memory: 15Mi\n        volumeMounts:\n          - name: device-plugin\n            mountPath: /var/lib/kubelet/device-plugins\n          - name: dev-dir\n            mountPath: /dev\n          - name: sys-dir\n            mountPath: /sys\n          - name: config\n            mountPath: /root/config\n      volumes:\n        - name: device-plugin\n          hostPath:\n            path: /var/lib/kubelet/device-plugins\n        - name: dev-dir\n          hostPath:\n            path: /dev\n        - name: sys-dir\n          hostPath:\n            path: /sys\n        - name: config\n          configMap:\n            name: smarter-device-manager\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smarter-device-manager/templates/configmap.yaml"}, "region": {"startLine": 3, "endLine": 9, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: device-manager\n  labels:\n    name: smarter-device-manager\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smarter-device-manager/templates/configmap.yaml"}, "region": {"startLine": 11, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: smarter-device-manager\ndata:\n  conf.yaml: |\n    - devicematch: ^kvm$\n      nummaxdevices: 256\n    - devicematch: ^vmx$\n      nummaxdevices: 256\n    - devicematch: ^nmihook$\n      nummaxdevices: 256\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/post-install-create-user-job.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: minio-make-user-job\n  namespace: \"default\"\n  labels:\n    app: minio-make-user-job\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded,before-hook-creation\nspec:\n  template:\n    metadata:\n      labels:\n        app: minio-job\n        release: minio\n    spec:\n      restartPolicy: OnFailure      \n      volumes:\n        - name: minio-configuration\n          projected:\n            sources:\n            - configMap:\n                name: minio\n            - secret:\n                name: minio\n      containers:\n      - name: minio-mc\n        image: \"quay.io/minio/mc:RELEASE.2022-04-16T21-11-21Z\"\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/sh\", \"/config/add-user\"]\n        env:\n          - name: MINIO_ENDPOINT\n            value: minio\n          - name: MINIO_PORT\n            value: \"9000\"\n        volumeMounts:\n          - name: minio-configuration\n            mountPath: /config\n        resources:\n          requests:\n            memory: 128Mi\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/post-install-create-user-job.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: minio-make-user-job\n  namespace: \"default\"\n  labels:\n    app: minio-make-user-job\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded,before-hook-creation\nspec:\n  template:\n    metadata:\n      labels:\n        app: minio-job\n        release: minio\n    spec:\n      restartPolicy: OnFailure      \n      volumes:\n        - name: minio-configuration\n          projected:\n            sources:\n            - configMap:\n                name: minio\n            - secret:\n                name: minio\n      containers:\n      - name: minio-mc\n        image: \"quay.io/minio/mc:RELEASE.2022-04-16T21-11-21Z\"\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/sh\", \"/config/add-user\"]\n        env:\n          - name: MINIO_ENDPOINT\n            value: minio\n          - name: MINIO_PORT\n            value: \"9000\"\n        volumeMounts:\n          - name: minio-configuration\n            mountPath: /config\n        resources:\n          requests:\n            memory: 128Mi\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/post-install-create-user-job.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: minio-make-user-job\n  namespace: \"default\"\n  labels:\n    app: minio-make-user-job\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded,before-hook-creation\nspec:\n  template:\n    metadata:\n      labels:\n        app: minio-job\n        release: minio\n    spec:\n      restartPolicy: OnFailure      \n      volumes:\n        - name: minio-configuration\n          projected:\n            sources:\n            - configMap:\n                name: minio\n            - secret:\n                name: minio\n      containers:\n      - name: minio-mc\n        image: \"quay.io/minio/mc:RELEASE.2022-04-16T21-11-21Z\"\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/sh\", \"/config/add-user\"]\n        env:\n          - name: MINIO_ENDPOINT\n            value: minio\n          - name: MINIO_PORT\n            value: \"9000\"\n        volumeMounts:\n          - name: minio-configuration\n            mountPath: /config\n        resources:\n          requests:\n            memory: 128Mi\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/post-install-create-user-job.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: minio-make-user-job\n  namespace: \"default\"\n  labels:\n    app: minio-make-user-job\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded,before-hook-creation\nspec:\n  template:\n    metadata:\n      labels:\n        app: minio-job\n        release: minio\n    spec:\n      restartPolicy: OnFailure      \n      volumes:\n        - name: minio-configuration\n          projected:\n            sources:\n            - configMap:\n                name: minio\n            - secret:\n                name: minio\n      containers:\n      - name: minio-mc\n        image: \"quay.io/minio/mc:RELEASE.2022-04-16T21-11-21Z\"\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/sh\", \"/config/add-user\"]\n        env:\n          - name: MINIO_ENDPOINT\n            value: minio\n          - name: MINIO_PORT\n            value: \"9000\"\n        volumeMounts:\n          - name: minio-configuration\n            mountPath: /config\n        resources:\n          requests:\n            memory: 128Mi\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/post-install-create-user-job.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: minio-make-user-job\n  namespace: \"default\"\n  labels:\n    app: minio-make-user-job\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded,before-hook-creation\nspec:\n  template:\n    metadata:\n      labels:\n        app: minio-job\n        release: minio\n    spec:\n      restartPolicy: OnFailure      \n      volumes:\n        - name: minio-configuration\n          projected:\n            sources:\n            - configMap:\n                name: minio\n            - secret:\n                name: minio\n      containers:\n      - name: minio-mc\n        image: \"quay.io/minio/mc:RELEASE.2022-04-16T21-11-21Z\"\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/sh\", \"/config/add-user\"]\n        env:\n          - name: MINIO_ENDPOINT\n            value: minio\n          - name: MINIO_PORT\n            value: \"9000\"\n        volumeMounts:\n          - name: minio-configuration\n            mountPath: /config\n        resources:\n          requests:\n            memory: 128Mi\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/post-install-create-user-job.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: minio-make-user-job\n  namespace: \"default\"\n  labels:\n    app: minio-make-user-job\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded,before-hook-creation\nspec:\n  template:\n    metadata:\n      labels:\n        app: minio-job\n        release: minio\n    spec:\n      restartPolicy: OnFailure      \n      volumes:\n        - name: minio-configuration\n          projected:\n            sources:\n            - configMap:\n                name: minio\n            - secret:\n                name: minio\n      containers:\n      - name: minio-mc\n        image: \"quay.io/minio/mc:RELEASE.2022-04-16T21-11-21Z\"\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/sh\", \"/config/add-user\"]\n        env:\n          - name: MINIO_ENDPOINT\n            value: minio\n          - name: MINIO_PORT\n            value: \"9000\"\n        volumeMounts:\n          - name: minio-configuration\n            mountPath: /config\n        resources:\n          requests:\n            memory: 128Mi\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/post-install-create-user-job.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: minio-make-user-job\n  namespace: \"default\"\n  labels:\n    app: minio-make-user-job\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded,before-hook-creation\nspec:\n  template:\n    metadata:\n      labels:\n        app: minio-job\n        release: minio\n    spec:\n      restartPolicy: OnFailure      \n      volumes:\n        - name: minio-configuration\n          projected:\n            sources:\n            - configMap:\n                name: minio\n            - secret:\n                name: minio\n      containers:\n      - name: minio-mc\n        image: \"quay.io/minio/mc:RELEASE.2022-04-16T21-11-21Z\"\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/sh\", \"/config/add-user\"]\n        env:\n          - name: MINIO_ENDPOINT\n            value: minio\n          - name: MINIO_PORT\n            value: \"9000\"\n        volumeMounts:\n          - name: minio-configuration\n            mountPath: /config\n        resources:\n          requests:\n            memory: 128Mi\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/post-install-create-user-job.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: minio-make-user-job\n  namespace: \"default\"\n  labels:\n    app: minio-make-user-job\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded,before-hook-creation\nspec:\n  template:\n    metadata:\n      labels:\n        app: minio-job\n        release: minio\n    spec:\n      restartPolicy: OnFailure      \n      volumes:\n        - name: minio-configuration\n          projected:\n            sources:\n            - configMap:\n                name: minio\n            - secret:\n                name: minio\n      containers:\n      - name: minio-mc\n        image: \"quay.io/minio/mc:RELEASE.2022-04-16T21-11-21Z\"\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/sh\", \"/config/add-user\"]\n        env:\n          - name: MINIO_ENDPOINT\n            value: minio\n          - name: MINIO_PORT\n            value: \"9000\"\n        volumeMounts:\n          - name: minio-configuration\n            mountPath: /config\n        resources:\n          requests:\n            memory: 128Mi\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/post-install-create-user-job.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: minio-make-user-job\n  namespace: \"default\"\n  labels:\n    app: minio-make-user-job\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded,before-hook-creation\nspec:\n  template:\n    metadata:\n      labels:\n        app: minio-job\n        release: minio\n    spec:\n      restartPolicy: OnFailure      \n      volumes:\n        - name: minio-configuration\n          projected:\n            sources:\n            - configMap:\n                name: minio\n            - secret:\n                name: minio\n      containers:\n      - name: minio-mc\n        image: \"quay.io/minio/mc:RELEASE.2022-04-16T21-11-21Z\"\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/sh\", \"/config/add-user\"]\n        env:\n          - name: MINIO_ENDPOINT\n            value: minio\n          - name: MINIO_PORT\n            value: \"9000\"\n        volumeMounts:\n          - name: minio-configuration\n            mountPath: /config\n        resources:\n          requests:\n            memory: 128Mi\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/post-install-create-user-job.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: minio-make-user-job\n  namespace: \"default\"\n  labels:\n    app: minio-make-user-job\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded,before-hook-creation\nspec:\n  template:\n    metadata:\n      labels:\n        app: minio-job\n        release: minio\n    spec:\n      restartPolicy: OnFailure      \n      volumes:\n        - name: minio-configuration\n          projected:\n            sources:\n            - configMap:\n                name: minio\n            - secret:\n                name: minio\n      containers:\n      - name: minio-mc\n        image: \"quay.io/minio/mc:RELEASE.2022-04-16T21-11-21Z\"\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/sh\", \"/config/add-user\"]\n        env:\n          - name: MINIO_ENDPOINT\n            value: minio\n          - name: MINIO_PORT\n            value: \"9000\"\n        volumeMounts:\n          - name: minio-configuration\n            mountPath: /config\n        resources:\n          requests:\n            memory: 128Mi\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/post-install-create-user-job.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: minio-make-user-job\n  namespace: \"default\"\n  labels:\n    app: minio-make-user-job\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded,before-hook-creation\nspec:\n  template:\n    metadata:\n      labels:\n        app: minio-job\n        release: minio\n    spec:\n      restartPolicy: OnFailure      \n      volumes:\n        - name: minio-configuration\n          projected:\n            sources:\n            - configMap:\n                name: minio\n            - secret:\n                name: minio\n      containers:\n      - name: minio-mc\n        image: \"quay.io/minio/mc:RELEASE.2022-04-16T21-11-21Z\"\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/sh\", \"/config/add-user\"]\n        env:\n          - name: MINIO_ENDPOINT\n            value: minio\n          - name: MINIO_PORT\n            value: \"9000\"\n        volumeMounts:\n          - name: minio-configuration\n            mountPath: /config\n        resources:\n          requests:\n            memory: 128Mi\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/post-install-create-user-job.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: minio-make-user-job\n  namespace: \"default\"\n  labels:\n    app: minio-make-user-job\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded,before-hook-creation\nspec:\n  template:\n    metadata:\n      labels:\n        app: minio-job\n        release: minio\n    spec:\n      restartPolicy: OnFailure      \n      volumes:\n        - name: minio-configuration\n          projected:\n            sources:\n            - configMap:\n                name: minio\n            - secret:\n                name: minio\n      containers:\n      - name: minio-mc\n        image: \"quay.io/minio/mc:RELEASE.2022-04-16T21-11-21Z\"\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/sh\", \"/config/add-user\"]\n        env:\n          - name: MINIO_ENDPOINT\n            value: minio\n          - name: MINIO_PORT\n            value: \"9000\"\n        volumeMounts:\n          - name: minio-configuration\n            mountPath: /config\n        resources:\n          requests:\n            memory: 128Mi\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/post-install-create-user-job.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: minio-make-user-job\n  namespace: \"default\"\n  labels:\n    app: minio-make-user-job\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded,before-hook-creation\nspec:\n  template:\n    metadata:\n      labels:\n        app: minio-job\n        release: minio\n    spec:\n      restartPolicy: OnFailure      \n      volumes:\n        - name: minio-configuration\n          projected:\n            sources:\n            - configMap:\n                name: minio\n            - secret:\n                name: minio\n      containers:\n      - name: minio-mc\n        image: \"quay.io/minio/mc:RELEASE.2022-04-16T21-11-21Z\"\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/sh\", \"/config/add-user\"]\n        env:\n          - name: MINIO_ENDPOINT\n            value: minio\n          - name: MINIO_PORT\n            value: \"9000\"\n        volumeMounts:\n          - name: minio-configuration\n            mountPath: /config\n        resources:\n          requests:\n            memory: 128Mi\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/post-install-create-user-job.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: minio-make-user-job\n  namespace: \"default\"\n  labels:\n    app: minio-make-user-job\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded,before-hook-creation\nspec:\n  template:\n    metadata:\n      labels:\n        app: minio-job\n        release: minio\n    spec:\n      restartPolicy: OnFailure      \n      volumes:\n        - name: minio-configuration\n          projected:\n            sources:\n            - configMap:\n                name: minio\n            - secret:\n                name: minio\n      containers:\n      - name: minio-mc\n        image: \"quay.io/minio/mc:RELEASE.2022-04-16T21-11-21Z\"\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/sh\", \"/config/add-user\"]\n        env:\n          - name: MINIO_ENDPOINT\n            value: minio\n          - name: MINIO_PORT\n            value: \"9000\"\n        volumeMounts:\n          - name: minio-configuration\n            mountPath: /config\n        resources:\n          requests:\n            memory: 128Mi\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/post-install-create-user-job.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: minio-make-user-job\n  namespace: \"default\"\n  labels:\n    app: minio-make-user-job\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded,before-hook-creation\nspec:\n  template:\n    metadata:\n      labels:\n        app: minio-job\n        release: minio\n    spec:\n      restartPolicy: OnFailure      \n      volumes:\n        - name: minio-configuration\n          projected:\n            sources:\n            - configMap:\n                name: minio\n            - secret:\n                name: minio\n      containers:\n      - name: minio-mc\n        image: \"quay.io/minio/mc:RELEASE.2022-04-16T21-11-21Z\"\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/sh\", \"/config/add-user\"]\n        env:\n          - name: MINIO_ENDPOINT\n            value: minio\n          - name: MINIO_PORT\n            value: \"9000\"\n        volumeMounts:\n          - name: minio-configuration\n            mountPath: /config\n        resources:\n          requests:\n            memory: 128Mi\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/post-install-create-user-job.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: minio-make-user-job\n  namespace: \"default\"\n  labels:\n    app: minio-make-user-job\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded,before-hook-creation\nspec:\n  template:\n    metadata:\n      labels:\n        app: minio-job\n        release: minio\n    spec:\n      restartPolicy: OnFailure      \n      volumes:\n        - name: minio-configuration\n          projected:\n            sources:\n            - configMap:\n                name: minio\n            - secret:\n                name: minio\n      containers:\n      - name: minio-mc\n        image: \"quay.io/minio/mc:RELEASE.2022-04-16T21-11-21Z\"\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/sh\", \"/config/add-user\"]\n        env:\n          - name: MINIO_ENDPOINT\n            value: minio\n          - name: MINIO_PORT\n            value: \"9000\"\n        volumeMounts:\n          - name: minio-configuration\n            mountPath: /config\n        resources:\n          requests:\n            memory: 128Mi\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/secrets.yaml"}, "region": {"startLine": 3, "endLine": 16, "snippet": {"text": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: minio\n  namespace: \"default\"\n  labels:\n    app: minio\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\ntype: Opaque\ndata:\n  rootUser: \"VTM0dXZVZEFnWm56eVNIMzVZeTc=\"\n  rootPassword: \"VTdpREszRTljZG5HdnI5anVxRjFVdFVrNUd0bHpGY3BMT2VaVkFxWA==\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/service.yaml"}, "region": {"startLine": 3, "endLine": 23, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: minio\n  namespace: \"default\"\n  labels:\n    app: minio\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\n    monitoring: \"true\"\nspec:\n  type: ClusterIP\n  ports:\n    - name: http\n      port: 9000\n      protocol: TCP\n      targetPort: 9000\n  selector:\n    app: minio\n    release: minio\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 7, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: \"minio-sa\"\n  namespace: \"default\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/configmap.yaml"}, "region": {"startLine": 3, "endLine": 286, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: minio\n  namespace: \"default\"\n  labels:\n    app: minio\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\ndata:\n  initialize: |-\n    #!/bin/sh\n    set -e ; # Have script exit in the event of a failed command.\n    MC_CONFIG_DIR=\"/etc/minio/mc/\"\n    MC=\"/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}\"\n    \n    # connectToMinio\n    # Use a check-sleep-check loop to wait for MinIO service to be available\n    connectToMinio() {\n      SCHEME=$1\n      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts\n      set -e ; # fail if we can't read the keys.\n      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;  set +e ; # The connections to minio are allowed to fail.\n      echo \"Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT\" ;\n      MC_COMMAND=\"${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET\" ;\n      $MC_COMMAND ;\n      STATUS=$? ;\n      until [ $STATUS = 0 ]\n      do\n        ATTEMPTS=`expr $ATTEMPTS + 1` ;\n        echo \\\"Failed attempts: $ATTEMPTS\\\" ;\n        if [ $ATTEMPTS -gt $LIMIT ]; then\n          exit 1 ;\n        fi ;\n        sleep 2 ; # 1 second intervals between attempts\n        $MC_COMMAND ;\n        STATUS=$? ;\n      done ;\n      set -e ; # reset `e` as active\n      return 0\n    }\n    \n    # checkBucketExists ($bucket)\n    # Check if the bucket exists, by using the exit code of `mc ls`\n    checkBucketExists() {\n      BUCKET=$1\n      CMD=$(${MC} ls myminio/$BUCKET > /dev/null 2>&1)\n      return $?\n    }\n    \n    # createBucket ($bucket, $policy, $purge)\n    # Ensure bucket exists, purging if asked to\n    createBucket() {\n      BUCKET=$1\n      POLICY=$2\n      PURGE=$3\n      VERSIONING=$4\n    \n      # Purge the bucket, if set & exists\n      # Since PURGE is user input, check explicitly for `true`\n      if [ $PURGE = true ]; then\n        if checkBucketExists $BUCKET ; then\n          echo \"Purging bucket '$BUCKET'.\"\n          set +e ; # don't exit if this fails\n          ${MC} rm -r --force myminio/$BUCKET\n          set -e ; # reset `e` as active\n        else\n          echo \"Bucket '$BUCKET' does not exist, skipping purge.\"\n        fi\n      fi\n    \n      # Create the bucket if it does not exist\n      if ! checkBucketExists $BUCKET ; then\n        echo \"Creating bucket '$BUCKET'\"\n        ${MC} mb myminio/$BUCKET\n      else\n        echo \"Bucket '$BUCKET' already exists.\"\n      fi\n    \n    \n      # set versioning for bucket\n      if [ ! -z $VERSIONING ] ; then\n        if [ $VERSIONING = true ] ; then\n            echo \"Enabling versioning for '$BUCKET'\"\n            ${MC} version enable myminio/$BUCKET\n        elif [ $VERSIONING = false ] ; then\n            echo \"Suspending versioning for '$BUCKET'\"\n            ${MC} version suspend myminio/$BUCKET\n        fi\n      else\n          echo \"Bucket '$BUCKET' versioning unchanged.\"\n      fi\n    \n      # At this point, the bucket should exist, skip checking for existence\n      # Set policy on the bucket\n      echo \"Setting policy of bucket '$BUCKET' to '$POLICY'.\"\n      ${MC} policy set $POLICY myminio/$BUCKET\n    }\n    \n    # Try connecting to MinIO instance\n    scheme=http\n    connectToMinio $scheme\n    \n    \n    \n  add-user: |-\n    #!/bin/sh\n    set -e ; # Have script exit in the event of a failed command.\n    MC_CONFIG_DIR=\"/etc/minio/mc/\"\n    MC=\"/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}\"\n    \n    # connectToMinio\n    # Use a check-sleep-check loop to wait for MinIO service to be available\n    connectToMinio() {\n      SCHEME=$1\n      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts\n      set -e ; # fail if we can't read the keys.\n      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;\n      set +e ; # The connections to minio are allowed to fail.\n      echo \"Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT\" ;\n      MC_COMMAND=\"${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET\" ;\n      $MC_COMMAND ;\n      STATUS=$? ;\n      until [ $STATUS = 0 ]\n      do\n        ATTEMPTS=`expr $ATTEMPTS + 1` ;\n        echo \\\"Failed attempts: $ATTEMPTS\\\" ;\n        if [ $ATTEMPTS -gt $LIMIT ]; then\n          exit 1 ;\n        fi ;\n        sleep 2 ; # 1 second intervals between attempts\n        $MC_COMMAND ;\n        STATUS=$? ;\n      done ;\n      set -e ; # reset `e` as active\n      return 0\n    }\n    \n    # checkUserExists ($username)\n    # Check if the user exists, by using the exit code of `mc admin user info`\n    checkUserExists() {\n      USER=$1\n      CMD=$(${MC} admin user info myminio $USER > /dev/null 2>&1)\n      return $?\n    }\n    \n    # createUser ($username, $password, $policy)\n    createUser() {\n      USER=$1\n      PASS=$2\n      POLICY=$3\n    \n      # Create the user if it does not exist\n      if ! checkUserExists $USER ; then\n        echo \"Creating user '$USER'\"\n        ${MC} admin user add myminio $USER $PASS\n      else\n        echo \"User '$USER' already exists.\"\n      fi\n    \n    \n      # set policy for user\n      if [ ! -z $POLICY -a $POLICY != \" \" ] ; then\n          echo \"Adding policy '$POLICY' for '$USER'\"\n          ${MC} admin policy set myminio $POLICY user=$USER\n      else\n          echo \"User '$USER' has no policy attached.\"\n      fi\n    }\n    \n    # Try connecting to MinIO instance\n    scheme=http\n    connectToMinio $scheme\n    \n    \n    \n    # Create the users\n    createUser console console123 consoleAdmin\n    \n  add-policy: |-\n    #!/bin/sh\n    set -e ; # Have script exit in the event of a failed command.\n    MC_CONFIG_DIR=\"/etc/minio/mc/\"\n    MC=\"/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}\"\n    \n    # connectToMinio\n    # Use a check-sleep-check loop to wait for MinIO service to be available\n    connectToMinio() {\n      SCHEME=$1\n      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts\n      set -e ; # fail if we can't read the keys.\n      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;  set +e ; # The connections to minio are allowed to fail.\n      echo \"Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT\" ;\n      MC_COMMAND=\"${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET\" ;\n      $MC_COMMAND ;\n      STATUS=$? ;\n      until [ $STATUS = 0 ]\n      do\n        ATTEMPTS=`expr $ATTEMPTS + 1` ;\n        echo \\\"Failed attempts: $ATTEMPTS\\\" ;\n        if [ $ATTEMPTS -gt $LIMIT ]; then\n          exit 1 ;\n        fi ;\n        sleep 2 ; # 1 second intervals between attempts\n        $MC_COMMAND ;\n        STATUS=$? ;\n      done ;\n      set -e ; # reset `e` as active\n      return 0\n    }\n    \n    # checkPolicyExists ($policy)\n    # Check if the policy exists, by using the exit code of `mc admin policy info`\n    checkPolicyExists() {\n      POLICY=$1\n      CMD=$(${MC} admin policy info myminio $POLICY > /dev/null 2>&1)\n      return $?\n    }\n    \n    # createPolicy($name, $filename)\n    createPolicy () {\n      NAME=$1\n      FILENAME=$2\n    \n      # Create the name if it does not exist\n      echo \"Checking policy: $NAME (in /config/$FILENAME.json)\"\n      if ! checkPolicyExists $NAME ; then\n        echo \"Creating policy '$NAME'\"\n      else\n        echo \"Policy '$NAME' already exists.\"\n      fi\n      ${MC} admin policy add myminio $NAME /config/$FILENAME.json\n    \n    }\n    \n    # Try connecting to MinIO instance\n    scheme=http\n    connectToMinio $scheme\n    \n    \n    \n  custom-command: |-\n    #!/bin/sh\n    set -e ; # Have script exit in the event of a failed command.\n    MC_CONFIG_DIR=\"/etc/minio/mc/\"\n    MC=\"/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}\"\n    \n    # connectToMinio\n    # Use a check-sleep-check loop to wait for MinIO service to be available\n    connectToMinio() {\n      SCHEME=$1\n      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts\n      set -e ; # fail if we can't read the keys.\n      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;  set +e ; # The connections to minio are allowed to fail.\n      echo \"Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT\" ;\n      MC_COMMAND=\"${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET\" ;\n      $MC_COMMAND ;\n      STATUS=$? ;\n      until [ $STATUS = 0 ]\n      do\n        ATTEMPTS=`expr $ATTEMPTS + 1` ;\n        echo \\\"Failed attempts: $ATTEMPTS\\\" ;\n        if [ $ATTEMPTS -gt $LIMIT ]; then\n          exit 1 ;\n        fi ;\n        sleep 2 ; # 1 second intervals between attempts\n        $MC_COMMAND ;\n        STATUS=$? ;\n      done ;\n      set -e ; # reset `e` as active\n      return 0\n    }\n    \n    # runCommand ($@)\n    # Run custom mc command\n    runCommand() {\n      ${MC} \"$@\"\n      return $?\n    }\n    \n    # Try connecting to MinIO instance\n    scheme=http\n    connectToMinio $scheme\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 23, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: minio-svc\n  namespace: \"default\"\n  labels:\n    app: minio\n    chart: minio-4.0.1\n    release: \"minio\"\n    heritage: \"Helm\"\nspec:\n  publishNotReadyAddresses: true\n  clusterIP: None\n  ports:\n    - name: http\n      port: 9000\n      protocol: TCP\n  selector:\n    app: minio\n    release: minio\n---\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/statefulset.yaml"}, "region": {"startLine": 25, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\n  namespace: \"default\"\n  labels:\n    app: minio\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  podManagementPolicy: \"Parallel\"\n  serviceName: minio-svc\n  replicas: 16\n  selector:\n    matchLabels:\n      app: minio\n      release: minio\n  template:\n    metadata:\n      name: minio\n      labels:\n        app: minio\n        release: minio\n      annotations:\n        checksum/secrets: fb493769d34bada41f1b25a9f5e0f1aecbd8b5c3059276a7b44297cffdb5ae8e\n        checksum/config: 38e836a40bf068574c583e885605cdea0398da30e818b685b6e8b8d34a1f4790\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n        fsGroupChangePolicy: OnRootMismatch\n\n      serviceAccountName: minio-sa\n      containers:\n        - name: minio\n          image: quay.io/minio/minio:RELEASE.2022-04-30T22-23-53Z\n          imagePullPolicy: IfNotPresent\n\n          command: [ \"/bin/sh\",\n            \"-ce\",\n            \"/usr/bin/docker-entrypoint.sh minio server  http://minio-{0...15}.minio-svc.default.svc.cluster.local/export -S /etc/minio/certs/ --address :9000 --console-address :9001\" ]\n          volumeMounts:\n            - name: export\n              mountPath: /export            \n          ports:\n            - name: http\n              containerPort: 9000\n            - name: http-console\n              containerPort: 9001\n          livenessProbe:\n            httpGet:\n              path: /minio/health/live\n              port: 9000\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 30\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n            - name: MINIO_ROOT_USER\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootUser\n            - name: MINIO_ROOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootPassword\n            - name: MINIO_PROMETHEUS_AUTH_TYPE\n              value: \"public\"\n          resources:\n            requests:\n              memory: 16Gi      \n      volumes:\n        - name: minio-user\n          secret:\n            secretName: minio        \n  volumeClaimTemplates:\n    - metadata:\n        name: export\n      spec:\n        accessModes: [ \"ReadWriteOnce\" ]\n        resources:\n          requests:\n            storage: 500Gi\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/statefulset.yaml"}, "region": {"startLine": 25, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\n  namespace: \"default\"\n  labels:\n    app: minio\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  podManagementPolicy: \"Parallel\"\n  serviceName: minio-svc\n  replicas: 16\n  selector:\n    matchLabels:\n      app: minio\n      release: minio\n  template:\n    metadata:\n      name: minio\n      labels:\n        app: minio\n        release: minio\n      annotations:\n        checksum/secrets: fb493769d34bada41f1b25a9f5e0f1aecbd8b5c3059276a7b44297cffdb5ae8e\n        checksum/config: 38e836a40bf068574c583e885605cdea0398da30e818b685b6e8b8d34a1f4790\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n        fsGroupChangePolicy: OnRootMismatch\n\n      serviceAccountName: minio-sa\n      containers:\n        - name: minio\n          image: quay.io/minio/minio:RELEASE.2022-04-30T22-23-53Z\n          imagePullPolicy: IfNotPresent\n\n          command: [ \"/bin/sh\",\n            \"-ce\",\n            \"/usr/bin/docker-entrypoint.sh minio server  http://minio-{0...15}.minio-svc.default.svc.cluster.local/export -S /etc/minio/certs/ --address :9000 --console-address :9001\" ]\n          volumeMounts:\n            - name: export\n              mountPath: /export            \n          ports:\n            - name: http\n              containerPort: 9000\n            - name: http-console\n              containerPort: 9001\n          livenessProbe:\n            httpGet:\n              path: /minio/health/live\n              port: 9000\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 30\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n            - name: MINIO_ROOT_USER\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootUser\n            - name: MINIO_ROOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootPassword\n            - name: MINIO_PROMETHEUS_AUTH_TYPE\n              value: \"public\"\n          resources:\n            requests:\n              memory: 16Gi      \n      volumes:\n        - name: minio-user\n          secret:\n            secretName: minio        \n  volumeClaimTemplates:\n    - metadata:\n        name: export\n      spec:\n        accessModes: [ \"ReadWriteOnce\" ]\n        resources:\n          requests:\n            storage: 500Gi\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/statefulset.yaml"}, "region": {"startLine": 25, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\n  namespace: \"default\"\n  labels:\n    app: minio\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  podManagementPolicy: \"Parallel\"\n  serviceName: minio-svc\n  replicas: 16\n  selector:\n    matchLabels:\n      app: minio\n      release: minio\n  template:\n    metadata:\n      name: minio\n      labels:\n        app: minio\n        release: minio\n      annotations:\n        checksum/secrets: fb493769d34bada41f1b25a9f5e0f1aecbd8b5c3059276a7b44297cffdb5ae8e\n        checksum/config: 38e836a40bf068574c583e885605cdea0398da30e818b685b6e8b8d34a1f4790\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n        fsGroupChangePolicy: OnRootMismatch\n\n      serviceAccountName: minio-sa\n      containers:\n        - name: minio\n          image: quay.io/minio/minio:RELEASE.2022-04-30T22-23-53Z\n          imagePullPolicy: IfNotPresent\n\n          command: [ \"/bin/sh\",\n            \"-ce\",\n            \"/usr/bin/docker-entrypoint.sh minio server  http://minio-{0...15}.minio-svc.default.svc.cluster.local/export -S /etc/minio/certs/ --address :9000 --console-address :9001\" ]\n          volumeMounts:\n            - name: export\n              mountPath: /export            \n          ports:\n            - name: http\n              containerPort: 9000\n            - name: http-console\n              containerPort: 9001\n          livenessProbe:\n            httpGet:\n              path: /minio/health/live\n              port: 9000\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 30\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n            - name: MINIO_ROOT_USER\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootUser\n            - name: MINIO_ROOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootPassword\n            - name: MINIO_PROMETHEUS_AUTH_TYPE\n              value: \"public\"\n          resources:\n            requests:\n              memory: 16Gi      \n      volumes:\n        - name: minio-user\n          secret:\n            secretName: minio        \n  volumeClaimTemplates:\n    - metadata:\n        name: export\n      spec:\n        accessModes: [ \"ReadWriteOnce\" ]\n        resources:\n          requests:\n            storage: 500Gi\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/statefulset.yaml"}, "region": {"startLine": 25, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\n  namespace: \"default\"\n  labels:\n    app: minio\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  podManagementPolicy: \"Parallel\"\n  serviceName: minio-svc\n  replicas: 16\n  selector:\n    matchLabels:\n      app: minio\n      release: minio\n  template:\n    metadata:\n      name: minio\n      labels:\n        app: minio\n        release: minio\n      annotations:\n        checksum/secrets: fb493769d34bada41f1b25a9f5e0f1aecbd8b5c3059276a7b44297cffdb5ae8e\n        checksum/config: 38e836a40bf068574c583e885605cdea0398da30e818b685b6e8b8d34a1f4790\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n        fsGroupChangePolicy: OnRootMismatch\n\n      serviceAccountName: minio-sa\n      containers:\n        - name: minio\n          image: quay.io/minio/minio:RELEASE.2022-04-30T22-23-53Z\n          imagePullPolicy: IfNotPresent\n\n          command: [ \"/bin/sh\",\n            \"-ce\",\n            \"/usr/bin/docker-entrypoint.sh minio server  http://minio-{0...15}.minio-svc.default.svc.cluster.local/export -S /etc/minio/certs/ --address :9000 --console-address :9001\" ]\n          volumeMounts:\n            - name: export\n              mountPath: /export            \n          ports:\n            - name: http\n              containerPort: 9000\n            - name: http-console\n              containerPort: 9001\n          livenessProbe:\n            httpGet:\n              path: /minio/health/live\n              port: 9000\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 30\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n            - name: MINIO_ROOT_USER\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootUser\n            - name: MINIO_ROOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootPassword\n            - name: MINIO_PROMETHEUS_AUTH_TYPE\n              value: \"public\"\n          resources:\n            requests:\n              memory: 16Gi      \n      volumes:\n        - name: minio-user\n          secret:\n            secretName: minio        \n  volumeClaimTemplates:\n    - metadata:\n        name: export\n      spec:\n        accessModes: [ \"ReadWriteOnce\" ]\n        resources:\n          requests:\n            storage: 500Gi\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/statefulset.yaml"}, "region": {"startLine": 25, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\n  namespace: \"default\"\n  labels:\n    app: minio\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  podManagementPolicy: \"Parallel\"\n  serviceName: minio-svc\n  replicas: 16\n  selector:\n    matchLabels:\n      app: minio\n      release: minio\n  template:\n    metadata:\n      name: minio\n      labels:\n        app: minio\n        release: minio\n      annotations:\n        checksum/secrets: fb493769d34bada41f1b25a9f5e0f1aecbd8b5c3059276a7b44297cffdb5ae8e\n        checksum/config: 38e836a40bf068574c583e885605cdea0398da30e818b685b6e8b8d34a1f4790\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n        fsGroupChangePolicy: OnRootMismatch\n\n      serviceAccountName: minio-sa\n      containers:\n        - name: minio\n          image: quay.io/minio/minio:RELEASE.2022-04-30T22-23-53Z\n          imagePullPolicy: IfNotPresent\n\n          command: [ \"/bin/sh\",\n            \"-ce\",\n            \"/usr/bin/docker-entrypoint.sh minio server  http://minio-{0...15}.minio-svc.default.svc.cluster.local/export -S /etc/minio/certs/ --address :9000 --console-address :9001\" ]\n          volumeMounts:\n            - name: export\n              mountPath: /export            \n          ports:\n            - name: http\n              containerPort: 9000\n            - name: http-console\n              containerPort: 9001\n          livenessProbe:\n            httpGet:\n              path: /minio/health/live\n              port: 9000\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 30\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n            - name: MINIO_ROOT_USER\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootUser\n            - name: MINIO_ROOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootPassword\n            - name: MINIO_PROMETHEUS_AUTH_TYPE\n              value: \"public\"\n          resources:\n            requests:\n              memory: 16Gi      \n      volumes:\n        - name: minio-user\n          secret:\n            secretName: minio        \n  volumeClaimTemplates:\n    - metadata:\n        name: export\n      spec:\n        accessModes: [ \"ReadWriteOnce\" ]\n        resources:\n          requests:\n            storage: 500Gi\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/statefulset.yaml"}, "region": {"startLine": 25, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\n  namespace: \"default\"\n  labels:\n    app: minio\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  podManagementPolicy: \"Parallel\"\n  serviceName: minio-svc\n  replicas: 16\n  selector:\n    matchLabels:\n      app: minio\n      release: minio\n  template:\n    metadata:\n      name: minio\n      labels:\n        app: minio\n        release: minio\n      annotations:\n        checksum/secrets: fb493769d34bada41f1b25a9f5e0f1aecbd8b5c3059276a7b44297cffdb5ae8e\n        checksum/config: 38e836a40bf068574c583e885605cdea0398da30e818b685b6e8b8d34a1f4790\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n        fsGroupChangePolicy: OnRootMismatch\n\n      serviceAccountName: minio-sa\n      containers:\n        - name: minio\n          image: quay.io/minio/minio:RELEASE.2022-04-30T22-23-53Z\n          imagePullPolicy: IfNotPresent\n\n          command: [ \"/bin/sh\",\n            \"-ce\",\n            \"/usr/bin/docker-entrypoint.sh minio server  http://minio-{0...15}.minio-svc.default.svc.cluster.local/export -S /etc/minio/certs/ --address :9000 --console-address :9001\" ]\n          volumeMounts:\n            - name: export\n              mountPath: /export            \n          ports:\n            - name: http\n              containerPort: 9000\n            - name: http-console\n              containerPort: 9001\n          livenessProbe:\n            httpGet:\n              path: /minio/health/live\n              port: 9000\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 30\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n            - name: MINIO_ROOT_USER\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootUser\n            - name: MINIO_ROOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootPassword\n            - name: MINIO_PROMETHEUS_AUTH_TYPE\n              value: \"public\"\n          resources:\n            requests:\n              memory: 16Gi      \n      volumes:\n        - name: minio-user\n          secret:\n            secretName: minio        \n  volumeClaimTemplates:\n    - metadata:\n        name: export\n      spec:\n        accessModes: [ \"ReadWriteOnce\" ]\n        resources:\n          requests:\n            storage: 500Gi\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/statefulset.yaml"}, "region": {"startLine": 25, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\n  namespace: \"default\"\n  labels:\n    app: minio\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  podManagementPolicy: \"Parallel\"\n  serviceName: minio-svc\n  replicas: 16\n  selector:\n    matchLabels:\n      app: minio\n      release: minio\n  template:\n    metadata:\n      name: minio\n      labels:\n        app: minio\n        release: minio\n      annotations:\n        checksum/secrets: fb493769d34bada41f1b25a9f5e0f1aecbd8b5c3059276a7b44297cffdb5ae8e\n        checksum/config: 38e836a40bf068574c583e885605cdea0398da30e818b685b6e8b8d34a1f4790\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n        fsGroupChangePolicy: OnRootMismatch\n\n      serviceAccountName: minio-sa\n      containers:\n        - name: minio\n          image: quay.io/minio/minio:RELEASE.2022-04-30T22-23-53Z\n          imagePullPolicy: IfNotPresent\n\n          command: [ \"/bin/sh\",\n            \"-ce\",\n            \"/usr/bin/docker-entrypoint.sh minio server  http://minio-{0...15}.minio-svc.default.svc.cluster.local/export -S /etc/minio/certs/ --address :9000 --console-address :9001\" ]\n          volumeMounts:\n            - name: export\n              mountPath: /export            \n          ports:\n            - name: http\n              containerPort: 9000\n            - name: http-console\n              containerPort: 9001\n          livenessProbe:\n            httpGet:\n              path: /minio/health/live\n              port: 9000\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 30\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n            - name: MINIO_ROOT_USER\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootUser\n            - name: MINIO_ROOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootPassword\n            - name: MINIO_PROMETHEUS_AUTH_TYPE\n              value: \"public\"\n          resources:\n            requests:\n              memory: 16Gi      \n      volumes:\n        - name: minio-user\n          secret:\n            secretName: minio        \n  volumeClaimTemplates:\n    - metadata:\n        name: export\n      spec:\n        accessModes: [ \"ReadWriteOnce\" ]\n        resources:\n          requests:\n            storage: 500Gi\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/statefulset.yaml"}, "region": {"startLine": 25, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\n  namespace: \"default\"\n  labels:\n    app: minio\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  podManagementPolicy: \"Parallel\"\n  serviceName: minio-svc\n  replicas: 16\n  selector:\n    matchLabels:\n      app: minio\n      release: minio\n  template:\n    metadata:\n      name: minio\n      labels:\n        app: minio\n        release: minio\n      annotations:\n        checksum/secrets: fb493769d34bada41f1b25a9f5e0f1aecbd8b5c3059276a7b44297cffdb5ae8e\n        checksum/config: 38e836a40bf068574c583e885605cdea0398da30e818b685b6e8b8d34a1f4790\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n        fsGroupChangePolicy: OnRootMismatch\n\n      serviceAccountName: minio-sa\n      containers:\n        - name: minio\n          image: quay.io/minio/minio:RELEASE.2022-04-30T22-23-53Z\n          imagePullPolicy: IfNotPresent\n\n          command: [ \"/bin/sh\",\n            \"-ce\",\n            \"/usr/bin/docker-entrypoint.sh minio server  http://minio-{0...15}.minio-svc.default.svc.cluster.local/export -S /etc/minio/certs/ --address :9000 --console-address :9001\" ]\n          volumeMounts:\n            - name: export\n              mountPath: /export            \n          ports:\n            - name: http\n              containerPort: 9000\n            - name: http-console\n              containerPort: 9001\n          livenessProbe:\n            httpGet:\n              path: /minio/health/live\n              port: 9000\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 30\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n            - name: MINIO_ROOT_USER\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootUser\n            - name: MINIO_ROOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootPassword\n            - name: MINIO_PROMETHEUS_AUTH_TYPE\n              value: \"public\"\n          resources:\n            requests:\n              memory: 16Gi      \n      volumes:\n        - name: minio-user\n          secret:\n            secretName: minio        \n  volumeClaimTemplates:\n    - metadata:\n        name: export\n      spec:\n        accessModes: [ \"ReadWriteOnce\" ]\n        resources:\n          requests:\n            storage: 500Gi\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/statefulset.yaml"}, "region": {"startLine": 25, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\n  namespace: \"default\"\n  labels:\n    app: minio\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  podManagementPolicy: \"Parallel\"\n  serviceName: minio-svc\n  replicas: 16\n  selector:\n    matchLabels:\n      app: minio\n      release: minio\n  template:\n    metadata:\n      name: minio\n      labels:\n        app: minio\n        release: minio\n      annotations:\n        checksum/secrets: fb493769d34bada41f1b25a9f5e0f1aecbd8b5c3059276a7b44297cffdb5ae8e\n        checksum/config: 38e836a40bf068574c583e885605cdea0398da30e818b685b6e8b8d34a1f4790\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n        fsGroupChangePolicy: OnRootMismatch\n\n      serviceAccountName: minio-sa\n      containers:\n        - name: minio\n          image: quay.io/minio/minio:RELEASE.2022-04-30T22-23-53Z\n          imagePullPolicy: IfNotPresent\n\n          command: [ \"/bin/sh\",\n            \"-ce\",\n            \"/usr/bin/docker-entrypoint.sh minio server  http://minio-{0...15}.minio-svc.default.svc.cluster.local/export -S /etc/minio/certs/ --address :9000 --console-address :9001\" ]\n          volumeMounts:\n            - name: export\n              mountPath: /export            \n          ports:\n            - name: http\n              containerPort: 9000\n            - name: http-console\n              containerPort: 9001\n          livenessProbe:\n            httpGet:\n              path: /minio/health/live\n              port: 9000\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 30\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n            - name: MINIO_ROOT_USER\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootUser\n            - name: MINIO_ROOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootPassword\n            - name: MINIO_PROMETHEUS_AUTH_TYPE\n              value: \"public\"\n          resources:\n            requests:\n              memory: 16Gi      \n      volumes:\n        - name: minio-user\n          secret:\n            secretName: minio        \n  volumeClaimTemplates:\n    - metadata:\n        name: export\n      spec:\n        accessModes: [ \"ReadWriteOnce\" ]\n        resources:\n          requests:\n            storage: 500Gi\n"}}}}]}, {"ruleId": "CKV_K8S_35", "ruleIndex": 6, "level": "error", "attachments": [], "message": {"text": "Prefer using secrets as files over secrets as environment variables"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/statefulset.yaml"}, "region": {"startLine": 25, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\n  namespace: \"default\"\n  labels:\n    app: minio\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  podManagementPolicy: \"Parallel\"\n  serviceName: minio-svc\n  replicas: 16\n  selector:\n    matchLabels:\n      app: minio\n      release: minio\n  template:\n    metadata:\n      name: minio\n      labels:\n        app: minio\n        release: minio\n      annotations:\n        checksum/secrets: fb493769d34bada41f1b25a9f5e0f1aecbd8b5c3059276a7b44297cffdb5ae8e\n        checksum/config: 38e836a40bf068574c583e885605cdea0398da30e818b685b6e8b8d34a1f4790\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n        fsGroupChangePolicy: OnRootMismatch\n\n      serviceAccountName: minio-sa\n      containers:\n        - name: minio\n          image: quay.io/minio/minio:RELEASE.2022-04-30T22-23-53Z\n          imagePullPolicy: IfNotPresent\n\n          command: [ \"/bin/sh\",\n            \"-ce\",\n            \"/usr/bin/docker-entrypoint.sh minio server  http://minio-{0...15}.minio-svc.default.svc.cluster.local/export -S /etc/minio/certs/ --address :9000 --console-address :9001\" ]\n          volumeMounts:\n            - name: export\n              mountPath: /export            \n          ports:\n            - name: http\n              containerPort: 9000\n            - name: http-console\n              containerPort: 9001\n          livenessProbe:\n            httpGet:\n              path: /minio/health/live\n              port: 9000\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 30\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n            - name: MINIO_ROOT_USER\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootUser\n            - name: MINIO_ROOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootPassword\n            - name: MINIO_PROMETHEUS_AUTH_TYPE\n              value: \"public\"\n          resources:\n            requests:\n              memory: 16Gi      \n      volumes:\n        - name: minio-user\n          secret:\n            secretName: minio        \n  volumeClaimTemplates:\n    - metadata:\n        name: export\n      spec:\n        accessModes: [ \"ReadWriteOnce\" ]\n        resources:\n          requests:\n            storage: 500Gi\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/statefulset.yaml"}, "region": {"startLine": 25, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\n  namespace: \"default\"\n  labels:\n    app: minio\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  podManagementPolicy: \"Parallel\"\n  serviceName: minio-svc\n  replicas: 16\n  selector:\n    matchLabels:\n      app: minio\n      release: minio\n  template:\n    metadata:\n      name: minio\n      labels:\n        app: minio\n        release: minio\n      annotations:\n        checksum/secrets: fb493769d34bada41f1b25a9f5e0f1aecbd8b5c3059276a7b44297cffdb5ae8e\n        checksum/config: 38e836a40bf068574c583e885605cdea0398da30e818b685b6e8b8d34a1f4790\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n        fsGroupChangePolicy: OnRootMismatch\n\n      serviceAccountName: minio-sa\n      containers:\n        - name: minio\n          image: quay.io/minio/minio:RELEASE.2022-04-30T22-23-53Z\n          imagePullPolicy: IfNotPresent\n\n          command: [ \"/bin/sh\",\n            \"-ce\",\n            \"/usr/bin/docker-entrypoint.sh minio server  http://minio-{0...15}.minio-svc.default.svc.cluster.local/export -S /etc/minio/certs/ --address :9000 --console-address :9001\" ]\n          volumeMounts:\n            - name: export\n              mountPath: /export            \n          ports:\n            - name: http\n              containerPort: 9000\n            - name: http-console\n              containerPort: 9001\n          livenessProbe:\n            httpGet:\n              path: /minio/health/live\n              port: 9000\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 30\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n            - name: MINIO_ROOT_USER\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootUser\n            - name: MINIO_ROOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootPassword\n            - name: MINIO_PROMETHEUS_AUTH_TYPE\n              value: \"public\"\n          resources:\n            requests:\n              memory: 16Gi      \n      volumes:\n        - name: minio-user\n          secret:\n            secretName: minio        \n  volumeClaimTemplates:\n    - metadata:\n        name: export\n      spec:\n        accessModes: [ \"ReadWriteOnce\" ]\n        resources:\n          requests:\n            storage: 500Gi\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/statefulset.yaml"}, "region": {"startLine": 25, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\n  namespace: \"default\"\n  labels:\n    app: minio\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  podManagementPolicy: \"Parallel\"\n  serviceName: minio-svc\n  replicas: 16\n  selector:\n    matchLabels:\n      app: minio\n      release: minio\n  template:\n    metadata:\n      name: minio\n      labels:\n        app: minio\n        release: minio\n      annotations:\n        checksum/secrets: fb493769d34bada41f1b25a9f5e0f1aecbd8b5c3059276a7b44297cffdb5ae8e\n        checksum/config: 38e836a40bf068574c583e885605cdea0398da30e818b685b6e8b8d34a1f4790\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n        fsGroupChangePolicy: OnRootMismatch\n\n      serviceAccountName: minio-sa\n      containers:\n        - name: minio\n          image: quay.io/minio/minio:RELEASE.2022-04-30T22-23-53Z\n          imagePullPolicy: IfNotPresent\n\n          command: [ \"/bin/sh\",\n            \"-ce\",\n            \"/usr/bin/docker-entrypoint.sh minio server  http://minio-{0...15}.minio-svc.default.svc.cluster.local/export -S /etc/minio/certs/ --address :9000 --console-address :9001\" ]\n          volumeMounts:\n            - name: export\n              mountPath: /export            \n          ports:\n            - name: http\n              containerPort: 9000\n            - name: http-console\n              containerPort: 9001\n          livenessProbe:\n            httpGet:\n              path: /minio/health/live\n              port: 9000\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 30\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n            - name: MINIO_ROOT_USER\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootUser\n            - name: MINIO_ROOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootPassword\n            - name: MINIO_PROMETHEUS_AUTH_TYPE\n              value: \"public\"\n          resources:\n            requests:\n              memory: 16Gi      \n      volumes:\n        - name: minio-user\n          secret:\n            secretName: minio        \n  volumeClaimTemplates:\n    - metadata:\n        name: export\n      spec:\n        accessModes: [ \"ReadWriteOnce\" ]\n        resources:\n          requests:\n            storage: 500Gi\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/statefulset.yaml"}, "region": {"startLine": 25, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\n  namespace: \"default\"\n  labels:\n    app: minio\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  podManagementPolicy: \"Parallel\"\n  serviceName: minio-svc\n  replicas: 16\n  selector:\n    matchLabels:\n      app: minio\n      release: minio\n  template:\n    metadata:\n      name: minio\n      labels:\n        app: minio\n        release: minio\n      annotations:\n        checksum/secrets: fb493769d34bada41f1b25a9f5e0f1aecbd8b5c3059276a7b44297cffdb5ae8e\n        checksum/config: 38e836a40bf068574c583e885605cdea0398da30e818b685b6e8b8d34a1f4790\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n        fsGroupChangePolicy: OnRootMismatch\n\n      serviceAccountName: minio-sa\n      containers:\n        - name: minio\n          image: quay.io/minio/minio:RELEASE.2022-04-30T22-23-53Z\n          imagePullPolicy: IfNotPresent\n\n          command: [ \"/bin/sh\",\n            \"-ce\",\n            \"/usr/bin/docker-entrypoint.sh minio server  http://minio-{0...15}.minio-svc.default.svc.cluster.local/export -S /etc/minio/certs/ --address :9000 --console-address :9001\" ]\n          volumeMounts:\n            - name: export\n              mountPath: /export            \n          ports:\n            - name: http\n              containerPort: 9000\n            - name: http-console\n              containerPort: 9001\n          livenessProbe:\n            httpGet:\n              path: /minio/health/live\n              port: 9000\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 30\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n            - name: MINIO_ROOT_USER\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootUser\n            - name: MINIO_ROOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootPassword\n            - name: MINIO_PROMETHEUS_AUTH_TYPE\n              value: \"public\"\n          resources:\n            requests:\n              memory: 16Gi      \n      volumes:\n        - name: minio-user\n          secret:\n            secretName: minio        \n  volumeClaimTemplates:\n    - metadata:\n        name: export\n      spec:\n        accessModes: [ \"ReadWriteOnce\" ]\n        resources:\n          requests:\n            storage: 500Gi\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/statefulset.yaml"}, "region": {"startLine": 25, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\n  namespace: \"default\"\n  labels:\n    app: minio\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  podManagementPolicy: \"Parallel\"\n  serviceName: minio-svc\n  replicas: 16\n  selector:\n    matchLabels:\n      app: minio\n      release: minio\n  template:\n    metadata:\n      name: minio\n      labels:\n        app: minio\n        release: minio\n      annotations:\n        checksum/secrets: fb493769d34bada41f1b25a9f5e0f1aecbd8b5c3059276a7b44297cffdb5ae8e\n        checksum/config: 38e836a40bf068574c583e885605cdea0398da30e818b685b6e8b8d34a1f4790\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n        fsGroupChangePolicy: OnRootMismatch\n\n      serviceAccountName: minio-sa\n      containers:\n        - name: minio\n          image: quay.io/minio/minio:RELEASE.2022-04-30T22-23-53Z\n          imagePullPolicy: IfNotPresent\n\n          command: [ \"/bin/sh\",\n            \"-ce\",\n            \"/usr/bin/docker-entrypoint.sh minio server  http://minio-{0...15}.minio-svc.default.svc.cluster.local/export -S /etc/minio/certs/ --address :9000 --console-address :9001\" ]\n          volumeMounts:\n            - name: export\n              mountPath: /export            \n          ports:\n            - name: http\n              containerPort: 9000\n            - name: http-console\n              containerPort: 9001\n          livenessProbe:\n            httpGet:\n              path: /minio/health/live\n              port: 9000\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 30\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n            - name: MINIO_ROOT_USER\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootUser\n            - name: MINIO_ROOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootPassword\n            - name: MINIO_PROMETHEUS_AUTH_TYPE\n              value: \"public\"\n          resources:\n            requests:\n              memory: 16Gi      \n      volumes:\n        - name: minio-user\n          secret:\n            secretName: minio        \n  volumeClaimTemplates:\n    - metadata:\n        name: export\n      spec:\n        accessModes: [ \"ReadWriteOnce\" ]\n        resources:\n          requests:\n            storage: 500Gi\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/statefulset.yaml"}, "region": {"startLine": 25, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\n  namespace: \"default\"\n  labels:\n    app: minio\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  podManagementPolicy: \"Parallel\"\n  serviceName: minio-svc\n  replicas: 16\n  selector:\n    matchLabels:\n      app: minio\n      release: minio\n  template:\n    metadata:\n      name: minio\n      labels:\n        app: minio\n        release: minio\n      annotations:\n        checksum/secrets: fb493769d34bada41f1b25a9f5e0f1aecbd8b5c3059276a7b44297cffdb5ae8e\n        checksum/config: 38e836a40bf068574c583e885605cdea0398da30e818b685b6e8b8d34a1f4790\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n        fsGroupChangePolicy: OnRootMismatch\n\n      serviceAccountName: minio-sa\n      containers:\n        - name: minio\n          image: quay.io/minio/minio:RELEASE.2022-04-30T22-23-53Z\n          imagePullPolicy: IfNotPresent\n\n          command: [ \"/bin/sh\",\n            \"-ce\",\n            \"/usr/bin/docker-entrypoint.sh minio server  http://minio-{0...15}.minio-svc.default.svc.cluster.local/export -S /etc/minio/certs/ --address :9000 --console-address :9001\" ]\n          volumeMounts:\n            - name: export\n              mountPath: /export            \n          ports:\n            - name: http\n              containerPort: 9000\n            - name: http-console\n              containerPort: 9001\n          livenessProbe:\n            httpGet:\n              path: /minio/health/live\n              port: 9000\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 30\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n            - name: MINIO_ROOT_USER\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootUser\n            - name: MINIO_ROOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootPassword\n            - name: MINIO_PROMETHEUS_AUTH_TYPE\n              value: \"public\"\n          resources:\n            requests:\n              memory: 16Gi      \n      volumes:\n        - name: minio-user\n          secret:\n            secretName: minio        \n  volumeClaimTemplates:\n    - metadata:\n        name: export\n      spec:\n        accessModes: [ \"ReadWriteOnce\" ]\n        resources:\n          requests:\n            storage: 500Gi\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/statefulset.yaml"}, "region": {"startLine": 25, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: minio\n  namespace: \"default\"\n  labels:\n    app: minio\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  podManagementPolicy: \"Parallel\"\n  serviceName: minio-svc\n  replicas: 16\n  selector:\n    matchLabels:\n      app: minio\n      release: minio\n  template:\n    metadata:\n      name: minio\n      labels:\n        app: minio\n        release: minio\n      annotations:\n        checksum/secrets: fb493769d34bada41f1b25a9f5e0f1aecbd8b5c3059276a7b44297cffdb5ae8e\n        checksum/config: 38e836a40bf068574c583e885605cdea0398da30e818b685b6e8b8d34a1f4790\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n        fsGroupChangePolicy: OnRootMismatch\n\n      serviceAccountName: minio-sa\n      containers:\n        - name: minio\n          image: quay.io/minio/minio:RELEASE.2022-04-30T22-23-53Z\n          imagePullPolicy: IfNotPresent\n\n          command: [ \"/bin/sh\",\n            \"-ce\",\n            \"/usr/bin/docker-entrypoint.sh minio server  http://minio-{0...15}.minio-svc.default.svc.cluster.local/export -S /etc/minio/certs/ --address :9000 --console-address :9001\" ]\n          volumeMounts:\n            - name: export\n              mountPath: /export            \n          ports:\n            - name: http\n              containerPort: 9000\n            - name: http-console\n              containerPort: 9001\n          livenessProbe:\n            httpGet:\n              path: /minio/health/live\n              port: 9000\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 30\n            timeoutSeconds: 1\n            successThreshold: 1\n            failureThreshold: 3\n          env:\n            - name: MINIO_ROOT_USER\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootUser\n            - name: MINIO_ROOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: minio\n                  key: rootPassword\n            - name: MINIO_PROMETHEUS_AUTH_TYPE\n              value: \"public\"\n          resources:\n            requests:\n              memory: 16Gi      \n      volumes:\n        - name: minio-user\n          secret:\n            secretName: minio        \n  volumeClaimTemplates:\n    - metadata:\n        name: export\n      spec:\n        accessModes: [ \"ReadWriteOnce\" ]\n        resources:\n          requests:\n            storage: 500Gi\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/minio/templates/console-service.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: minio-console\n  namespace: \"default\"\n  labels:\n    app: minio\n    chart: minio-4.0.1\n    release: minio\n    heritage: Helm\nspec:\n  type: ClusterIP\n  ports:\n    - name: http\n      port: 9001\n      protocol: TCP\n      targetPort: 9001\n  selector:\n    app: minio\n    release: minio\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault-secrets-webhook/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 64, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: vault-secrets-webhook-1.15.11\n    app.kubernetes.io/name: vault-secrets-webhook\n    app.kubernetes.io/instance: vault-secrets-webhook\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: mutating-webhook\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-secrets-webhook\n      app.kubernetes.io/instance: vault-secrets-webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-secrets-webhook\n        app.kubernetes.io/instance: vault-secrets-webhook\n        security.banzaicloud.io/mutate: skip\n      annotations:\n        checksum/config: 289f9a074d80fd4eb8e08ce525939e50a9be2dbbb822a87845e93f63ef0f19a9\n    spec:\n      serviceAccountName: vault-secrets-webhook\n      volumes:\n        - name: serving-cert\n          secret:\n            defaultMode: 420\n            secretName: vault-secrets-webhook-webhook-tls\n      containers:\n        - name: vault-secrets-webhook\n          image: \"ghcr.io/banzaicloud/vault-secrets-webhook:1.15.3\"\n          env:\n            - name: TLS_CERT_FILE\n              value: /var/serving-cert/tls.crt\n            - name: TLS_PRIVATE_KEY_FILE\n              value: /var/serving-cert/tls.key\n            - name: LISTEN_ADDRESS\n              value: \":8443\"\n            - name: VAULT_ENV_IMAGE\n              value: \"ghcr.io/banzaicloud/vault-env:1.15.3\"\n            - name: VAULT_IMAGE\n              value: \"vault:1.6.2\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - containerPort: 8443\n          readinessProbe:\n            httpGet:\n              scheme: HTTPS\n              path: /healthz\n              port: 8443\n          volumeMounts:\n            - mountPath: /var/serving-cert\n              name: serving-cert\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 65534\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault-secrets-webhook/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 64, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: vault-secrets-webhook-1.15.11\n    app.kubernetes.io/name: vault-secrets-webhook\n    app.kubernetes.io/instance: vault-secrets-webhook\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: mutating-webhook\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-secrets-webhook\n      app.kubernetes.io/instance: vault-secrets-webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-secrets-webhook\n        app.kubernetes.io/instance: vault-secrets-webhook\n        security.banzaicloud.io/mutate: skip\n      annotations:\n        checksum/config: 289f9a074d80fd4eb8e08ce525939e50a9be2dbbb822a87845e93f63ef0f19a9\n    spec:\n      serviceAccountName: vault-secrets-webhook\n      volumes:\n        - name: serving-cert\n          secret:\n            defaultMode: 420\n            secretName: vault-secrets-webhook-webhook-tls\n      containers:\n        - name: vault-secrets-webhook\n          image: \"ghcr.io/banzaicloud/vault-secrets-webhook:1.15.3\"\n          env:\n            - name: TLS_CERT_FILE\n              value: /var/serving-cert/tls.crt\n            - name: TLS_PRIVATE_KEY_FILE\n              value: /var/serving-cert/tls.key\n            - name: LISTEN_ADDRESS\n              value: \":8443\"\n            - name: VAULT_ENV_IMAGE\n              value: \"ghcr.io/banzaicloud/vault-env:1.15.3\"\n            - name: VAULT_IMAGE\n              value: \"vault:1.6.2\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - containerPort: 8443\n          readinessProbe:\n            httpGet:\n              scheme: HTTPS\n              path: /healthz\n              port: 8443\n          volumeMounts:\n            - mountPath: /var/serving-cert\n              name: serving-cert\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 65534\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault-secrets-webhook/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 64, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: vault-secrets-webhook-1.15.11\n    app.kubernetes.io/name: vault-secrets-webhook\n    app.kubernetes.io/instance: vault-secrets-webhook\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: mutating-webhook\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-secrets-webhook\n      app.kubernetes.io/instance: vault-secrets-webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-secrets-webhook\n        app.kubernetes.io/instance: vault-secrets-webhook\n        security.banzaicloud.io/mutate: skip\n      annotations:\n        checksum/config: 289f9a074d80fd4eb8e08ce525939e50a9be2dbbb822a87845e93f63ef0f19a9\n    spec:\n      serviceAccountName: vault-secrets-webhook\n      volumes:\n        - name: serving-cert\n          secret:\n            defaultMode: 420\n            secretName: vault-secrets-webhook-webhook-tls\n      containers:\n        - name: vault-secrets-webhook\n          image: \"ghcr.io/banzaicloud/vault-secrets-webhook:1.15.3\"\n          env:\n            - name: TLS_CERT_FILE\n              value: /var/serving-cert/tls.crt\n            - name: TLS_PRIVATE_KEY_FILE\n              value: /var/serving-cert/tls.key\n            - name: LISTEN_ADDRESS\n              value: \":8443\"\n            - name: VAULT_ENV_IMAGE\n              value: \"ghcr.io/banzaicloud/vault-env:1.15.3\"\n            - name: VAULT_IMAGE\n              value: \"vault:1.6.2\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - containerPort: 8443\n          readinessProbe:\n            httpGet:\n              scheme: HTTPS\n              path: /healthz\n              port: 8443\n          volumeMounts:\n            - mountPath: /var/serving-cert\n              name: serving-cert\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 65534\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault-secrets-webhook/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 64, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: vault-secrets-webhook-1.15.11\n    app.kubernetes.io/name: vault-secrets-webhook\n    app.kubernetes.io/instance: vault-secrets-webhook\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: mutating-webhook\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-secrets-webhook\n      app.kubernetes.io/instance: vault-secrets-webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-secrets-webhook\n        app.kubernetes.io/instance: vault-secrets-webhook\n        security.banzaicloud.io/mutate: skip\n      annotations:\n        checksum/config: 289f9a074d80fd4eb8e08ce525939e50a9be2dbbb822a87845e93f63ef0f19a9\n    spec:\n      serviceAccountName: vault-secrets-webhook\n      volumes:\n        - name: serving-cert\n          secret:\n            defaultMode: 420\n            secretName: vault-secrets-webhook-webhook-tls\n      containers:\n        - name: vault-secrets-webhook\n          image: \"ghcr.io/banzaicloud/vault-secrets-webhook:1.15.3\"\n          env:\n            - name: TLS_CERT_FILE\n              value: /var/serving-cert/tls.crt\n            - name: TLS_PRIVATE_KEY_FILE\n              value: /var/serving-cert/tls.key\n            - name: LISTEN_ADDRESS\n              value: \":8443\"\n            - name: VAULT_ENV_IMAGE\n              value: \"ghcr.io/banzaicloud/vault-env:1.15.3\"\n            - name: VAULT_IMAGE\n              value: \"vault:1.6.2\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - containerPort: 8443\n          readinessProbe:\n            httpGet:\n              scheme: HTTPS\n              path: /healthz\n              port: 8443\n          volumeMounts:\n            - mountPath: /var/serving-cert\n              name: serving-cert\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 65534\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault-secrets-webhook/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 64, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: vault-secrets-webhook-1.15.11\n    app.kubernetes.io/name: vault-secrets-webhook\n    app.kubernetes.io/instance: vault-secrets-webhook\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: mutating-webhook\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-secrets-webhook\n      app.kubernetes.io/instance: vault-secrets-webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-secrets-webhook\n        app.kubernetes.io/instance: vault-secrets-webhook\n        security.banzaicloud.io/mutate: skip\n      annotations:\n        checksum/config: 289f9a074d80fd4eb8e08ce525939e50a9be2dbbb822a87845e93f63ef0f19a9\n    spec:\n      serviceAccountName: vault-secrets-webhook\n      volumes:\n        - name: serving-cert\n          secret:\n            defaultMode: 420\n            secretName: vault-secrets-webhook-webhook-tls\n      containers:\n        - name: vault-secrets-webhook\n          image: \"ghcr.io/banzaicloud/vault-secrets-webhook:1.15.3\"\n          env:\n            - name: TLS_CERT_FILE\n              value: /var/serving-cert/tls.crt\n            - name: TLS_PRIVATE_KEY_FILE\n              value: /var/serving-cert/tls.key\n            - name: LISTEN_ADDRESS\n              value: \":8443\"\n            - name: VAULT_ENV_IMAGE\n              value: \"ghcr.io/banzaicloud/vault-env:1.15.3\"\n            - name: VAULT_IMAGE\n              value: \"vault:1.6.2\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - containerPort: 8443\n          readinessProbe:\n            httpGet:\n              scheme: HTTPS\n              path: /healthz\n              port: 8443\n          volumeMounts:\n            - mountPath: /var/serving-cert\n              name: serving-cert\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 65534\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault-secrets-webhook/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 64, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: vault-secrets-webhook-1.15.11\n    app.kubernetes.io/name: vault-secrets-webhook\n    app.kubernetes.io/instance: vault-secrets-webhook\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: mutating-webhook\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-secrets-webhook\n      app.kubernetes.io/instance: vault-secrets-webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-secrets-webhook\n        app.kubernetes.io/instance: vault-secrets-webhook\n        security.banzaicloud.io/mutate: skip\n      annotations:\n        checksum/config: 289f9a074d80fd4eb8e08ce525939e50a9be2dbbb822a87845e93f63ef0f19a9\n    spec:\n      serviceAccountName: vault-secrets-webhook\n      volumes:\n        - name: serving-cert\n          secret:\n            defaultMode: 420\n            secretName: vault-secrets-webhook-webhook-tls\n      containers:\n        - name: vault-secrets-webhook\n          image: \"ghcr.io/banzaicloud/vault-secrets-webhook:1.15.3\"\n          env:\n            - name: TLS_CERT_FILE\n              value: /var/serving-cert/tls.crt\n            - name: TLS_PRIVATE_KEY_FILE\n              value: /var/serving-cert/tls.key\n            - name: LISTEN_ADDRESS\n              value: \":8443\"\n            - name: VAULT_ENV_IMAGE\n              value: \"ghcr.io/banzaicloud/vault-env:1.15.3\"\n            - name: VAULT_IMAGE\n              value: \"vault:1.6.2\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - containerPort: 8443\n          readinessProbe:\n            httpGet:\n              scheme: HTTPS\n              path: /healthz\n              port: 8443\n          volumeMounts:\n            - mountPath: /var/serving-cert\n              name: serving-cert\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 65534\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault-secrets-webhook/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 64, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: vault-secrets-webhook-1.15.11\n    app.kubernetes.io/name: vault-secrets-webhook\n    app.kubernetes.io/instance: vault-secrets-webhook\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: mutating-webhook\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-secrets-webhook\n      app.kubernetes.io/instance: vault-secrets-webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-secrets-webhook\n        app.kubernetes.io/instance: vault-secrets-webhook\n        security.banzaicloud.io/mutate: skip\n      annotations:\n        checksum/config: 289f9a074d80fd4eb8e08ce525939e50a9be2dbbb822a87845e93f63ef0f19a9\n    spec:\n      serviceAccountName: vault-secrets-webhook\n      volumes:\n        - name: serving-cert\n          secret:\n            defaultMode: 420\n            secretName: vault-secrets-webhook-webhook-tls\n      containers:\n        - name: vault-secrets-webhook\n          image: \"ghcr.io/banzaicloud/vault-secrets-webhook:1.15.3\"\n          env:\n            - name: TLS_CERT_FILE\n              value: /var/serving-cert/tls.crt\n            - name: TLS_PRIVATE_KEY_FILE\n              value: /var/serving-cert/tls.key\n            - name: LISTEN_ADDRESS\n              value: \":8443\"\n            - name: VAULT_ENV_IMAGE\n              value: \"ghcr.io/banzaicloud/vault-env:1.15.3\"\n            - name: VAULT_IMAGE\n              value: \"vault:1.6.2\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - containerPort: 8443\n          readinessProbe:\n            httpGet:\n              scheme: HTTPS\n              path: /healthz\n              port: 8443\n          volumeMounts:\n            - mountPath: /var/serving-cert\n              name: serving-cert\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 65534\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault-secrets-webhook/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 64, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: vault-secrets-webhook-1.15.11\n    app.kubernetes.io/name: vault-secrets-webhook\n    app.kubernetes.io/instance: vault-secrets-webhook\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: mutating-webhook\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-secrets-webhook\n      app.kubernetes.io/instance: vault-secrets-webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-secrets-webhook\n        app.kubernetes.io/instance: vault-secrets-webhook\n        security.banzaicloud.io/mutate: skip\n      annotations:\n        checksum/config: 289f9a074d80fd4eb8e08ce525939e50a9be2dbbb822a87845e93f63ef0f19a9\n    spec:\n      serviceAccountName: vault-secrets-webhook\n      volumes:\n        - name: serving-cert\n          secret:\n            defaultMode: 420\n            secretName: vault-secrets-webhook-webhook-tls\n      containers:\n        - name: vault-secrets-webhook\n          image: \"ghcr.io/banzaicloud/vault-secrets-webhook:1.15.3\"\n          env:\n            - name: TLS_CERT_FILE\n              value: /var/serving-cert/tls.crt\n            - name: TLS_PRIVATE_KEY_FILE\n              value: /var/serving-cert/tls.key\n            - name: LISTEN_ADDRESS\n              value: \":8443\"\n            - name: VAULT_ENV_IMAGE\n              value: \"ghcr.io/banzaicloud/vault-env:1.15.3\"\n            - name: VAULT_IMAGE\n              value: \"vault:1.6.2\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - containerPort: 8443\n          readinessProbe:\n            httpGet:\n              scheme: HTTPS\n              path: /healthz\n              port: 8443\n          volumeMounts:\n            - mountPath: /var/serving-cert\n              name: serving-cert\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 65534\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault-secrets-webhook/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 64, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: vault-secrets-webhook-1.15.11\n    app.kubernetes.io/name: vault-secrets-webhook\n    app.kubernetes.io/instance: vault-secrets-webhook\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: mutating-webhook\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-secrets-webhook\n      app.kubernetes.io/instance: vault-secrets-webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-secrets-webhook\n        app.kubernetes.io/instance: vault-secrets-webhook\n        security.banzaicloud.io/mutate: skip\n      annotations:\n        checksum/config: 289f9a074d80fd4eb8e08ce525939e50a9be2dbbb822a87845e93f63ef0f19a9\n    spec:\n      serviceAccountName: vault-secrets-webhook\n      volumes:\n        - name: serving-cert\n          secret:\n            defaultMode: 420\n            secretName: vault-secrets-webhook-webhook-tls\n      containers:\n        - name: vault-secrets-webhook\n          image: \"ghcr.io/banzaicloud/vault-secrets-webhook:1.15.3\"\n          env:\n            - name: TLS_CERT_FILE\n              value: /var/serving-cert/tls.crt\n            - name: TLS_PRIVATE_KEY_FILE\n              value: /var/serving-cert/tls.key\n            - name: LISTEN_ADDRESS\n              value: \":8443\"\n            - name: VAULT_ENV_IMAGE\n              value: \"ghcr.io/banzaicloud/vault-env:1.15.3\"\n            - name: VAULT_IMAGE\n              value: \"vault:1.6.2\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - containerPort: 8443\n          readinessProbe:\n            httpGet:\n              scheme: HTTPS\n              path: /healthz\n              port: 8443\n          volumeMounts:\n            - mountPath: /var/serving-cert\n              name: serving-cert\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 65534\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault-secrets-webhook/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 64, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: vault-secrets-webhook-1.15.11\n    app.kubernetes.io/name: vault-secrets-webhook\n    app.kubernetes.io/instance: vault-secrets-webhook\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: mutating-webhook\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-secrets-webhook\n      app.kubernetes.io/instance: vault-secrets-webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-secrets-webhook\n        app.kubernetes.io/instance: vault-secrets-webhook\n        security.banzaicloud.io/mutate: skip\n      annotations:\n        checksum/config: 289f9a074d80fd4eb8e08ce525939e50a9be2dbbb822a87845e93f63ef0f19a9\n    spec:\n      serviceAccountName: vault-secrets-webhook\n      volumes:\n        - name: serving-cert\n          secret:\n            defaultMode: 420\n            secretName: vault-secrets-webhook-webhook-tls\n      containers:\n        - name: vault-secrets-webhook\n          image: \"ghcr.io/banzaicloud/vault-secrets-webhook:1.15.3\"\n          env:\n            - name: TLS_CERT_FILE\n              value: /var/serving-cert/tls.crt\n            - name: TLS_PRIVATE_KEY_FILE\n              value: /var/serving-cert/tls.key\n            - name: LISTEN_ADDRESS\n              value: \":8443\"\n            - name: VAULT_ENV_IMAGE\n              value: \"ghcr.io/banzaicloud/vault-env:1.15.3\"\n            - name: VAULT_IMAGE\n              value: \"vault:1.6.2\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - containerPort: 8443\n          readinessProbe:\n            httpGet:\n              scheme: HTTPS\n              path: /healthz\n              port: 8443\n          volumeMounts:\n            - mountPath: /var/serving-cert\n              name: serving-cert\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 65534\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault-secrets-webhook/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 64, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: vault-secrets-webhook-1.15.11\n    app.kubernetes.io/name: vault-secrets-webhook\n    app.kubernetes.io/instance: vault-secrets-webhook\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: mutating-webhook\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-secrets-webhook\n      app.kubernetes.io/instance: vault-secrets-webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-secrets-webhook\n        app.kubernetes.io/instance: vault-secrets-webhook\n        security.banzaicloud.io/mutate: skip\n      annotations:\n        checksum/config: 289f9a074d80fd4eb8e08ce525939e50a9be2dbbb822a87845e93f63ef0f19a9\n    spec:\n      serviceAccountName: vault-secrets-webhook\n      volumes:\n        - name: serving-cert\n          secret:\n            defaultMode: 420\n            secretName: vault-secrets-webhook-webhook-tls\n      containers:\n        - name: vault-secrets-webhook\n          image: \"ghcr.io/banzaicloud/vault-secrets-webhook:1.15.3\"\n          env:\n            - name: TLS_CERT_FILE\n              value: /var/serving-cert/tls.crt\n            - name: TLS_PRIVATE_KEY_FILE\n              value: /var/serving-cert/tls.key\n            - name: LISTEN_ADDRESS\n              value: \":8443\"\n            - name: VAULT_ENV_IMAGE\n              value: \"ghcr.io/banzaicloud/vault-env:1.15.3\"\n            - name: VAULT_IMAGE\n              value: \"vault:1.6.2\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - containerPort: 8443\n          readinessProbe:\n            httpGet:\n              scheme: HTTPS\n              path: /healthz\n              port: 8443\n          volumeMounts:\n            - mountPath: /var/serving-cert\n              name: serving-cert\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 65534\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault-secrets-webhook/templates/webhook-deployment.yaml"}, "region": {"startLine": 3, "endLine": 64, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: vault-secrets-webhook-1.15.11\n    app.kubernetes.io/name: vault-secrets-webhook\n    app.kubernetes.io/instance: vault-secrets-webhook\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: mutating-webhook\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-secrets-webhook\n      app.kubernetes.io/instance: vault-secrets-webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-secrets-webhook\n        app.kubernetes.io/instance: vault-secrets-webhook\n        security.banzaicloud.io/mutate: skip\n      annotations:\n        checksum/config: 289f9a074d80fd4eb8e08ce525939e50a9be2dbbb822a87845e93f63ef0f19a9\n    spec:\n      serviceAccountName: vault-secrets-webhook\n      volumes:\n        - name: serving-cert\n          secret:\n            defaultMode: 420\n            secretName: vault-secrets-webhook-webhook-tls\n      containers:\n        - name: vault-secrets-webhook\n          image: \"ghcr.io/banzaicloud/vault-secrets-webhook:1.15.3\"\n          env:\n            - name: TLS_CERT_FILE\n              value: /var/serving-cert/tls.crt\n            - name: TLS_PRIVATE_KEY_FILE\n              value: /var/serving-cert/tls.key\n            - name: LISTEN_ADDRESS\n              value: \":8443\"\n            - name: VAULT_ENV_IMAGE\n              value: \"ghcr.io/banzaicloud/vault-env:1.15.3\"\n            - name: VAULT_IMAGE\n              value: \"vault:1.6.2\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - containerPort: 8443\n          readinessProbe:\n            httpGet:\n              scheme: HTTPS\n              path: /healthz\n              port: 8443\n          volumeMounts:\n            - mountPath: /var/serving-cert\n              name: serving-cert\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 65534\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault-secrets-webhook/templates/webhook-service.yaml"}, "region": {"startLine": 3, "endLine": 23, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: vault-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: vault-secrets-webhook-1.15.11\n    app.kubernetes.io/name: vault-secrets-webhook\n    app.kubernetes.io/instance: vault-secrets-webhook\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: mutating-webhook  \nspec:\n  type: ClusterIP\n  ports:\n    - port: 443\n      targetPort: 8443\n      protocol: TCP\n      name: vault-secrets-webhook\n  selector:\n    app.kubernetes.io/name: vault-secrets-webhook\n    app.kubernetes.io/instance: vault-secrets-webhook\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault-secrets-webhook/templates/apiservice-webhook.yaml"}, "region": {"startLine": 3, "endLine": 12, "snippet": {"text": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: vault-secrets-webhook-webhook-tls\n  namespace: default\ndata:\n  tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURqRENDQW5TZ0F3SUJBZ0lRWm5lb0VNSU1vYU43TGNLbXJrazJjREFOQmdrcWhraUc5dzBCQVFzRkFEQVYKTVJNd0VRWURWUVFERXdwemRtTXRZMkYwTFdOaE1CNFhEVEl6TURnd09URTVNREl4TTFvWERUSTBNRGd3T0RFNQpNREl4TTFvd0xERXFNQ2dHQTFVRUF4TWhkbUYxYkhRdGMyVmpjbVYwY3kxM1pXSm9iMjlyTG1SbFptRjFiSFF1CmMzWmpNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQTNMc0lpRUVyYlpCbkY3SWoKQ2tOdlFoRmJ2SmNyc1hYSHIvWXFGbkgvcGprQk5HemRFMTZsa3NQTWZVSlBRTFJ2UjBiVVErMkZINUkybmFNWQo1VHhERkVqOWZML2hkSFBiNFBnd3lTMHYzcHZCK3g1Z3JSOW90VU9aMFB2Wm1vU2cveUo2WVQ1RFV6RTdrdnRBCmtqTHB5YlQwZGVUV2Fua1B1dkwyQ0dLVlhEdjJPcThKTEdBVWVaZk9HVkNESHhValRnczFLcVRacVg5UmtEV3cKZDB3dHF6SGQyY2EvSVFYMGFQYlAxbXVqNC9tN1B4Ui9QdEJ0U1lRWDYzMmFsL3daNWpkRnZ6cnFWS3BpV3JuagpqbENXRC9SMTJzcVYxa3hveDFNS3BlSzJ5SktObUMzMC9VZHI1cG93UEZ4ajhDRUlNalQ3V1JYVVFJWDdvVU4zCjNYZjgwd0lEQVFBQm80SEFNSUc5TUE0R0ExVWREd0VCL3dRRUF3SUZvREFkQmdOVkhTVUVGakFVQmdnckJnRUYKQlFjREFRWUlLd1lCQlFVSEF3SXdEQVlEVlIwVEFRSC9CQUl3QURBZkJnTlZIU01FR0RBV2dCU3pSTmdmTVJGUwpBUmlETm01aU9VcjlrVmtia0RCZEJnTlZIUkVFVmpCVWdpOTJZWFZzZEMxelpXTnlaWFJ6TFhkbFltaHZiMnN1ClpHVm1ZWFZzZEM1emRtTXVZMngxYzNSbGNpNXNiMk5oYklJaGRtRjFiSFF0YzJWamNtVjBjeTEzWldKb2IyOXIKTG1SbFptRjFiSFF1YzNaak1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQVJUb0ZlU0drajFNY2RoSnUvMHlTVApVUTRwZFg1alJwK1pGV3AxQXhhSEY3T1FEYnRkKzBXdjdZTGNSM1RmenpxUnlaaDBiWktkK3N6OW9DbFJpREJLCmNUWTQ3OHJuR1hFTkg3clBpdkJEeXFoZ1RUdG9VcWNSS1FwK085Q3ZQTFY2UWRBa0tJcE9HZ2pwb1Z4SVFaaXcKSU9obkt4TjZCbTJhOFhzM2pnd1BFNFMwZmtndHdmNnVKUnBxZi9JakdOd0Q3d0ZxRXdmbnE4cGNaczNoSGZTego4NlRjSVRHVHNBVnd0TXNUUjhQWThIZlQrUlNXaVgxVFdVYWVSSXBqRFUxdjNXSmpNenAvM2E3S0IrWHFZM0puCk9VU2F2Q3Bha1ZtSjZLN21pQVZZNVMyYTVjOWY1cnd5NkgzbmFhR1gxTHE2Nm5Uc0xJSkxHS1QxcC93V280M1cKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=\n  tls.key: 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\n  ca.crt:  LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURGakNDQWY2Z0F3SUJBZ0lSQVBrREQrR0hCQ3M5NUpObHpybkg4R1l3RFFZSktvWklodmNOQVFFTEJRQXcKRlRFVE1CRUdBMVVFQXhNS2MzWmpMV05oZEMxallUQWVGdzB5TXpBNE1Ea3hPVEF5TVROYUZ3MHpNekE0TURZeApPVEF5TVROYU1CVXhFekFSQmdOVkJBTVRDbk4yWXkxallYUXRZMkV3Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBCkE0SUJEd0F3Z2dFS0FvSUJBUUNsTTBMV0tSSFFUSDRrZ3hqcHVkTGNlaDNsd3M3UkRFWjVBNGh2YmVqVERXN1oKYSs2anNZQXlpNjh6eVBKNEo2RzRKZ010dThvSHZ5M1pxSVQ0WFgwV05sRUdyWjM0ZksraGx4U2Q0QkRYcVZNRwp6Z2dLc0VUNWJNcnBlRzl5WUVDL0gydHhaLzY0L1ZRU1RYVnZ2OEw2dlpYY29HVkxIWmZFTTBvZUdvWFUyS0daCnpQRkZSOVByeHNrTmF0WTNsTXFSZ3NQTUxmWm5uYTRhclg0SWFYQnN4aC9XdVVjbWpZd1AvM00vM0NZckpmdUwKMVl2TFEzckh5VWlBYVRvK2RWTXpFVWQ5aFZhcXVweWtham8vU2p6ZVJpdnBCOVFZZUdOeUdLVXQ2Z0hkM3pJWApuVGRnbERDZWNPRkhNa0s3MWtoenZRbndtajNkeFNQZXU4dFB1TjQ5QWdNQkFBR2pZVEJmTUE0R0ExVWREd0VCCi93UUVBd0lDcERBZEJnTlZIU1VFRmpBVUJnZ3JCZ0VGQlFjREFRWUlLd1lCQlFVSEF3SXdEd1lEVlIwVEFRSC8KQkFVd0F3RUIvekFkQmdOVkhRNEVGZ1FVczBUWUh6RVJVZ0VZZ3padVlqbEsvWkZaRzVBd0RRWUpLb1pJaHZjTgpBUUVMQlFBRGdnRUJBQ3g1NURTTTlDM3llUWlkK2IzUWZOTGdiVUNjZGo3clBCWUY1OGt3MEFibisvL2FjMnd2CjQ0SkxyYzl5TllkTzN6Y1hMMXdNaG9UdFlSOWh2YklsaFNIRll3R0V0UEd3OEk4Y0MrblpxQk5SVVYvSndudzUKUjBoZTlkazNkZFQ4K0R1VTFxTUFhUVJKRVk2Q05VcGVtMHRaYWs3WkxqSk1LWkpvWlEvbmYvTng1U3hxcWFtUQpzUmVramdIeUU4K2czTXpKdmRsTmFBa0cxQlpCWXVtK1pGamYwMGpMbHd4MElNaGIxZ2NjZnE5dTU2ZUNEekRJCmk4YUVuOGlPWUUzUjU0OWJiTm15TVd2M1cwMERqNnd6bzA3T3B6TGljUVRuSXdTMXNKQ212czhzcm52SUErTUMKSWd5am9tMis3NGQwMUlyZVRNbGZFcHhuRndaWWJueFVxQjA9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault-secrets-webhook/templates/webhook-rbac.yaml"}, "region": {"startLine": 3, "endLine": 13, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: vault-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: vault-secrets-webhook-1.15.11\n    app.kubernetes.io/name: vault-secrets-webhook\n    app.kubernetes.io/instance: vault-secrets-webhook\n    app.kubernetes.io/managed-by: Helm\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/zookeeper/templates/service.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: zookeeper\n  labels:\n    app: zookeeper\n    chart: zookeeper-0.1.0\n    release: zookeeper\n    heritage: Helm\nspec:\n  type: \n  ports:\n    - port: 2181\n      name: client\n  selector:\n    app: zookeeper\n    release: zookeeper\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/zookeeper/templates/headless-service.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: zookeeper-headless\n  labels:\n    app: zookeeper\n    chart: zookeeper-0.1.0\n    release: zookeeper\n    heritage: Helm\nspec:\n  ports:\n    - port: 2888\n      name: server\n    - port: 3888\n      name: leader-election\n  clusterIP: None\n  selector:\n    app: zookeeper\n    release: zookeeper\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/zookeeper/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper\n  labels:\n    app: zookeeper\n    chart: zookeeper-0.1.0\n    release: zookeeper\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: zookeeper\n      release: zookeeper\n  serviceName: zookeeper-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n        release: zookeeper\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - zookeeper\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - zookeeper\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: zookeeper-server\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 2181\n          name: client\n        - containerPort: 2888\n          name: server\n        - containerPort: 3888\n          name: leader-election\n        resources:\n          {}\n        env:\n        - name : KAFKA_HEAP_OPTS\n          value: \"-Xms512M -Xmx512M\"\n        - name : KAFKA_JMX_PORT\n          value: \"5555\"\n        - name : ZOOKEEPER_TICK_TIME\n          value: \"2000\"\n        - name : ZOOKEEPER_SYNC_LIMIT\n          value: \"5\"\n        - name : ZOOKEEPER_INIT_LIMIT\n          value: \"10\"\n        - name : ZOOKEEPER_MAX_CLIENT_CNXNS\n          value: \"60\"\n        - name : ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT\n          value: \"3\"\n        - name : ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL\n          value: \"24\"\n        - name: ZOOKEEPER_CLIENT_PORT\n          value: \"2181\"\n        - name : ZOOKEEPER_SERVERS\n          value: \"zookeeper-0.zookeeper-headless.default:2888:3888;zookeeper-1.zookeeper-headless.default:2888:3888;zookeeper-2.zookeeper-headless.default:2888:3888\"\n        # ZOOKEEPER_SERVER_ID is required just to pass zookeeper ensure script for env check,\n        # the value(metadata.mame) is not used and will be overwritten in command part\n        - name: ZOOKEEPER_SERVER_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        command:\n        - \"bash\"\n        - \"-c\"\n        - |\n          ZK_FIX_HOST_REGEX=\"s/${HOSTNAME}\\.[^:]*:/0.0.0.0:/g\"\n          ZOOKEEPER_SERVER_ID=$((${HOSTNAME##*-}+1)) \\\n          ZOOKEEPER_SERVERS=`echo $ZOOKEEPER_SERVERS | sed -e \"$ZK_FIX_HOST_REGEX\"` \\\n          /etc/confluent/docker/run\n        volumeMounts:\n        - name: datadir\n          mountPath: /var/lib/zookeeper/data\n        - name: datalogdir\n          mountPath: /var/lib/zookeeper/log\n      volumes:\n      \n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n  - metadata:\n      name: datalogdir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/zookeeper/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper\n  labels:\n    app: zookeeper\n    chart: zookeeper-0.1.0\n    release: zookeeper\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: zookeeper\n      release: zookeeper\n  serviceName: zookeeper-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n        release: zookeeper\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - zookeeper\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - zookeeper\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: zookeeper-server\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 2181\n          name: client\n        - containerPort: 2888\n          name: server\n        - containerPort: 3888\n          name: leader-election\n        resources:\n          {}\n        env:\n        - name : KAFKA_HEAP_OPTS\n          value: \"-Xms512M -Xmx512M\"\n        - name : KAFKA_JMX_PORT\n          value: \"5555\"\n        - name : ZOOKEEPER_TICK_TIME\n          value: \"2000\"\n        - name : ZOOKEEPER_SYNC_LIMIT\n          value: \"5\"\n        - name : ZOOKEEPER_INIT_LIMIT\n          value: \"10\"\n        - name : ZOOKEEPER_MAX_CLIENT_CNXNS\n          value: \"60\"\n        - name : ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT\n          value: \"3\"\n        - name : ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL\n          value: \"24\"\n        - name: ZOOKEEPER_CLIENT_PORT\n          value: \"2181\"\n        - name : ZOOKEEPER_SERVERS\n          value: \"zookeeper-0.zookeeper-headless.default:2888:3888;zookeeper-1.zookeeper-headless.default:2888:3888;zookeeper-2.zookeeper-headless.default:2888:3888\"\n        # ZOOKEEPER_SERVER_ID is required just to pass zookeeper ensure script for env check,\n        # the value(metadata.mame) is not used and will be overwritten in command part\n        - name: ZOOKEEPER_SERVER_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        command:\n        - \"bash\"\n        - \"-c\"\n        - |\n          ZK_FIX_HOST_REGEX=\"s/${HOSTNAME}\\.[^:]*:/0.0.0.0:/g\"\n          ZOOKEEPER_SERVER_ID=$((${HOSTNAME##*-}+1)) \\\n          ZOOKEEPER_SERVERS=`echo $ZOOKEEPER_SERVERS | sed -e \"$ZK_FIX_HOST_REGEX\"` \\\n          /etc/confluent/docker/run\n        volumeMounts:\n        - name: datadir\n          mountPath: /var/lib/zookeeper/data\n        - name: datalogdir\n          mountPath: /var/lib/zookeeper/log\n      volumes:\n      \n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n  - metadata:\n      name: datalogdir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/zookeeper/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper\n  labels:\n    app: zookeeper\n    chart: zookeeper-0.1.0\n    release: zookeeper\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: zookeeper\n      release: zookeeper\n  serviceName: zookeeper-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n        release: zookeeper\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - zookeeper\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - zookeeper\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: zookeeper-server\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 2181\n          name: client\n        - containerPort: 2888\n          name: server\n        - containerPort: 3888\n          name: leader-election\n        resources:\n          {}\n        env:\n        - name : KAFKA_HEAP_OPTS\n          value: \"-Xms512M -Xmx512M\"\n        - name : KAFKA_JMX_PORT\n          value: \"5555\"\n        - name : ZOOKEEPER_TICK_TIME\n          value: \"2000\"\n        - name : ZOOKEEPER_SYNC_LIMIT\n          value: \"5\"\n        - name : ZOOKEEPER_INIT_LIMIT\n          value: \"10\"\n        - name : ZOOKEEPER_MAX_CLIENT_CNXNS\n          value: \"60\"\n        - name : ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT\n          value: \"3\"\n        - name : ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL\n          value: \"24\"\n        - name: ZOOKEEPER_CLIENT_PORT\n          value: \"2181\"\n        - name : ZOOKEEPER_SERVERS\n          value: \"zookeeper-0.zookeeper-headless.default:2888:3888;zookeeper-1.zookeeper-headless.default:2888:3888;zookeeper-2.zookeeper-headless.default:2888:3888\"\n        # ZOOKEEPER_SERVER_ID is required just to pass zookeeper ensure script for env check,\n        # the value(metadata.mame) is not used and will be overwritten in command part\n        - name: ZOOKEEPER_SERVER_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        command:\n        - \"bash\"\n        - \"-c\"\n        - |\n          ZK_FIX_HOST_REGEX=\"s/${HOSTNAME}\\.[^:]*:/0.0.0.0:/g\"\n          ZOOKEEPER_SERVER_ID=$((${HOSTNAME##*-}+1)) \\\n          ZOOKEEPER_SERVERS=`echo $ZOOKEEPER_SERVERS | sed -e \"$ZK_FIX_HOST_REGEX\"` \\\n          /etc/confluent/docker/run\n        volumeMounts:\n        - name: datadir\n          mountPath: /var/lib/zookeeper/data\n        - name: datalogdir\n          mountPath: /var/lib/zookeeper/log\n      volumes:\n      \n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n  - metadata:\n      name: datalogdir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/zookeeper/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper\n  labels:\n    app: zookeeper\n    chart: zookeeper-0.1.0\n    release: zookeeper\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: zookeeper\n      release: zookeeper\n  serviceName: zookeeper-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n        release: zookeeper\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - zookeeper\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - zookeeper\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: zookeeper-server\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 2181\n          name: client\n        - containerPort: 2888\n          name: server\n        - containerPort: 3888\n          name: leader-election\n        resources:\n          {}\n        env:\n        - name : KAFKA_HEAP_OPTS\n          value: \"-Xms512M -Xmx512M\"\n        - name : KAFKA_JMX_PORT\n          value: \"5555\"\n        - name : ZOOKEEPER_TICK_TIME\n          value: \"2000\"\n        - name : ZOOKEEPER_SYNC_LIMIT\n          value: \"5\"\n        - name : ZOOKEEPER_INIT_LIMIT\n          value: \"10\"\n        - name : ZOOKEEPER_MAX_CLIENT_CNXNS\n          value: \"60\"\n        - name : ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT\n          value: \"3\"\n        - name : ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL\n          value: \"24\"\n        - name: ZOOKEEPER_CLIENT_PORT\n          value: \"2181\"\n        - name : ZOOKEEPER_SERVERS\n          value: \"zookeeper-0.zookeeper-headless.default:2888:3888;zookeeper-1.zookeeper-headless.default:2888:3888;zookeeper-2.zookeeper-headless.default:2888:3888\"\n        # ZOOKEEPER_SERVER_ID is required just to pass zookeeper ensure script for env check,\n        # the value(metadata.mame) is not used and will be overwritten in command part\n        - name: ZOOKEEPER_SERVER_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        command:\n        - \"bash\"\n        - \"-c\"\n        - |\n          ZK_FIX_HOST_REGEX=\"s/${HOSTNAME}\\.[^:]*:/0.0.0.0:/g\"\n          ZOOKEEPER_SERVER_ID=$((${HOSTNAME##*-}+1)) \\\n          ZOOKEEPER_SERVERS=`echo $ZOOKEEPER_SERVERS | sed -e \"$ZK_FIX_HOST_REGEX\"` \\\n          /etc/confluent/docker/run\n        volumeMounts:\n        - name: datadir\n          mountPath: /var/lib/zookeeper/data\n        - name: datalogdir\n          mountPath: /var/lib/zookeeper/log\n      volumes:\n      \n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n  - metadata:\n      name: datalogdir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/zookeeper/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper\n  labels:\n    app: zookeeper\n    chart: zookeeper-0.1.0\n    release: zookeeper\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: zookeeper\n      release: zookeeper\n  serviceName: zookeeper-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n        release: zookeeper\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - zookeeper\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - zookeeper\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: zookeeper-server\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 2181\n          name: client\n        - containerPort: 2888\n          name: server\n        - containerPort: 3888\n          name: leader-election\n        resources:\n          {}\n        env:\n        - name : KAFKA_HEAP_OPTS\n          value: \"-Xms512M -Xmx512M\"\n        - name : KAFKA_JMX_PORT\n          value: \"5555\"\n        - name : ZOOKEEPER_TICK_TIME\n          value: \"2000\"\n        - name : ZOOKEEPER_SYNC_LIMIT\n          value: \"5\"\n        - name : ZOOKEEPER_INIT_LIMIT\n          value: \"10\"\n        - name : ZOOKEEPER_MAX_CLIENT_CNXNS\n          value: \"60\"\n        - name : ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT\n          value: \"3\"\n        - name : ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL\n          value: \"24\"\n        - name: ZOOKEEPER_CLIENT_PORT\n          value: \"2181\"\n        - name : ZOOKEEPER_SERVERS\n          value: \"zookeeper-0.zookeeper-headless.default:2888:3888;zookeeper-1.zookeeper-headless.default:2888:3888;zookeeper-2.zookeeper-headless.default:2888:3888\"\n        # ZOOKEEPER_SERVER_ID is required just to pass zookeeper ensure script for env check,\n        # the value(metadata.mame) is not used and will be overwritten in command part\n        - name: ZOOKEEPER_SERVER_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        command:\n        - \"bash\"\n        - \"-c\"\n        - |\n          ZK_FIX_HOST_REGEX=\"s/${HOSTNAME}\\.[^:]*:/0.0.0.0:/g\"\n          ZOOKEEPER_SERVER_ID=$((${HOSTNAME##*-}+1)) \\\n          ZOOKEEPER_SERVERS=`echo $ZOOKEEPER_SERVERS | sed -e \"$ZK_FIX_HOST_REGEX\"` \\\n          /etc/confluent/docker/run\n        volumeMounts:\n        - name: datadir\n          mountPath: /var/lib/zookeeper/data\n        - name: datalogdir\n          mountPath: /var/lib/zookeeper/log\n      volumes:\n      \n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n  - metadata:\n      name: datalogdir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/zookeeper/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper\n  labels:\n    app: zookeeper\n    chart: zookeeper-0.1.0\n    release: zookeeper\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: zookeeper\n      release: zookeeper\n  serviceName: zookeeper-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n        release: zookeeper\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - zookeeper\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - zookeeper\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: zookeeper-server\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 2181\n          name: client\n        - containerPort: 2888\n          name: server\n        - containerPort: 3888\n          name: leader-election\n        resources:\n          {}\n        env:\n        - name : KAFKA_HEAP_OPTS\n          value: \"-Xms512M -Xmx512M\"\n        - name : KAFKA_JMX_PORT\n          value: \"5555\"\n        - name : ZOOKEEPER_TICK_TIME\n          value: \"2000\"\n        - name : ZOOKEEPER_SYNC_LIMIT\n          value: \"5\"\n        - name : ZOOKEEPER_INIT_LIMIT\n          value: \"10\"\n        - name : ZOOKEEPER_MAX_CLIENT_CNXNS\n          value: \"60\"\n        - name : ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT\n          value: \"3\"\n        - name : ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL\n          value: \"24\"\n        - name: ZOOKEEPER_CLIENT_PORT\n          value: \"2181\"\n        - name : ZOOKEEPER_SERVERS\n          value: \"zookeeper-0.zookeeper-headless.default:2888:3888;zookeeper-1.zookeeper-headless.default:2888:3888;zookeeper-2.zookeeper-headless.default:2888:3888\"\n        # ZOOKEEPER_SERVER_ID is required just to pass zookeeper ensure script for env check,\n        # the value(metadata.mame) is not used and will be overwritten in command part\n        - name: ZOOKEEPER_SERVER_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        command:\n        - \"bash\"\n        - \"-c\"\n        - |\n          ZK_FIX_HOST_REGEX=\"s/${HOSTNAME}\\.[^:]*:/0.0.0.0:/g\"\n          ZOOKEEPER_SERVER_ID=$((${HOSTNAME##*-}+1)) \\\n          ZOOKEEPER_SERVERS=`echo $ZOOKEEPER_SERVERS | sed -e \"$ZK_FIX_HOST_REGEX\"` \\\n          /etc/confluent/docker/run\n        volumeMounts:\n        - name: datadir\n          mountPath: /var/lib/zookeeper/data\n        - name: datalogdir\n          mountPath: /var/lib/zookeeper/log\n      volumes:\n      \n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n  - metadata:\n      name: datalogdir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/zookeeper/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper\n  labels:\n    app: zookeeper\n    chart: zookeeper-0.1.0\n    release: zookeeper\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: zookeeper\n      release: zookeeper\n  serviceName: zookeeper-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n        release: zookeeper\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - zookeeper\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - zookeeper\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: zookeeper-server\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 2181\n          name: client\n        - containerPort: 2888\n          name: server\n        - containerPort: 3888\n          name: leader-election\n        resources:\n          {}\n        env:\n        - name : KAFKA_HEAP_OPTS\n          value: \"-Xms512M -Xmx512M\"\n        - name : KAFKA_JMX_PORT\n          value: \"5555\"\n        - name : ZOOKEEPER_TICK_TIME\n          value: \"2000\"\n        - name : ZOOKEEPER_SYNC_LIMIT\n          value: \"5\"\n        - name : ZOOKEEPER_INIT_LIMIT\n          value: \"10\"\n        - name : ZOOKEEPER_MAX_CLIENT_CNXNS\n          value: \"60\"\n        - name : ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT\n          value: \"3\"\n        - name : ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL\n          value: \"24\"\n        - name: ZOOKEEPER_CLIENT_PORT\n          value: \"2181\"\n        - name : ZOOKEEPER_SERVERS\n          value: \"zookeeper-0.zookeeper-headless.default:2888:3888;zookeeper-1.zookeeper-headless.default:2888:3888;zookeeper-2.zookeeper-headless.default:2888:3888\"\n        # ZOOKEEPER_SERVER_ID is required just to pass zookeeper ensure script for env check,\n        # the value(metadata.mame) is not used and will be overwritten in command part\n        - name: ZOOKEEPER_SERVER_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        command:\n        - \"bash\"\n        - \"-c\"\n        - |\n          ZK_FIX_HOST_REGEX=\"s/${HOSTNAME}\\.[^:]*:/0.0.0.0:/g\"\n          ZOOKEEPER_SERVER_ID=$((${HOSTNAME##*-}+1)) \\\n          ZOOKEEPER_SERVERS=`echo $ZOOKEEPER_SERVERS | sed -e \"$ZK_FIX_HOST_REGEX\"` \\\n          /etc/confluent/docker/run\n        volumeMounts:\n        - name: datadir\n          mountPath: /var/lib/zookeeper/data\n        - name: datalogdir\n          mountPath: /var/lib/zookeeper/log\n      volumes:\n      \n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n  - metadata:\n      name: datalogdir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/zookeeper/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper\n  labels:\n    app: zookeeper\n    chart: zookeeper-0.1.0\n    release: zookeeper\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: zookeeper\n      release: zookeeper\n  serviceName: zookeeper-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n        release: zookeeper\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - zookeeper\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - zookeeper\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: zookeeper-server\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 2181\n          name: client\n        - containerPort: 2888\n          name: server\n        - containerPort: 3888\n          name: leader-election\n        resources:\n          {}\n        env:\n        - name : KAFKA_HEAP_OPTS\n          value: \"-Xms512M -Xmx512M\"\n        - name : KAFKA_JMX_PORT\n          value: \"5555\"\n        - name : ZOOKEEPER_TICK_TIME\n          value: \"2000\"\n        - name : ZOOKEEPER_SYNC_LIMIT\n          value: \"5\"\n        - name : ZOOKEEPER_INIT_LIMIT\n          value: \"10\"\n        - name : ZOOKEEPER_MAX_CLIENT_CNXNS\n          value: \"60\"\n        - name : ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT\n          value: \"3\"\n        - name : ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL\n          value: \"24\"\n        - name: ZOOKEEPER_CLIENT_PORT\n          value: \"2181\"\n        - name : ZOOKEEPER_SERVERS\n          value: \"zookeeper-0.zookeeper-headless.default:2888:3888;zookeeper-1.zookeeper-headless.default:2888:3888;zookeeper-2.zookeeper-headless.default:2888:3888\"\n        # ZOOKEEPER_SERVER_ID is required just to pass zookeeper ensure script for env check,\n        # the value(metadata.mame) is not used and will be overwritten in command part\n        - name: ZOOKEEPER_SERVER_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        command:\n        - \"bash\"\n        - \"-c\"\n        - |\n          ZK_FIX_HOST_REGEX=\"s/${HOSTNAME}\\.[^:]*:/0.0.0.0:/g\"\n          ZOOKEEPER_SERVER_ID=$((${HOSTNAME##*-}+1)) \\\n          ZOOKEEPER_SERVERS=`echo $ZOOKEEPER_SERVERS | sed -e \"$ZK_FIX_HOST_REGEX\"` \\\n          /etc/confluent/docker/run\n        volumeMounts:\n        - name: datadir\n          mountPath: /var/lib/zookeeper/data\n        - name: datalogdir\n          mountPath: /var/lib/zookeeper/log\n      volumes:\n      \n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n  - metadata:\n      name: datalogdir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/zookeeper/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper\n  labels:\n    app: zookeeper\n    chart: zookeeper-0.1.0\n    release: zookeeper\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: zookeeper\n      release: zookeeper\n  serviceName: zookeeper-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n        release: zookeeper\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - zookeeper\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - zookeeper\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: zookeeper-server\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 2181\n          name: client\n        - containerPort: 2888\n          name: server\n        - containerPort: 3888\n          name: leader-election\n        resources:\n          {}\n        env:\n        - name : KAFKA_HEAP_OPTS\n          value: \"-Xms512M -Xmx512M\"\n        - name : KAFKA_JMX_PORT\n          value: \"5555\"\n        - name : ZOOKEEPER_TICK_TIME\n          value: \"2000\"\n        - name : ZOOKEEPER_SYNC_LIMIT\n          value: \"5\"\n        - name : ZOOKEEPER_INIT_LIMIT\n          value: \"10\"\n        - name : ZOOKEEPER_MAX_CLIENT_CNXNS\n          value: \"60\"\n        - name : ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT\n          value: \"3\"\n        - name : ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL\n          value: \"24\"\n        - name: ZOOKEEPER_CLIENT_PORT\n          value: \"2181\"\n        - name : ZOOKEEPER_SERVERS\n          value: \"zookeeper-0.zookeeper-headless.default:2888:3888;zookeeper-1.zookeeper-headless.default:2888:3888;zookeeper-2.zookeeper-headless.default:2888:3888\"\n        # ZOOKEEPER_SERVER_ID is required just to pass zookeeper ensure script for env check,\n        # the value(metadata.mame) is not used and will be overwritten in command part\n        - name: ZOOKEEPER_SERVER_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        command:\n        - \"bash\"\n        - \"-c\"\n        - |\n          ZK_FIX_HOST_REGEX=\"s/${HOSTNAME}\\.[^:]*:/0.0.0.0:/g\"\n          ZOOKEEPER_SERVER_ID=$((${HOSTNAME##*-}+1)) \\\n          ZOOKEEPER_SERVERS=`echo $ZOOKEEPER_SERVERS | sed -e \"$ZK_FIX_HOST_REGEX\"` \\\n          /etc/confluent/docker/run\n        volumeMounts:\n        - name: datadir\n          mountPath: /var/lib/zookeeper/data\n        - name: datalogdir\n          mountPath: /var/lib/zookeeper/log\n      volumes:\n      \n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n  - metadata:\n      name: datalogdir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/zookeeper/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper\n  labels:\n    app: zookeeper\n    chart: zookeeper-0.1.0\n    release: zookeeper\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: zookeeper\n      release: zookeeper\n  serviceName: zookeeper-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n        release: zookeeper\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - zookeeper\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - zookeeper\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: zookeeper-server\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 2181\n          name: client\n        - containerPort: 2888\n          name: server\n        - containerPort: 3888\n          name: leader-election\n        resources:\n          {}\n        env:\n        - name : KAFKA_HEAP_OPTS\n          value: \"-Xms512M -Xmx512M\"\n        - name : KAFKA_JMX_PORT\n          value: \"5555\"\n        - name : ZOOKEEPER_TICK_TIME\n          value: \"2000\"\n        - name : ZOOKEEPER_SYNC_LIMIT\n          value: \"5\"\n        - name : ZOOKEEPER_INIT_LIMIT\n          value: \"10\"\n        - name : ZOOKEEPER_MAX_CLIENT_CNXNS\n          value: \"60\"\n        - name : ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT\n          value: \"3\"\n        - name : ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL\n          value: \"24\"\n        - name: ZOOKEEPER_CLIENT_PORT\n          value: \"2181\"\n        - name : ZOOKEEPER_SERVERS\n          value: \"zookeeper-0.zookeeper-headless.default:2888:3888;zookeeper-1.zookeeper-headless.default:2888:3888;zookeeper-2.zookeeper-headless.default:2888:3888\"\n        # ZOOKEEPER_SERVER_ID is required just to pass zookeeper ensure script for env check,\n        # the value(metadata.mame) is not used and will be overwritten in command part\n        - name: ZOOKEEPER_SERVER_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        command:\n        - \"bash\"\n        - \"-c\"\n        - |\n          ZK_FIX_HOST_REGEX=\"s/${HOSTNAME}\\.[^:]*:/0.0.0.0:/g\"\n          ZOOKEEPER_SERVER_ID=$((${HOSTNAME##*-}+1)) \\\n          ZOOKEEPER_SERVERS=`echo $ZOOKEEPER_SERVERS | sed -e \"$ZK_FIX_HOST_REGEX\"` \\\n          /etc/confluent/docker/run\n        volumeMounts:\n        - name: datadir\n          mountPath: /var/lib/zookeeper/data\n        - name: datalogdir\n          mountPath: /var/lib/zookeeper/log\n      volumes:\n      \n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n  - metadata:\n      name: datalogdir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/zookeeper/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper\n  labels:\n    app: zookeeper\n    chart: zookeeper-0.1.0\n    release: zookeeper\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: zookeeper\n      release: zookeeper\n  serviceName: zookeeper-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n        release: zookeeper\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - zookeeper\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - zookeeper\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: zookeeper-server\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 2181\n          name: client\n        - containerPort: 2888\n          name: server\n        - containerPort: 3888\n          name: leader-election\n        resources:\n          {}\n        env:\n        - name : KAFKA_HEAP_OPTS\n          value: \"-Xms512M -Xmx512M\"\n        - name : KAFKA_JMX_PORT\n          value: \"5555\"\n        - name : ZOOKEEPER_TICK_TIME\n          value: \"2000\"\n        - name : ZOOKEEPER_SYNC_LIMIT\n          value: \"5\"\n        - name : ZOOKEEPER_INIT_LIMIT\n          value: \"10\"\n        - name : ZOOKEEPER_MAX_CLIENT_CNXNS\n          value: \"60\"\n        - name : ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT\n          value: \"3\"\n        - name : ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL\n          value: \"24\"\n        - name: ZOOKEEPER_CLIENT_PORT\n          value: \"2181\"\n        - name : ZOOKEEPER_SERVERS\n          value: \"zookeeper-0.zookeeper-headless.default:2888:3888;zookeeper-1.zookeeper-headless.default:2888:3888;zookeeper-2.zookeeper-headless.default:2888:3888\"\n        # ZOOKEEPER_SERVER_ID is required just to pass zookeeper ensure script for env check,\n        # the value(metadata.mame) is not used and will be overwritten in command part\n        - name: ZOOKEEPER_SERVER_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        command:\n        - \"bash\"\n        - \"-c\"\n        - |\n          ZK_FIX_HOST_REGEX=\"s/${HOSTNAME}\\.[^:]*:/0.0.0.0:/g\"\n          ZOOKEEPER_SERVER_ID=$((${HOSTNAME##*-}+1)) \\\n          ZOOKEEPER_SERVERS=`echo $ZOOKEEPER_SERVERS | sed -e \"$ZK_FIX_HOST_REGEX\"` \\\n          /etc/confluent/docker/run\n        volumeMounts:\n        - name: datadir\n          mountPath: /var/lib/zookeeper/data\n        - name: datalogdir\n          mountPath: /var/lib/zookeeper/log\n      volumes:\n      \n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n  - metadata:\n      name: datalogdir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/zookeeper/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper\n  labels:\n    app: zookeeper\n    chart: zookeeper-0.1.0\n    release: zookeeper\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: zookeeper\n      release: zookeeper\n  serviceName: zookeeper-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n        release: zookeeper\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - zookeeper\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - zookeeper\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: zookeeper-server\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 2181\n          name: client\n        - containerPort: 2888\n          name: server\n        - containerPort: 3888\n          name: leader-election\n        resources:\n          {}\n        env:\n        - name : KAFKA_HEAP_OPTS\n          value: \"-Xms512M -Xmx512M\"\n        - name : KAFKA_JMX_PORT\n          value: \"5555\"\n        - name : ZOOKEEPER_TICK_TIME\n          value: \"2000\"\n        - name : ZOOKEEPER_SYNC_LIMIT\n          value: \"5\"\n        - name : ZOOKEEPER_INIT_LIMIT\n          value: \"10\"\n        - name : ZOOKEEPER_MAX_CLIENT_CNXNS\n          value: \"60\"\n        - name : ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT\n          value: \"3\"\n        - name : ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL\n          value: \"24\"\n        - name: ZOOKEEPER_CLIENT_PORT\n          value: \"2181\"\n        - name : ZOOKEEPER_SERVERS\n          value: \"zookeeper-0.zookeeper-headless.default:2888:3888;zookeeper-1.zookeeper-headless.default:2888:3888;zookeeper-2.zookeeper-headless.default:2888:3888\"\n        # ZOOKEEPER_SERVER_ID is required just to pass zookeeper ensure script for env check,\n        # the value(metadata.mame) is not used and will be overwritten in command part\n        - name: ZOOKEEPER_SERVER_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        command:\n        - \"bash\"\n        - \"-c\"\n        - |\n          ZK_FIX_HOST_REGEX=\"s/${HOSTNAME}\\.[^:]*:/0.0.0.0:/g\"\n          ZOOKEEPER_SERVER_ID=$((${HOSTNAME##*-}+1)) \\\n          ZOOKEEPER_SERVERS=`echo $ZOOKEEPER_SERVERS | sed -e \"$ZK_FIX_HOST_REGEX\"` \\\n          /etc/confluent/docker/run\n        volumeMounts:\n        - name: datadir\n          mountPath: /var/lib/zookeeper/data\n        - name: datalogdir\n          mountPath: /var/lib/zookeeper/log\n      volumes:\n      \n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n  - metadata:\n      name: datalogdir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/zookeeper/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper\n  labels:\n    app: zookeeper\n    chart: zookeeper-0.1.0\n    release: zookeeper\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: zookeeper\n      release: zookeeper\n  serviceName: zookeeper-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n        release: zookeeper\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - zookeeper\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - zookeeper\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: zookeeper-server\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 2181\n          name: client\n        - containerPort: 2888\n          name: server\n        - containerPort: 3888\n          name: leader-election\n        resources:\n          {}\n        env:\n        - name : KAFKA_HEAP_OPTS\n          value: \"-Xms512M -Xmx512M\"\n        - name : KAFKA_JMX_PORT\n          value: \"5555\"\n        - name : ZOOKEEPER_TICK_TIME\n          value: \"2000\"\n        - name : ZOOKEEPER_SYNC_LIMIT\n          value: \"5\"\n        - name : ZOOKEEPER_INIT_LIMIT\n          value: \"10\"\n        - name : ZOOKEEPER_MAX_CLIENT_CNXNS\n          value: \"60\"\n        - name : ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT\n          value: \"3\"\n        - name : ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL\n          value: \"24\"\n        - name: ZOOKEEPER_CLIENT_PORT\n          value: \"2181\"\n        - name : ZOOKEEPER_SERVERS\n          value: \"zookeeper-0.zookeeper-headless.default:2888:3888;zookeeper-1.zookeeper-headless.default:2888:3888;zookeeper-2.zookeeper-headless.default:2888:3888\"\n        # ZOOKEEPER_SERVER_ID is required just to pass zookeeper ensure script for env check,\n        # the value(metadata.mame) is not used and will be overwritten in command part\n        - name: ZOOKEEPER_SERVER_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        command:\n        - \"bash\"\n        - \"-c\"\n        - |\n          ZK_FIX_HOST_REGEX=\"s/${HOSTNAME}\\.[^:]*:/0.0.0.0:/g\"\n          ZOOKEEPER_SERVER_ID=$((${HOSTNAME##*-}+1)) \\\n          ZOOKEEPER_SERVERS=`echo $ZOOKEEPER_SERVERS | sed -e \"$ZK_FIX_HOST_REGEX\"` \\\n          /etc/confluent/docker/run\n        volumeMounts:\n        - name: datadir\n          mountPath: /var/lib/zookeeper/data\n        - name: datalogdir\n          mountPath: /var/lib/zookeeper/log\n      volumes:\n      \n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n  - metadata:\n      name: datalogdir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/zookeeper/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper\n  labels:\n    app: zookeeper\n    chart: zookeeper-0.1.0\n    release: zookeeper\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: zookeeper\n      release: zookeeper\n  serviceName: zookeeper-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n        release: zookeeper\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - zookeeper\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - zookeeper\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: zookeeper-server\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 2181\n          name: client\n        - containerPort: 2888\n          name: server\n        - containerPort: 3888\n          name: leader-election\n        resources:\n          {}\n        env:\n        - name : KAFKA_HEAP_OPTS\n          value: \"-Xms512M -Xmx512M\"\n        - name : KAFKA_JMX_PORT\n          value: \"5555\"\n        - name : ZOOKEEPER_TICK_TIME\n          value: \"2000\"\n        - name : ZOOKEEPER_SYNC_LIMIT\n          value: \"5\"\n        - name : ZOOKEEPER_INIT_LIMIT\n          value: \"10\"\n        - name : ZOOKEEPER_MAX_CLIENT_CNXNS\n          value: \"60\"\n        - name : ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT\n          value: \"3\"\n        - name : ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL\n          value: \"24\"\n        - name: ZOOKEEPER_CLIENT_PORT\n          value: \"2181\"\n        - name : ZOOKEEPER_SERVERS\n          value: \"zookeeper-0.zookeeper-headless.default:2888:3888;zookeeper-1.zookeeper-headless.default:2888:3888;zookeeper-2.zookeeper-headless.default:2888:3888\"\n        # ZOOKEEPER_SERVER_ID is required just to pass zookeeper ensure script for env check,\n        # the value(metadata.mame) is not used and will be overwritten in command part\n        - name: ZOOKEEPER_SERVER_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        command:\n        - \"bash\"\n        - \"-c\"\n        - |\n          ZK_FIX_HOST_REGEX=\"s/${HOSTNAME}\\.[^:]*:/0.0.0.0:/g\"\n          ZOOKEEPER_SERVER_ID=$((${HOSTNAME##*-}+1)) \\\n          ZOOKEEPER_SERVERS=`echo $ZOOKEEPER_SERVERS | sed -e \"$ZK_FIX_HOST_REGEX\"` \\\n          /etc/confluent/docker/run\n        volumeMounts:\n        - name: datadir\n          mountPath: /var/lib/zookeeper/data\n        - name: datalogdir\n          mountPath: /var/lib/zookeeper/log\n      volumes:\n      \n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n  - metadata:\n      name: datalogdir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/zookeeper/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper\n  labels:\n    app: zookeeper\n    chart: zookeeper-0.1.0\n    release: zookeeper\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: zookeeper\n      release: zookeeper\n  serviceName: zookeeper-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n        release: zookeeper\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - zookeeper\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - zookeeper\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: zookeeper-server\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 2181\n          name: client\n        - containerPort: 2888\n          name: server\n        - containerPort: 3888\n          name: leader-election\n        resources:\n          {}\n        env:\n        - name : KAFKA_HEAP_OPTS\n          value: \"-Xms512M -Xmx512M\"\n        - name : KAFKA_JMX_PORT\n          value: \"5555\"\n        - name : ZOOKEEPER_TICK_TIME\n          value: \"2000\"\n        - name : ZOOKEEPER_SYNC_LIMIT\n          value: \"5\"\n        - name : ZOOKEEPER_INIT_LIMIT\n          value: \"10\"\n        - name : ZOOKEEPER_MAX_CLIENT_CNXNS\n          value: \"60\"\n        - name : ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT\n          value: \"3\"\n        - name : ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL\n          value: \"24\"\n        - name: ZOOKEEPER_CLIENT_PORT\n          value: \"2181\"\n        - name : ZOOKEEPER_SERVERS\n          value: \"zookeeper-0.zookeeper-headless.default:2888:3888;zookeeper-1.zookeeper-headless.default:2888:3888;zookeeper-2.zookeeper-headless.default:2888:3888\"\n        # ZOOKEEPER_SERVER_ID is required just to pass zookeeper ensure script for env check,\n        # the value(metadata.mame) is not used and will be overwritten in command part\n        - name: ZOOKEEPER_SERVER_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        command:\n        - \"bash\"\n        - \"-c\"\n        - |\n          ZK_FIX_HOST_REGEX=\"s/${HOSTNAME}\\.[^:]*:/0.0.0.0:/g\"\n          ZOOKEEPER_SERVER_ID=$((${HOSTNAME##*-}+1)) \\\n          ZOOKEEPER_SERVERS=`echo $ZOOKEEPER_SERVERS | sed -e \"$ZK_FIX_HOST_REGEX\"` \\\n          /etc/confluent/docker/run\n        volumeMounts:\n        - name: datadir\n          mountPath: /var/lib/zookeeper/data\n        - name: datalogdir\n          mountPath: /var/lib/zookeeper/log\n      volumes:\n      \n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n  - metadata:\n      name: datalogdir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/zookeeper/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 115, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper\n  labels:\n    app: zookeeper\n    chart: zookeeper-0.1.0\n    release: zookeeper\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: zookeeper\n      release: zookeeper\n  serviceName: zookeeper-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n        release: zookeeper\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - zookeeper\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - zookeeper\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: zookeeper-server\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 2181\n          name: client\n        - containerPort: 2888\n          name: server\n        - containerPort: 3888\n          name: leader-election\n        resources:\n          {}\n        env:\n        - name : KAFKA_HEAP_OPTS\n          value: \"-Xms512M -Xmx512M\"\n        - name : KAFKA_JMX_PORT\n          value: \"5555\"\n        - name : ZOOKEEPER_TICK_TIME\n          value: \"2000\"\n        - name : ZOOKEEPER_SYNC_LIMIT\n          value: \"5\"\n        - name : ZOOKEEPER_INIT_LIMIT\n          value: \"10\"\n        - name : ZOOKEEPER_MAX_CLIENT_CNXNS\n          value: \"60\"\n        - name : ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT\n          value: \"3\"\n        - name : ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL\n          value: \"24\"\n        - name: ZOOKEEPER_CLIENT_PORT\n          value: \"2181\"\n        - name : ZOOKEEPER_SERVERS\n          value: \"zookeeper-0.zookeeper-headless.default:2888:3888;zookeeper-1.zookeeper-headless.default:2888:3888;zookeeper-2.zookeeper-headless.default:2888:3888\"\n        # ZOOKEEPER_SERVER_ID is required just to pass zookeeper ensure script for env check,\n        # the value(metadata.mame) is not used and will be overwritten in command part\n        - name: ZOOKEEPER_SERVER_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        command:\n        - \"bash\"\n        - \"-c\"\n        - |\n          ZK_FIX_HOST_REGEX=\"s/${HOSTNAME}\\.[^:]*:/0.0.0.0:/g\"\n          ZOOKEEPER_SERVER_ID=$((${HOSTNAME##*-}+1)) \\\n          ZOOKEEPER_SERVERS=`echo $ZOOKEEPER_SERVERS | sed -e \"$ZK_FIX_HOST_REGEX\"` \\\n          /etc/confluent/docker/run\n        volumeMounts:\n        - name: datadir\n          mountPath: /var/lib/zookeeper/data\n        - name: datalogdir\n          mountPath: /var/lib/zookeeper/log\n      volumes:\n      \n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n  - metadata:\n      name: datalogdir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-discovery/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 147, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    app: istiod\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: \"Pilot\"\n    istio: pilot\n    release: istio-discovery\nspec:\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 25%\n  selector:\n    matchLabels:\n      istio: pilot\n  template:\n    metadata:\n      labels:\n        app: istiod\n        istio.io/rev: default\n        install.operator.istio.io/owning-resource: unknown\n        sidecar.istio.io/inject: \"false\"\n        operator.istio.io/component: \"Pilot\"\n        istio: pilot\n      annotations:\n        prometheus.io/port: \"15014\"\n        prometheus.io/scrape: \"true\"\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: istiod-service-account\n      securityContext:\n        fsGroup: 1337\n      containers:\n        - name: discovery\n          image: \"docker.io/istio/pilot:1.10.4\"\n          args:\n          - \"discovery\"\n          - --monitoringAddr=:15014\n          - --log_output_level=default:info\n          - --domain\n          - cluster.local\n          - --keepaliveMaxServerConnectionAge\n          - \"30m\"\n          ports:\n          - containerPort: 8080\n            protocol: TCP\n          - containerPort: 15010\n            protocol: TCP\n          - containerPort: 15017\n            protocol: TCP\n          readinessProbe:\n            httpGet:\n              path: /ready\n              port: 8080\n            initialDelaySeconds: 1\n            periodSeconds: 3\n            timeoutSeconds: 5\n          env:\n          - name: REVISION\n            value: \"default\"\n          - name: JWT_POLICY\n            value: third-party-jwt\n          - name: PILOT_CERT_PROVIDER\n            value: istiod\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.namespace\n          - name: SERVICE_ACCOUNT\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: spec.serviceAccountName\n          - name: KUBECONFIG\n            value: /var/run/secrets/remote/config\n          - name: ENABLE_LEGACY_FSGROUP_INJECTION\n            value: \"false\"\n          - name: PILOT_TRACE_SAMPLING\n            value: \"1\"\n          - name: PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_OUTBOUND\n            value: \"true\"\n          - name: PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_INBOUND\n            value: \"true\"\n          - name: ISTIOD_ADDR\n            value: istiod.default.svc:15012\n          - name: PILOT_ENABLE_ANALYSIS\n            value: \"false\"\n          - name: CLUSTER_ID\n            value: \"Kubernetes\"\n          resources:\n            requests:\n              cpu: 500m\n              memory: 2048Mi\n          securityContext:\n            runAsUser: 1337\n            runAsGroup: 1337\n            runAsNonRoot: true\n            capabilities:\n              drop:\n              - ALL\n          volumeMounts:\n          - name: istio-token\n            mountPath: /var/run/secrets/tokens\n            readOnly: true\n          - name: local-certs\n            mountPath: /var/run/secrets/istio-dns\n          - name: cacerts\n            mountPath: /etc/cacerts\n            readOnly: true\n          - name: istio-kubeconfig\n            mountPath: /var/run/secrets/remote\n            readOnly: true\n      volumes:\n      # Technically not needed on this pod - but it helps debugging/testing SDS\n      # Should be removed after everything works.\n      - emptyDir:\n          medium: Memory\n        name: local-certs\n      - name: istio-token\n        projected:\n          sources:\n            - serviceAccountToken:\n                audience: istio-ca\n                expirationSeconds: 43200\n                path: istio-token\n      # Optional: user-generated root\n      - name: cacerts\n        secret:\n          secretName: cacerts\n          optional: true\n      - name: istio-kubeconfig\n        secret:\n          secretName: istio-kubeconfig\n          optional: true\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-discovery/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 147, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    app: istiod\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: \"Pilot\"\n    istio: pilot\n    release: istio-discovery\nspec:\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 25%\n  selector:\n    matchLabels:\n      istio: pilot\n  template:\n    metadata:\n      labels:\n        app: istiod\n        istio.io/rev: default\n        install.operator.istio.io/owning-resource: unknown\n        sidecar.istio.io/inject: \"false\"\n        operator.istio.io/component: \"Pilot\"\n        istio: pilot\n      annotations:\n        prometheus.io/port: \"15014\"\n        prometheus.io/scrape: \"true\"\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: istiod-service-account\n      securityContext:\n        fsGroup: 1337\n      containers:\n        - name: discovery\n          image: \"docker.io/istio/pilot:1.10.4\"\n          args:\n          - \"discovery\"\n          - --monitoringAddr=:15014\n          - --log_output_level=default:info\n          - --domain\n          - cluster.local\n          - --keepaliveMaxServerConnectionAge\n          - \"30m\"\n          ports:\n          - containerPort: 8080\n            protocol: TCP\n          - containerPort: 15010\n            protocol: TCP\n          - containerPort: 15017\n            protocol: TCP\n          readinessProbe:\n            httpGet:\n              path: /ready\n              port: 8080\n            initialDelaySeconds: 1\n            periodSeconds: 3\n            timeoutSeconds: 5\n          env:\n          - name: REVISION\n            value: \"default\"\n          - name: JWT_POLICY\n            value: third-party-jwt\n          - name: PILOT_CERT_PROVIDER\n            value: istiod\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.namespace\n          - name: SERVICE_ACCOUNT\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: spec.serviceAccountName\n          - name: KUBECONFIG\n            value: /var/run/secrets/remote/config\n          - name: ENABLE_LEGACY_FSGROUP_INJECTION\n            value: \"false\"\n          - name: PILOT_TRACE_SAMPLING\n            value: \"1\"\n          - name: PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_OUTBOUND\n            value: \"true\"\n          - name: PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_INBOUND\n            value: \"true\"\n          - name: ISTIOD_ADDR\n            value: istiod.default.svc:15012\n          - name: PILOT_ENABLE_ANALYSIS\n            value: \"false\"\n          - name: CLUSTER_ID\n            value: \"Kubernetes\"\n          resources:\n            requests:\n              cpu: 500m\n              memory: 2048Mi\n          securityContext:\n            runAsUser: 1337\n            runAsGroup: 1337\n            runAsNonRoot: true\n            capabilities:\n              drop:\n              - ALL\n          volumeMounts:\n          - name: istio-token\n            mountPath: /var/run/secrets/tokens\n            readOnly: true\n          - name: local-certs\n            mountPath: /var/run/secrets/istio-dns\n          - name: cacerts\n            mountPath: /etc/cacerts\n            readOnly: true\n          - name: istio-kubeconfig\n            mountPath: /var/run/secrets/remote\n            readOnly: true\n      volumes:\n      # Technically not needed on this pod - but it helps debugging/testing SDS\n      # Should be removed after everything works.\n      - emptyDir:\n          medium: Memory\n        name: local-certs\n      - name: istio-token\n        projected:\n          sources:\n            - serviceAccountToken:\n                audience: istio-ca\n                expirationSeconds: 43200\n                path: istio-token\n      # Optional: user-generated root\n      - name: cacerts\n        secret:\n          secretName: cacerts\n          optional: true\n      - name: istio-kubeconfig\n        secret:\n          secretName: istio-kubeconfig\n          optional: true\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-discovery/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 147, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    app: istiod\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: \"Pilot\"\n    istio: pilot\n    release: istio-discovery\nspec:\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 25%\n  selector:\n    matchLabels:\n      istio: pilot\n  template:\n    metadata:\n      labels:\n        app: istiod\n        istio.io/rev: default\n        install.operator.istio.io/owning-resource: unknown\n        sidecar.istio.io/inject: \"false\"\n        operator.istio.io/component: \"Pilot\"\n        istio: pilot\n      annotations:\n        prometheus.io/port: \"15014\"\n        prometheus.io/scrape: \"true\"\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: istiod-service-account\n      securityContext:\n        fsGroup: 1337\n      containers:\n        - name: discovery\n          image: \"docker.io/istio/pilot:1.10.4\"\n          args:\n          - \"discovery\"\n          - --monitoringAddr=:15014\n          - --log_output_level=default:info\n          - --domain\n          - cluster.local\n          - --keepaliveMaxServerConnectionAge\n          - \"30m\"\n          ports:\n          - containerPort: 8080\n            protocol: TCP\n          - containerPort: 15010\n            protocol: TCP\n          - containerPort: 15017\n            protocol: TCP\n          readinessProbe:\n            httpGet:\n              path: /ready\n              port: 8080\n            initialDelaySeconds: 1\n            periodSeconds: 3\n            timeoutSeconds: 5\n          env:\n          - name: REVISION\n            value: \"default\"\n          - name: JWT_POLICY\n            value: third-party-jwt\n          - name: PILOT_CERT_PROVIDER\n            value: istiod\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.namespace\n          - name: SERVICE_ACCOUNT\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: spec.serviceAccountName\n          - name: KUBECONFIG\n            value: /var/run/secrets/remote/config\n          - name: ENABLE_LEGACY_FSGROUP_INJECTION\n            value: \"false\"\n          - name: PILOT_TRACE_SAMPLING\n            value: \"1\"\n          - name: PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_OUTBOUND\n            value: \"true\"\n          - name: PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_INBOUND\n            value: \"true\"\n          - name: ISTIOD_ADDR\n            value: istiod.default.svc:15012\n          - name: PILOT_ENABLE_ANALYSIS\n            value: \"false\"\n          - name: CLUSTER_ID\n            value: \"Kubernetes\"\n          resources:\n            requests:\n              cpu: 500m\n              memory: 2048Mi\n          securityContext:\n            runAsUser: 1337\n            runAsGroup: 1337\n            runAsNonRoot: true\n            capabilities:\n              drop:\n              - ALL\n          volumeMounts:\n          - name: istio-token\n            mountPath: /var/run/secrets/tokens\n            readOnly: true\n          - name: local-certs\n            mountPath: /var/run/secrets/istio-dns\n          - name: cacerts\n            mountPath: /etc/cacerts\n            readOnly: true\n          - name: istio-kubeconfig\n            mountPath: /var/run/secrets/remote\n            readOnly: true\n      volumes:\n      # Technically not needed on this pod - but it helps debugging/testing SDS\n      # Should be removed after everything works.\n      - emptyDir:\n          medium: Memory\n        name: local-certs\n      - name: istio-token\n        projected:\n          sources:\n            - serviceAccountToken:\n                audience: istio-ca\n                expirationSeconds: 43200\n                path: istio-token\n      # Optional: user-generated root\n      - name: cacerts\n        secret:\n          secretName: cacerts\n          optional: true\n      - name: istio-kubeconfig\n        secret:\n          secretName: istio-kubeconfig\n          optional: true\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-discovery/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 147, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    app: istiod\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: \"Pilot\"\n    istio: pilot\n    release: istio-discovery\nspec:\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 25%\n  selector:\n    matchLabels:\n      istio: pilot\n  template:\n    metadata:\n      labels:\n        app: istiod\n        istio.io/rev: default\n        install.operator.istio.io/owning-resource: unknown\n        sidecar.istio.io/inject: \"false\"\n        operator.istio.io/component: \"Pilot\"\n        istio: pilot\n      annotations:\n        prometheus.io/port: \"15014\"\n        prometheus.io/scrape: \"true\"\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: istiod-service-account\n      securityContext:\n        fsGroup: 1337\n      containers:\n        - name: discovery\n          image: \"docker.io/istio/pilot:1.10.4\"\n          args:\n          - \"discovery\"\n          - --monitoringAddr=:15014\n          - --log_output_level=default:info\n          - --domain\n          - cluster.local\n          - --keepaliveMaxServerConnectionAge\n          - \"30m\"\n          ports:\n          - containerPort: 8080\n            protocol: TCP\n          - containerPort: 15010\n            protocol: TCP\n          - containerPort: 15017\n            protocol: TCP\n          readinessProbe:\n            httpGet:\n              path: /ready\n              port: 8080\n            initialDelaySeconds: 1\n            periodSeconds: 3\n            timeoutSeconds: 5\n          env:\n          - name: REVISION\n            value: \"default\"\n          - name: JWT_POLICY\n            value: third-party-jwt\n          - name: PILOT_CERT_PROVIDER\n            value: istiod\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.namespace\n          - name: SERVICE_ACCOUNT\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: spec.serviceAccountName\n          - name: KUBECONFIG\n            value: /var/run/secrets/remote/config\n          - name: ENABLE_LEGACY_FSGROUP_INJECTION\n            value: \"false\"\n          - name: PILOT_TRACE_SAMPLING\n            value: \"1\"\n          - name: PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_OUTBOUND\n            value: \"true\"\n          - name: PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_INBOUND\n            value: \"true\"\n          - name: ISTIOD_ADDR\n            value: istiod.default.svc:15012\n          - name: PILOT_ENABLE_ANALYSIS\n            value: \"false\"\n          - name: CLUSTER_ID\n            value: \"Kubernetes\"\n          resources:\n            requests:\n              cpu: 500m\n              memory: 2048Mi\n          securityContext:\n            runAsUser: 1337\n            runAsGroup: 1337\n            runAsNonRoot: true\n            capabilities:\n              drop:\n              - ALL\n          volumeMounts:\n          - name: istio-token\n            mountPath: /var/run/secrets/tokens\n            readOnly: true\n          - name: local-certs\n            mountPath: /var/run/secrets/istio-dns\n          - name: cacerts\n            mountPath: /etc/cacerts\n            readOnly: true\n          - name: istio-kubeconfig\n            mountPath: /var/run/secrets/remote\n            readOnly: true\n      volumes:\n      # Technically not needed on this pod - but it helps debugging/testing SDS\n      # Should be removed after everything works.\n      - emptyDir:\n          medium: Memory\n        name: local-certs\n      - name: istio-token\n        projected:\n          sources:\n            - serviceAccountToken:\n                audience: istio-ca\n                expirationSeconds: 43200\n                path: istio-token\n      # Optional: user-generated root\n      - name: cacerts\n        secret:\n          secretName: cacerts\n          optional: true\n      - name: istio-kubeconfig\n        secret:\n          secretName: istio-kubeconfig\n          optional: true\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-discovery/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 147, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    app: istiod\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: \"Pilot\"\n    istio: pilot\n    release: istio-discovery\nspec:\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 25%\n  selector:\n    matchLabels:\n      istio: pilot\n  template:\n    metadata:\n      labels:\n        app: istiod\n        istio.io/rev: default\n        install.operator.istio.io/owning-resource: unknown\n        sidecar.istio.io/inject: \"false\"\n        operator.istio.io/component: \"Pilot\"\n        istio: pilot\n      annotations:\n        prometheus.io/port: \"15014\"\n        prometheus.io/scrape: \"true\"\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: istiod-service-account\n      securityContext:\n        fsGroup: 1337\n      containers:\n        - name: discovery\n          image: \"docker.io/istio/pilot:1.10.4\"\n          args:\n          - \"discovery\"\n          - --monitoringAddr=:15014\n          - --log_output_level=default:info\n          - --domain\n          - cluster.local\n          - --keepaliveMaxServerConnectionAge\n          - \"30m\"\n          ports:\n          - containerPort: 8080\n            protocol: TCP\n          - containerPort: 15010\n            protocol: TCP\n          - containerPort: 15017\n            protocol: TCP\n          readinessProbe:\n            httpGet:\n              path: /ready\n              port: 8080\n            initialDelaySeconds: 1\n            periodSeconds: 3\n            timeoutSeconds: 5\n          env:\n          - name: REVISION\n            value: \"default\"\n          - name: JWT_POLICY\n            value: third-party-jwt\n          - name: PILOT_CERT_PROVIDER\n            value: istiod\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.namespace\n          - name: SERVICE_ACCOUNT\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: spec.serviceAccountName\n          - name: KUBECONFIG\n            value: /var/run/secrets/remote/config\n          - name: ENABLE_LEGACY_FSGROUP_INJECTION\n            value: \"false\"\n          - name: PILOT_TRACE_SAMPLING\n            value: \"1\"\n          - name: PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_OUTBOUND\n            value: \"true\"\n          - name: PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_INBOUND\n            value: \"true\"\n          - name: ISTIOD_ADDR\n            value: istiod.default.svc:15012\n          - name: PILOT_ENABLE_ANALYSIS\n            value: \"false\"\n          - name: CLUSTER_ID\n            value: \"Kubernetes\"\n          resources:\n            requests:\n              cpu: 500m\n              memory: 2048Mi\n          securityContext:\n            runAsUser: 1337\n            runAsGroup: 1337\n            runAsNonRoot: true\n            capabilities:\n              drop:\n              - ALL\n          volumeMounts:\n          - name: istio-token\n            mountPath: /var/run/secrets/tokens\n            readOnly: true\n          - name: local-certs\n            mountPath: /var/run/secrets/istio-dns\n          - name: cacerts\n            mountPath: /etc/cacerts\n            readOnly: true\n          - name: istio-kubeconfig\n            mountPath: /var/run/secrets/remote\n            readOnly: true\n      volumes:\n      # Technically not needed on this pod - but it helps debugging/testing SDS\n      # Should be removed after everything works.\n      - emptyDir:\n          medium: Memory\n        name: local-certs\n      - name: istio-token\n        projected:\n          sources:\n            - serviceAccountToken:\n                audience: istio-ca\n                expirationSeconds: 43200\n                path: istio-token\n      # Optional: user-generated root\n      - name: cacerts\n        secret:\n          secretName: cacerts\n          optional: true\n      - name: istio-kubeconfig\n        secret:\n          secretName: istio-kubeconfig\n          optional: true\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-discovery/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 147, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    app: istiod\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: \"Pilot\"\n    istio: pilot\n    release: istio-discovery\nspec:\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 25%\n  selector:\n    matchLabels:\n      istio: pilot\n  template:\n    metadata:\n      labels:\n        app: istiod\n        istio.io/rev: default\n        install.operator.istio.io/owning-resource: unknown\n        sidecar.istio.io/inject: \"false\"\n        operator.istio.io/component: \"Pilot\"\n        istio: pilot\n      annotations:\n        prometheus.io/port: \"15014\"\n        prometheus.io/scrape: \"true\"\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: istiod-service-account\n      securityContext:\n        fsGroup: 1337\n      containers:\n        - name: discovery\n          image: \"docker.io/istio/pilot:1.10.4\"\n          args:\n          - \"discovery\"\n          - --monitoringAddr=:15014\n          - --log_output_level=default:info\n          - --domain\n          - cluster.local\n          - --keepaliveMaxServerConnectionAge\n          - \"30m\"\n          ports:\n          - containerPort: 8080\n            protocol: TCP\n          - containerPort: 15010\n            protocol: TCP\n          - containerPort: 15017\n            protocol: TCP\n          readinessProbe:\n            httpGet:\n              path: /ready\n              port: 8080\n            initialDelaySeconds: 1\n            periodSeconds: 3\n            timeoutSeconds: 5\n          env:\n          - name: REVISION\n            value: \"default\"\n          - name: JWT_POLICY\n            value: third-party-jwt\n          - name: PILOT_CERT_PROVIDER\n            value: istiod\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.namespace\n          - name: SERVICE_ACCOUNT\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: spec.serviceAccountName\n          - name: KUBECONFIG\n            value: /var/run/secrets/remote/config\n          - name: ENABLE_LEGACY_FSGROUP_INJECTION\n            value: \"false\"\n          - name: PILOT_TRACE_SAMPLING\n            value: \"1\"\n          - name: PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_OUTBOUND\n            value: \"true\"\n          - name: PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_INBOUND\n            value: \"true\"\n          - name: ISTIOD_ADDR\n            value: istiod.default.svc:15012\n          - name: PILOT_ENABLE_ANALYSIS\n            value: \"false\"\n          - name: CLUSTER_ID\n            value: \"Kubernetes\"\n          resources:\n            requests:\n              cpu: 500m\n              memory: 2048Mi\n          securityContext:\n            runAsUser: 1337\n            runAsGroup: 1337\n            runAsNonRoot: true\n            capabilities:\n              drop:\n              - ALL\n          volumeMounts:\n          - name: istio-token\n            mountPath: /var/run/secrets/tokens\n            readOnly: true\n          - name: local-certs\n            mountPath: /var/run/secrets/istio-dns\n          - name: cacerts\n            mountPath: /etc/cacerts\n            readOnly: true\n          - name: istio-kubeconfig\n            mountPath: /var/run/secrets/remote\n            readOnly: true\n      volumes:\n      # Technically not needed on this pod - but it helps debugging/testing SDS\n      # Should be removed after everything works.\n      - emptyDir:\n          medium: Memory\n        name: local-certs\n      - name: istio-token\n        projected:\n          sources:\n            - serviceAccountToken:\n                audience: istio-ca\n                expirationSeconds: 43200\n                path: istio-token\n      # Optional: user-generated root\n      - name: cacerts\n        secret:\n          secretName: cacerts\n          optional: true\n      - name: istio-kubeconfig\n        secret:\n          secretName: istio-kubeconfig\n          optional: true\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-discovery/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 147, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    app: istiod\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: \"Pilot\"\n    istio: pilot\n    release: istio-discovery\nspec:\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 25%\n  selector:\n    matchLabels:\n      istio: pilot\n  template:\n    metadata:\n      labels:\n        app: istiod\n        istio.io/rev: default\n        install.operator.istio.io/owning-resource: unknown\n        sidecar.istio.io/inject: \"false\"\n        operator.istio.io/component: \"Pilot\"\n        istio: pilot\n      annotations:\n        prometheus.io/port: \"15014\"\n        prometheus.io/scrape: \"true\"\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: istiod-service-account\n      securityContext:\n        fsGroup: 1337\n      containers:\n        - name: discovery\n          image: \"docker.io/istio/pilot:1.10.4\"\n          args:\n          - \"discovery\"\n          - --monitoringAddr=:15014\n          - --log_output_level=default:info\n          - --domain\n          - cluster.local\n          - --keepaliveMaxServerConnectionAge\n          - \"30m\"\n          ports:\n          - containerPort: 8080\n            protocol: TCP\n          - containerPort: 15010\n            protocol: TCP\n          - containerPort: 15017\n            protocol: TCP\n          readinessProbe:\n            httpGet:\n              path: /ready\n              port: 8080\n            initialDelaySeconds: 1\n            periodSeconds: 3\n            timeoutSeconds: 5\n          env:\n          - name: REVISION\n            value: \"default\"\n          - name: JWT_POLICY\n            value: third-party-jwt\n          - name: PILOT_CERT_PROVIDER\n            value: istiod\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.namespace\n          - name: SERVICE_ACCOUNT\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: spec.serviceAccountName\n          - name: KUBECONFIG\n            value: /var/run/secrets/remote/config\n          - name: ENABLE_LEGACY_FSGROUP_INJECTION\n            value: \"false\"\n          - name: PILOT_TRACE_SAMPLING\n            value: \"1\"\n          - name: PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_OUTBOUND\n            value: \"true\"\n          - name: PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_INBOUND\n            value: \"true\"\n          - name: ISTIOD_ADDR\n            value: istiod.default.svc:15012\n          - name: PILOT_ENABLE_ANALYSIS\n            value: \"false\"\n          - name: CLUSTER_ID\n            value: \"Kubernetes\"\n          resources:\n            requests:\n              cpu: 500m\n              memory: 2048Mi\n          securityContext:\n            runAsUser: 1337\n            runAsGroup: 1337\n            runAsNonRoot: true\n            capabilities:\n              drop:\n              - ALL\n          volumeMounts:\n          - name: istio-token\n            mountPath: /var/run/secrets/tokens\n            readOnly: true\n          - name: local-certs\n            mountPath: /var/run/secrets/istio-dns\n          - name: cacerts\n            mountPath: /etc/cacerts\n            readOnly: true\n          - name: istio-kubeconfig\n            mountPath: /var/run/secrets/remote\n            readOnly: true\n      volumes:\n      # Technically not needed on this pod - but it helps debugging/testing SDS\n      # Should be removed after everything works.\n      - emptyDir:\n          medium: Memory\n        name: local-certs\n      - name: istio-token\n        projected:\n          sources:\n            - serviceAccountToken:\n                audience: istio-ca\n                expirationSeconds: 43200\n                path: istio-token\n      # Optional: user-generated root\n      - name: cacerts\n        secret:\n          secretName: cacerts\n          optional: true\n      - name: istio-kubeconfig\n        secret:\n          secretName: istio-kubeconfig\n          optional: true\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-discovery/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 147, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    app: istiod\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: \"Pilot\"\n    istio: pilot\n    release: istio-discovery\nspec:\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 25%\n  selector:\n    matchLabels:\n      istio: pilot\n  template:\n    metadata:\n      labels:\n        app: istiod\n        istio.io/rev: default\n        install.operator.istio.io/owning-resource: unknown\n        sidecar.istio.io/inject: \"false\"\n        operator.istio.io/component: \"Pilot\"\n        istio: pilot\n      annotations:\n        prometheus.io/port: \"15014\"\n        prometheus.io/scrape: \"true\"\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: istiod-service-account\n      securityContext:\n        fsGroup: 1337\n      containers:\n        - name: discovery\n          image: \"docker.io/istio/pilot:1.10.4\"\n          args:\n          - \"discovery\"\n          - --monitoringAddr=:15014\n          - --log_output_level=default:info\n          - --domain\n          - cluster.local\n          - --keepaliveMaxServerConnectionAge\n          - \"30m\"\n          ports:\n          - containerPort: 8080\n            protocol: TCP\n          - containerPort: 15010\n            protocol: TCP\n          - containerPort: 15017\n            protocol: TCP\n          readinessProbe:\n            httpGet:\n              path: /ready\n              port: 8080\n            initialDelaySeconds: 1\n            periodSeconds: 3\n            timeoutSeconds: 5\n          env:\n          - name: REVISION\n            value: \"default\"\n          - name: JWT_POLICY\n            value: third-party-jwt\n          - name: PILOT_CERT_PROVIDER\n            value: istiod\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.namespace\n          - name: SERVICE_ACCOUNT\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: spec.serviceAccountName\n          - name: KUBECONFIG\n            value: /var/run/secrets/remote/config\n          - name: ENABLE_LEGACY_FSGROUP_INJECTION\n            value: \"false\"\n          - name: PILOT_TRACE_SAMPLING\n            value: \"1\"\n          - name: PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_OUTBOUND\n            value: \"true\"\n          - name: PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_INBOUND\n            value: \"true\"\n          - name: ISTIOD_ADDR\n            value: istiod.default.svc:15012\n          - name: PILOT_ENABLE_ANALYSIS\n            value: \"false\"\n          - name: CLUSTER_ID\n            value: \"Kubernetes\"\n          resources:\n            requests:\n              cpu: 500m\n              memory: 2048Mi\n          securityContext:\n            runAsUser: 1337\n            runAsGroup: 1337\n            runAsNonRoot: true\n            capabilities:\n              drop:\n              - ALL\n          volumeMounts:\n          - name: istio-token\n            mountPath: /var/run/secrets/tokens\n            readOnly: true\n          - name: local-certs\n            mountPath: /var/run/secrets/istio-dns\n          - name: cacerts\n            mountPath: /etc/cacerts\n            readOnly: true\n          - name: istio-kubeconfig\n            mountPath: /var/run/secrets/remote\n            readOnly: true\n      volumes:\n      # Technically not needed on this pod - but it helps debugging/testing SDS\n      # Should be removed after everything works.\n      - emptyDir:\n          medium: Memory\n        name: local-certs\n      - name: istio-token\n        projected:\n          sources:\n            - serviceAccountToken:\n                audience: istio-ca\n                expirationSeconds: 43200\n                path: istio-token\n      # Optional: user-generated root\n      - name: cacerts\n        secret:\n          secretName: cacerts\n          optional: true\n      - name: istio-kubeconfig\n        secret:\n          secretName: istio-kubeconfig\n          optional: true\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-discovery/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 147, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    app: istiod\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: \"Pilot\"\n    istio: pilot\n    release: istio-discovery\nspec:\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 25%\n  selector:\n    matchLabels:\n      istio: pilot\n  template:\n    metadata:\n      labels:\n        app: istiod\n        istio.io/rev: default\n        install.operator.istio.io/owning-resource: unknown\n        sidecar.istio.io/inject: \"false\"\n        operator.istio.io/component: \"Pilot\"\n        istio: pilot\n      annotations:\n        prometheus.io/port: \"15014\"\n        prometheus.io/scrape: \"true\"\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: istiod-service-account\n      securityContext:\n        fsGroup: 1337\n      containers:\n        - name: discovery\n          image: \"docker.io/istio/pilot:1.10.4\"\n          args:\n          - \"discovery\"\n          - --monitoringAddr=:15014\n          - --log_output_level=default:info\n          - --domain\n          - cluster.local\n          - --keepaliveMaxServerConnectionAge\n          - \"30m\"\n          ports:\n          - containerPort: 8080\n            protocol: TCP\n          - containerPort: 15010\n            protocol: TCP\n          - containerPort: 15017\n            protocol: TCP\n          readinessProbe:\n            httpGet:\n              path: /ready\n              port: 8080\n            initialDelaySeconds: 1\n            periodSeconds: 3\n            timeoutSeconds: 5\n          env:\n          - name: REVISION\n            value: \"default\"\n          - name: JWT_POLICY\n            value: third-party-jwt\n          - name: PILOT_CERT_PROVIDER\n            value: istiod\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.namespace\n          - name: SERVICE_ACCOUNT\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: spec.serviceAccountName\n          - name: KUBECONFIG\n            value: /var/run/secrets/remote/config\n          - name: ENABLE_LEGACY_FSGROUP_INJECTION\n            value: \"false\"\n          - name: PILOT_TRACE_SAMPLING\n            value: \"1\"\n          - name: PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_OUTBOUND\n            value: \"true\"\n          - name: PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_INBOUND\n            value: \"true\"\n          - name: ISTIOD_ADDR\n            value: istiod.default.svc:15012\n          - name: PILOT_ENABLE_ANALYSIS\n            value: \"false\"\n          - name: CLUSTER_ID\n            value: \"Kubernetes\"\n          resources:\n            requests:\n              cpu: 500m\n              memory: 2048Mi\n          securityContext:\n            runAsUser: 1337\n            runAsGroup: 1337\n            runAsNonRoot: true\n            capabilities:\n              drop:\n              - ALL\n          volumeMounts:\n          - name: istio-token\n            mountPath: /var/run/secrets/tokens\n            readOnly: true\n          - name: local-certs\n            mountPath: /var/run/secrets/istio-dns\n          - name: cacerts\n            mountPath: /etc/cacerts\n            readOnly: true\n          - name: istio-kubeconfig\n            mountPath: /var/run/secrets/remote\n            readOnly: true\n      volumes:\n      # Technically not needed on this pod - but it helps debugging/testing SDS\n      # Should be removed after everything works.\n      - emptyDir:\n          medium: Memory\n        name: local-certs\n      - name: istio-token\n        projected:\n          sources:\n            - serviceAccountToken:\n                audience: istio-ca\n                expirationSeconds: 43200\n                path: istio-token\n      # Optional: user-generated root\n      - name: cacerts\n        secret:\n          secretName: cacerts\n          optional: true\n      - name: istio-kubeconfig\n        secret:\n          secretName: istio-kubeconfig\n          optional: true\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-discovery/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 147, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    app: istiod\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: \"Pilot\"\n    istio: pilot\n    release: istio-discovery\nspec:\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 25%\n  selector:\n    matchLabels:\n      istio: pilot\n  template:\n    metadata:\n      labels:\n        app: istiod\n        istio.io/rev: default\n        install.operator.istio.io/owning-resource: unknown\n        sidecar.istio.io/inject: \"false\"\n        operator.istio.io/component: \"Pilot\"\n        istio: pilot\n      annotations:\n        prometheus.io/port: \"15014\"\n        prometheus.io/scrape: \"true\"\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: istiod-service-account\n      securityContext:\n        fsGroup: 1337\n      containers:\n        - name: discovery\n          image: \"docker.io/istio/pilot:1.10.4\"\n          args:\n          - \"discovery\"\n          - --monitoringAddr=:15014\n          - --log_output_level=default:info\n          - --domain\n          - cluster.local\n          - --keepaliveMaxServerConnectionAge\n          - \"30m\"\n          ports:\n          - containerPort: 8080\n            protocol: TCP\n          - containerPort: 15010\n            protocol: TCP\n          - containerPort: 15017\n            protocol: TCP\n          readinessProbe:\n            httpGet:\n              path: /ready\n              port: 8080\n            initialDelaySeconds: 1\n            periodSeconds: 3\n            timeoutSeconds: 5\n          env:\n          - name: REVISION\n            value: \"default\"\n          - name: JWT_POLICY\n            value: third-party-jwt\n          - name: PILOT_CERT_PROVIDER\n            value: istiod\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.namespace\n          - name: SERVICE_ACCOUNT\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: spec.serviceAccountName\n          - name: KUBECONFIG\n            value: /var/run/secrets/remote/config\n          - name: ENABLE_LEGACY_FSGROUP_INJECTION\n            value: \"false\"\n          - name: PILOT_TRACE_SAMPLING\n            value: \"1\"\n          - name: PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_OUTBOUND\n            value: \"true\"\n          - name: PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_INBOUND\n            value: \"true\"\n          - name: ISTIOD_ADDR\n            value: istiod.default.svc:15012\n          - name: PILOT_ENABLE_ANALYSIS\n            value: \"false\"\n          - name: CLUSTER_ID\n            value: \"Kubernetes\"\n          resources:\n            requests:\n              cpu: 500m\n              memory: 2048Mi\n          securityContext:\n            runAsUser: 1337\n            runAsGroup: 1337\n            runAsNonRoot: true\n            capabilities:\n              drop:\n              - ALL\n          volumeMounts:\n          - name: istio-token\n            mountPath: /var/run/secrets/tokens\n            readOnly: true\n          - name: local-certs\n            mountPath: /var/run/secrets/istio-dns\n          - name: cacerts\n            mountPath: /etc/cacerts\n            readOnly: true\n          - name: istio-kubeconfig\n            mountPath: /var/run/secrets/remote\n            readOnly: true\n      volumes:\n      # Technically not needed on this pod - but it helps debugging/testing SDS\n      # Should be removed after everything works.\n      - emptyDir:\n          medium: Memory\n        name: local-certs\n      - name: istio-token\n        projected:\n          sources:\n            - serviceAccountToken:\n                audience: istio-ca\n                expirationSeconds: 43200\n                path: istio-token\n      # Optional: user-generated root\n      - name: cacerts\n        secret:\n          secretName: cacerts\n          optional: true\n      - name: istio-kubeconfig\n        secret:\n          secretName: istio-kubeconfig\n          optional: true\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-discovery/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 147, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    app: istiod\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: \"Pilot\"\n    istio: pilot\n    release: istio-discovery\nspec:\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 25%\n  selector:\n    matchLabels:\n      istio: pilot\n  template:\n    metadata:\n      labels:\n        app: istiod\n        istio.io/rev: default\n        install.operator.istio.io/owning-resource: unknown\n        sidecar.istio.io/inject: \"false\"\n        operator.istio.io/component: \"Pilot\"\n        istio: pilot\n      annotations:\n        prometheus.io/port: \"15014\"\n        prometheus.io/scrape: \"true\"\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: istiod-service-account\n      securityContext:\n        fsGroup: 1337\n      containers:\n        - name: discovery\n          image: \"docker.io/istio/pilot:1.10.4\"\n          args:\n          - \"discovery\"\n          - --monitoringAddr=:15014\n          - --log_output_level=default:info\n          - --domain\n          - cluster.local\n          - --keepaliveMaxServerConnectionAge\n          - \"30m\"\n          ports:\n          - containerPort: 8080\n            protocol: TCP\n          - containerPort: 15010\n            protocol: TCP\n          - containerPort: 15017\n            protocol: TCP\n          readinessProbe:\n            httpGet:\n              path: /ready\n              port: 8080\n            initialDelaySeconds: 1\n            periodSeconds: 3\n            timeoutSeconds: 5\n          env:\n          - name: REVISION\n            value: \"default\"\n          - name: JWT_POLICY\n            value: third-party-jwt\n          - name: PILOT_CERT_PROVIDER\n            value: istiod\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.namespace\n          - name: SERVICE_ACCOUNT\n            valueFrom:\n              fieldRef:\n                apiVersion: v1\n                fieldPath: spec.serviceAccountName\n          - name: KUBECONFIG\n            value: /var/run/secrets/remote/config\n          - name: ENABLE_LEGACY_FSGROUP_INJECTION\n            value: \"false\"\n          - name: PILOT_TRACE_SAMPLING\n            value: \"1\"\n          - name: PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_OUTBOUND\n            value: \"true\"\n          - name: PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_INBOUND\n            value: \"true\"\n          - name: ISTIOD_ADDR\n            value: istiod.default.svc:15012\n          - name: PILOT_ENABLE_ANALYSIS\n            value: \"false\"\n          - name: CLUSTER_ID\n            value: \"Kubernetes\"\n          resources:\n            requests:\n              cpu: 500m\n              memory: 2048Mi\n          securityContext:\n            runAsUser: 1337\n            runAsGroup: 1337\n            runAsNonRoot: true\n            capabilities:\n              drop:\n              - ALL\n          volumeMounts:\n          - name: istio-token\n            mountPath: /var/run/secrets/tokens\n            readOnly: true\n          - name: local-certs\n            mountPath: /var/run/secrets/istio-dns\n          - name: cacerts\n            mountPath: /etc/cacerts\n            readOnly: true\n          - name: istio-kubeconfig\n            mountPath: /var/run/secrets/remote\n            readOnly: true\n      volumes:\n      # Technically not needed on this pod - but it helps debugging/testing SDS\n      # Should be removed after everything works.\n      - emptyDir:\n          medium: Memory\n        name: local-certs\n      - name: istio-token\n        projected:\n          sources:\n            - serviceAccountToken:\n                audience: istio-ca\n                expirationSeconds: 43200\n                path: istio-token\n      # Optional: user-generated root\n      - name: cacerts\n        secret:\n          secretName: cacerts\n          optional: true\n      - name: istio-kubeconfig\n        secret:\n          secretName: istio-kubeconfig\n          optional: true\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-discovery/templates/service.yaml"}, "region": {"startLine": 3, "endLine": 34, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: \"Pilot\"\n    app: istiod\n    istio: pilot\n    release: istio-discovery\nspec:\n  ports:\n    - port: 15010\n      name: grpc-xds # plaintext\n      protocol: TCP\n    - port: 15012\n      name: https-dns # mTLS with k8s-signed cert\n      protocol: TCP\n    - port: 443\n      name: https-webhook # validation and injection\n      targetPort: 15017\n      protocol: TCP\n    - port: 15014\n      name: http-monitoring # prometheus stats\n      protocol: TCP\n  selector:\n    app: istiod\n    # Label used by the 'default' service. For versioned deployments we match with app and version.\n    # This avoids default deployment picking the canary\n    istio: pilot\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/istio-discovery/templates/configmap.yaml"}, "region": {"startLine": 3, "endLine": 27, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: istio\n  namespace: default\n  labels:\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: \"Pilot\"\n    release: istio-discovery\ndata:\n\n  # Configuration file for the mesh networks to be used by the Split Horizon EDS.\n  meshNetworks: |-\n    networks: {}\n\n  mesh: |-\n    defaultConfig:\n      discoveryAddress: istiod.default.svc:15012\n      tracing:\n        zipkin:\n          address: zipkin.istio-system:9411\n    enablePrometheusMerge: true\n    rootNamespace: null\n    trustDomain: cluster.local\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/service.yaml"}, "region": {"startLine": 3, "endLine": 18, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kafka\n  labels:\n    app: kafka\n    chart: kafka-0.1.0\n    release: kafka\n    heritage: Helm\nspec:\n  ports:\n    - port: 9092\n      name: broker\n  selector:\n    app: kafka\n    release: kafka\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/headless-service.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kafka-headless\n  labels:\n    app: kafka\n    chart: kafka-0.1.0\n    release: kafka\n    heritage: Helm\nspec:\n  ports:\n    - port: 9092\n      name: broker\n  clusterIP: None\n  selector:\n    app: kafka\n    release: kafka\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  labels:\n    app: kafka\n    chart: kafka-0.1.0\n    release: kafka\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: kafka\n      release: kafka\n  serviceName: kafka-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: kafka\n        release: kafka\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - kafka\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - kafka\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: kafka-broker\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 9092\n          name: kafka\n        resources:\n          {}\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: KAFKA_ZOOKEEPER_CONNECT\n          value: \"usp-zookeeper:2181\"\n        - name: KAFKA_LOG_DIRS\n          value: \"/opt/kafka/data-0/logs\"\n#        - name: KAFKA_METRIC_REPORTERS\n#          value: \"io.confluent.metrics.reporter.ConfluentMetricsReporter\"\n        - name: CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS\n          value: \"PLAINTEXT://kafka-kafka-headless:9092\"\n        - name: \"KAFKA_LISTENER_SECURITY_PROTOCOL_MAP\"\n          value: \"PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT\"\n        - name: \"KAFKA_MESSAGE_MAX_BYTES\"\n          value: \"6291456\"\n        - name: \"KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR\"\n          value: \"3\"\n        - name: \"KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES\"\n          value: \"131072\"\n        - name: KAFKA_JMX_PORT\n          value: \"5555\"\n        # This is required because the Downward API does not yet support identification of\n        # pod numbering in statefulsets. Thus, we are required to specify a command which\n        # allows us to extract the pod ID for usage as the Kafka Broker ID.\n        # See: https://github.com/kubernetes/kubernetes/issues/31218\n        command:\n        - sh\n        - -exc\n        - |\n          export KAFKA_BROKER_ID=${HOSTNAME##*-} && \\\n          export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_NAME}.kafka-headless.${POD_NAMESPACE}:9092,EXTERNAL://${HOST_IP}:$((31090 + ${KAFKA_BROKER_ID})) && \\\n          exec /etc/confluent/docker/run\n        volumeMounts:\n          - name: datadir\n            mountPath: /opt/kafka/data-0\n      volumes:\n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  labels:\n    app: kafka\n    chart: kafka-0.1.0\n    release: kafka\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: kafka\n      release: kafka\n  serviceName: kafka-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: kafka\n        release: kafka\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - kafka\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - kafka\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: kafka-broker\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 9092\n          name: kafka\n        resources:\n          {}\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: KAFKA_ZOOKEEPER_CONNECT\n          value: \"usp-zookeeper:2181\"\n        - name: KAFKA_LOG_DIRS\n          value: \"/opt/kafka/data-0/logs\"\n#        - name: KAFKA_METRIC_REPORTERS\n#          value: \"io.confluent.metrics.reporter.ConfluentMetricsReporter\"\n        - name: CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS\n          value: \"PLAINTEXT://kafka-kafka-headless:9092\"\n        - name: \"KAFKA_LISTENER_SECURITY_PROTOCOL_MAP\"\n          value: \"PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT\"\n        - name: \"KAFKA_MESSAGE_MAX_BYTES\"\n          value: \"6291456\"\n        - name: \"KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR\"\n          value: \"3\"\n        - name: \"KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES\"\n          value: \"131072\"\n        - name: KAFKA_JMX_PORT\n          value: \"5555\"\n        # This is required because the Downward API does not yet support identification of\n        # pod numbering in statefulsets. Thus, we are required to specify a command which\n        # allows us to extract the pod ID for usage as the Kafka Broker ID.\n        # See: https://github.com/kubernetes/kubernetes/issues/31218\n        command:\n        - sh\n        - -exc\n        - |\n          export KAFKA_BROKER_ID=${HOSTNAME##*-} && \\\n          export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_NAME}.kafka-headless.${POD_NAMESPACE}:9092,EXTERNAL://${HOST_IP}:$((31090 + ${KAFKA_BROKER_ID})) && \\\n          exec /etc/confluent/docker/run\n        volumeMounts:\n          - name: datadir\n            mountPath: /opt/kafka/data-0\n      volumes:\n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  labels:\n    app: kafka\n    chart: kafka-0.1.0\n    release: kafka\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: kafka\n      release: kafka\n  serviceName: kafka-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: kafka\n        release: kafka\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - kafka\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - kafka\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: kafka-broker\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 9092\n          name: kafka\n        resources:\n          {}\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: KAFKA_ZOOKEEPER_CONNECT\n          value: \"usp-zookeeper:2181\"\n        - name: KAFKA_LOG_DIRS\n          value: \"/opt/kafka/data-0/logs\"\n#        - name: KAFKA_METRIC_REPORTERS\n#          value: \"io.confluent.metrics.reporter.ConfluentMetricsReporter\"\n        - name: CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS\n          value: \"PLAINTEXT://kafka-kafka-headless:9092\"\n        - name: \"KAFKA_LISTENER_SECURITY_PROTOCOL_MAP\"\n          value: \"PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT\"\n        - name: \"KAFKA_MESSAGE_MAX_BYTES\"\n          value: \"6291456\"\n        - name: \"KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR\"\n          value: \"3\"\n        - name: \"KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES\"\n          value: \"131072\"\n        - name: KAFKA_JMX_PORT\n          value: \"5555\"\n        # This is required because the Downward API does not yet support identification of\n        # pod numbering in statefulsets. Thus, we are required to specify a command which\n        # allows us to extract the pod ID for usage as the Kafka Broker ID.\n        # See: https://github.com/kubernetes/kubernetes/issues/31218\n        command:\n        - sh\n        - -exc\n        - |\n          export KAFKA_BROKER_ID=${HOSTNAME##*-} && \\\n          export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_NAME}.kafka-headless.${POD_NAMESPACE}:9092,EXTERNAL://${HOST_IP}:$((31090 + ${KAFKA_BROKER_ID})) && \\\n          exec /etc/confluent/docker/run\n        volumeMounts:\n          - name: datadir\n            mountPath: /opt/kafka/data-0\n      volumes:\n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  labels:\n    app: kafka\n    chart: kafka-0.1.0\n    release: kafka\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: kafka\n      release: kafka\n  serviceName: kafka-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: kafka\n        release: kafka\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - kafka\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - kafka\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: kafka-broker\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 9092\n          name: kafka\n        resources:\n          {}\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: KAFKA_ZOOKEEPER_CONNECT\n          value: \"usp-zookeeper:2181\"\n        - name: KAFKA_LOG_DIRS\n          value: \"/opt/kafka/data-0/logs\"\n#        - name: KAFKA_METRIC_REPORTERS\n#          value: \"io.confluent.metrics.reporter.ConfluentMetricsReporter\"\n        - name: CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS\n          value: \"PLAINTEXT://kafka-kafka-headless:9092\"\n        - name: \"KAFKA_LISTENER_SECURITY_PROTOCOL_MAP\"\n          value: \"PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT\"\n        - name: \"KAFKA_MESSAGE_MAX_BYTES\"\n          value: \"6291456\"\n        - name: \"KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR\"\n          value: \"3\"\n        - name: \"KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES\"\n          value: \"131072\"\n        - name: KAFKA_JMX_PORT\n          value: \"5555\"\n        # This is required because the Downward API does not yet support identification of\n        # pod numbering in statefulsets. Thus, we are required to specify a command which\n        # allows us to extract the pod ID for usage as the Kafka Broker ID.\n        # See: https://github.com/kubernetes/kubernetes/issues/31218\n        command:\n        - sh\n        - -exc\n        - |\n          export KAFKA_BROKER_ID=${HOSTNAME##*-} && \\\n          export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_NAME}.kafka-headless.${POD_NAMESPACE}:9092,EXTERNAL://${HOST_IP}:$((31090 + ${KAFKA_BROKER_ID})) && \\\n          exec /etc/confluent/docker/run\n        volumeMounts:\n          - name: datadir\n            mountPath: /opt/kafka/data-0\n      volumes:\n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  labels:\n    app: kafka\n    chart: kafka-0.1.0\n    release: kafka\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: kafka\n      release: kafka\n  serviceName: kafka-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: kafka\n        release: kafka\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - kafka\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - kafka\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: kafka-broker\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 9092\n          name: kafka\n        resources:\n          {}\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: KAFKA_ZOOKEEPER_CONNECT\n          value: \"usp-zookeeper:2181\"\n        - name: KAFKA_LOG_DIRS\n          value: \"/opt/kafka/data-0/logs\"\n#        - name: KAFKA_METRIC_REPORTERS\n#          value: \"io.confluent.metrics.reporter.ConfluentMetricsReporter\"\n        - name: CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS\n          value: \"PLAINTEXT://kafka-kafka-headless:9092\"\n        - name: \"KAFKA_LISTENER_SECURITY_PROTOCOL_MAP\"\n          value: \"PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT\"\n        - name: \"KAFKA_MESSAGE_MAX_BYTES\"\n          value: \"6291456\"\n        - name: \"KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR\"\n          value: \"3\"\n        - name: \"KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES\"\n          value: \"131072\"\n        - name: KAFKA_JMX_PORT\n          value: \"5555\"\n        # This is required because the Downward API does not yet support identification of\n        # pod numbering in statefulsets. Thus, we are required to specify a command which\n        # allows us to extract the pod ID for usage as the Kafka Broker ID.\n        # See: https://github.com/kubernetes/kubernetes/issues/31218\n        command:\n        - sh\n        - -exc\n        - |\n          export KAFKA_BROKER_ID=${HOSTNAME##*-} && \\\n          export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_NAME}.kafka-headless.${POD_NAMESPACE}:9092,EXTERNAL://${HOST_IP}:$((31090 + ${KAFKA_BROKER_ID})) && \\\n          exec /etc/confluent/docker/run\n        volumeMounts:\n          - name: datadir\n            mountPath: /opt/kafka/data-0\n      volumes:\n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  labels:\n    app: kafka\n    chart: kafka-0.1.0\n    release: kafka\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: kafka\n      release: kafka\n  serviceName: kafka-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: kafka\n        release: kafka\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - kafka\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - kafka\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: kafka-broker\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 9092\n          name: kafka\n        resources:\n          {}\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: KAFKA_ZOOKEEPER_CONNECT\n          value: \"usp-zookeeper:2181\"\n        - name: KAFKA_LOG_DIRS\n          value: \"/opt/kafka/data-0/logs\"\n#        - name: KAFKA_METRIC_REPORTERS\n#          value: \"io.confluent.metrics.reporter.ConfluentMetricsReporter\"\n        - name: CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS\n          value: \"PLAINTEXT://kafka-kafka-headless:9092\"\n        - name: \"KAFKA_LISTENER_SECURITY_PROTOCOL_MAP\"\n          value: \"PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT\"\n        - name: \"KAFKA_MESSAGE_MAX_BYTES\"\n          value: \"6291456\"\n        - name: \"KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR\"\n          value: \"3\"\n        - name: \"KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES\"\n          value: \"131072\"\n        - name: KAFKA_JMX_PORT\n          value: \"5555\"\n        # This is required because the Downward API does not yet support identification of\n        # pod numbering in statefulsets. Thus, we are required to specify a command which\n        # allows us to extract the pod ID for usage as the Kafka Broker ID.\n        # See: https://github.com/kubernetes/kubernetes/issues/31218\n        command:\n        - sh\n        - -exc\n        - |\n          export KAFKA_BROKER_ID=${HOSTNAME##*-} && \\\n          export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_NAME}.kafka-headless.${POD_NAMESPACE}:9092,EXTERNAL://${HOST_IP}:$((31090 + ${KAFKA_BROKER_ID})) && \\\n          exec /etc/confluent/docker/run\n        volumeMounts:\n          - name: datadir\n            mountPath: /opt/kafka/data-0\n      volumes:\n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  labels:\n    app: kafka\n    chart: kafka-0.1.0\n    release: kafka\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: kafka\n      release: kafka\n  serviceName: kafka-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: kafka\n        release: kafka\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - kafka\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - kafka\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: kafka-broker\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 9092\n          name: kafka\n        resources:\n          {}\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: KAFKA_ZOOKEEPER_CONNECT\n          value: \"usp-zookeeper:2181\"\n        - name: KAFKA_LOG_DIRS\n          value: \"/opt/kafka/data-0/logs\"\n#        - name: KAFKA_METRIC_REPORTERS\n#          value: \"io.confluent.metrics.reporter.ConfluentMetricsReporter\"\n        - name: CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS\n          value: \"PLAINTEXT://kafka-kafka-headless:9092\"\n        - name: \"KAFKA_LISTENER_SECURITY_PROTOCOL_MAP\"\n          value: \"PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT\"\n        - name: \"KAFKA_MESSAGE_MAX_BYTES\"\n          value: \"6291456\"\n        - name: \"KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR\"\n          value: \"3\"\n        - name: \"KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES\"\n          value: \"131072\"\n        - name: KAFKA_JMX_PORT\n          value: \"5555\"\n        # This is required because the Downward API does not yet support identification of\n        # pod numbering in statefulsets. Thus, we are required to specify a command which\n        # allows us to extract the pod ID for usage as the Kafka Broker ID.\n        # See: https://github.com/kubernetes/kubernetes/issues/31218\n        command:\n        - sh\n        - -exc\n        - |\n          export KAFKA_BROKER_ID=${HOSTNAME##*-} && \\\n          export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_NAME}.kafka-headless.${POD_NAMESPACE}:9092,EXTERNAL://${HOST_IP}:$((31090 + ${KAFKA_BROKER_ID})) && \\\n          exec /etc/confluent/docker/run\n        volumeMounts:\n          - name: datadir\n            mountPath: /opt/kafka/data-0\n      volumes:\n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  labels:\n    app: kafka\n    chart: kafka-0.1.0\n    release: kafka\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: kafka\n      release: kafka\n  serviceName: kafka-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: kafka\n        release: kafka\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - kafka\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - kafka\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: kafka-broker\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 9092\n          name: kafka\n        resources:\n          {}\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: KAFKA_ZOOKEEPER_CONNECT\n          value: \"usp-zookeeper:2181\"\n        - name: KAFKA_LOG_DIRS\n          value: \"/opt/kafka/data-0/logs\"\n#        - name: KAFKA_METRIC_REPORTERS\n#          value: \"io.confluent.metrics.reporter.ConfluentMetricsReporter\"\n        - name: CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS\n          value: \"PLAINTEXT://kafka-kafka-headless:9092\"\n        - name: \"KAFKA_LISTENER_SECURITY_PROTOCOL_MAP\"\n          value: \"PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT\"\n        - name: \"KAFKA_MESSAGE_MAX_BYTES\"\n          value: \"6291456\"\n        - name: \"KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR\"\n          value: \"3\"\n        - name: \"KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES\"\n          value: \"131072\"\n        - name: KAFKA_JMX_PORT\n          value: \"5555\"\n        # This is required because the Downward API does not yet support identification of\n        # pod numbering in statefulsets. Thus, we are required to specify a command which\n        # allows us to extract the pod ID for usage as the Kafka Broker ID.\n        # See: https://github.com/kubernetes/kubernetes/issues/31218\n        command:\n        - sh\n        - -exc\n        - |\n          export KAFKA_BROKER_ID=${HOSTNAME##*-} && \\\n          export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_NAME}.kafka-headless.${POD_NAMESPACE}:9092,EXTERNAL://${HOST_IP}:$((31090 + ${KAFKA_BROKER_ID})) && \\\n          exec /etc/confluent/docker/run\n        volumeMounts:\n          - name: datadir\n            mountPath: /opt/kafka/data-0\n      volumes:\n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  labels:\n    app: kafka\n    chart: kafka-0.1.0\n    release: kafka\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: kafka\n      release: kafka\n  serviceName: kafka-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: kafka\n        release: kafka\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - kafka\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - kafka\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: kafka-broker\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 9092\n          name: kafka\n        resources:\n          {}\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: KAFKA_ZOOKEEPER_CONNECT\n          value: \"usp-zookeeper:2181\"\n        - name: KAFKA_LOG_DIRS\n          value: \"/opt/kafka/data-0/logs\"\n#        - name: KAFKA_METRIC_REPORTERS\n#          value: \"io.confluent.metrics.reporter.ConfluentMetricsReporter\"\n        - name: CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS\n          value: \"PLAINTEXT://kafka-kafka-headless:9092\"\n        - name: \"KAFKA_LISTENER_SECURITY_PROTOCOL_MAP\"\n          value: \"PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT\"\n        - name: \"KAFKA_MESSAGE_MAX_BYTES\"\n          value: \"6291456\"\n        - name: \"KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR\"\n          value: \"3\"\n        - name: \"KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES\"\n          value: \"131072\"\n        - name: KAFKA_JMX_PORT\n          value: \"5555\"\n        # This is required because the Downward API does not yet support identification of\n        # pod numbering in statefulsets. Thus, we are required to specify a command which\n        # allows us to extract the pod ID for usage as the Kafka Broker ID.\n        # See: https://github.com/kubernetes/kubernetes/issues/31218\n        command:\n        - sh\n        - -exc\n        - |\n          export KAFKA_BROKER_ID=${HOSTNAME##*-} && \\\n          export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_NAME}.kafka-headless.${POD_NAMESPACE}:9092,EXTERNAL://${HOST_IP}:$((31090 + ${KAFKA_BROKER_ID})) && \\\n          exec /etc/confluent/docker/run\n        volumeMounts:\n          - name: datadir\n            mountPath: /opt/kafka/data-0\n      volumes:\n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  labels:\n    app: kafka\n    chart: kafka-0.1.0\n    release: kafka\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: kafka\n      release: kafka\n  serviceName: kafka-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: kafka\n        release: kafka\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - kafka\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - kafka\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: kafka-broker\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 9092\n          name: kafka\n        resources:\n          {}\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: KAFKA_ZOOKEEPER_CONNECT\n          value: \"usp-zookeeper:2181\"\n        - name: KAFKA_LOG_DIRS\n          value: \"/opt/kafka/data-0/logs\"\n#        - name: KAFKA_METRIC_REPORTERS\n#          value: \"io.confluent.metrics.reporter.ConfluentMetricsReporter\"\n        - name: CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS\n          value: \"PLAINTEXT://kafka-kafka-headless:9092\"\n        - name: \"KAFKA_LISTENER_SECURITY_PROTOCOL_MAP\"\n          value: \"PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT\"\n        - name: \"KAFKA_MESSAGE_MAX_BYTES\"\n          value: \"6291456\"\n        - name: \"KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR\"\n          value: \"3\"\n        - name: \"KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES\"\n          value: \"131072\"\n        - name: KAFKA_JMX_PORT\n          value: \"5555\"\n        # This is required because the Downward API does not yet support identification of\n        # pod numbering in statefulsets. Thus, we are required to specify a command which\n        # allows us to extract the pod ID for usage as the Kafka Broker ID.\n        # See: https://github.com/kubernetes/kubernetes/issues/31218\n        command:\n        - sh\n        - -exc\n        - |\n          export KAFKA_BROKER_ID=${HOSTNAME##*-} && \\\n          export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_NAME}.kafka-headless.${POD_NAMESPACE}:9092,EXTERNAL://${HOST_IP}:$((31090 + ${KAFKA_BROKER_ID})) && \\\n          exec /etc/confluent/docker/run\n        volumeMounts:\n          - name: datadir\n            mountPath: /opt/kafka/data-0\n      volumes:\n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  labels:\n    app: kafka\n    chart: kafka-0.1.0\n    release: kafka\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: kafka\n      release: kafka\n  serviceName: kafka-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: kafka\n        release: kafka\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - kafka\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - kafka\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: kafka-broker\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 9092\n          name: kafka\n        resources:\n          {}\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: KAFKA_ZOOKEEPER_CONNECT\n          value: \"usp-zookeeper:2181\"\n        - name: KAFKA_LOG_DIRS\n          value: \"/opt/kafka/data-0/logs\"\n#        - name: KAFKA_METRIC_REPORTERS\n#          value: \"io.confluent.metrics.reporter.ConfluentMetricsReporter\"\n        - name: CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS\n          value: \"PLAINTEXT://kafka-kafka-headless:9092\"\n        - name: \"KAFKA_LISTENER_SECURITY_PROTOCOL_MAP\"\n          value: \"PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT\"\n        - name: \"KAFKA_MESSAGE_MAX_BYTES\"\n          value: \"6291456\"\n        - name: \"KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR\"\n          value: \"3\"\n        - name: \"KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES\"\n          value: \"131072\"\n        - name: KAFKA_JMX_PORT\n          value: \"5555\"\n        # This is required because the Downward API does not yet support identification of\n        # pod numbering in statefulsets. Thus, we are required to specify a command which\n        # allows us to extract the pod ID for usage as the Kafka Broker ID.\n        # See: https://github.com/kubernetes/kubernetes/issues/31218\n        command:\n        - sh\n        - -exc\n        - |\n          export KAFKA_BROKER_ID=${HOSTNAME##*-} && \\\n          export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_NAME}.kafka-headless.${POD_NAMESPACE}:9092,EXTERNAL://${HOST_IP}:$((31090 + ${KAFKA_BROKER_ID})) && \\\n          exec /etc/confluent/docker/run\n        volumeMounts:\n          - name: datadir\n            mountPath: /opt/kafka/data-0\n      volumes:\n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  labels:\n    app: kafka\n    chart: kafka-0.1.0\n    release: kafka\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: kafka\n      release: kafka\n  serviceName: kafka-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: kafka\n        release: kafka\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - kafka\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - kafka\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: kafka-broker\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 9092\n          name: kafka\n        resources:\n          {}\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: KAFKA_ZOOKEEPER_CONNECT\n          value: \"usp-zookeeper:2181\"\n        - name: KAFKA_LOG_DIRS\n          value: \"/opt/kafka/data-0/logs\"\n#        - name: KAFKA_METRIC_REPORTERS\n#          value: \"io.confluent.metrics.reporter.ConfluentMetricsReporter\"\n        - name: CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS\n          value: \"PLAINTEXT://kafka-kafka-headless:9092\"\n        - name: \"KAFKA_LISTENER_SECURITY_PROTOCOL_MAP\"\n          value: \"PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT\"\n        - name: \"KAFKA_MESSAGE_MAX_BYTES\"\n          value: \"6291456\"\n        - name: \"KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR\"\n          value: \"3\"\n        - name: \"KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES\"\n          value: \"131072\"\n        - name: KAFKA_JMX_PORT\n          value: \"5555\"\n        # This is required because the Downward API does not yet support identification of\n        # pod numbering in statefulsets. Thus, we are required to specify a command which\n        # allows us to extract the pod ID for usage as the Kafka Broker ID.\n        # See: https://github.com/kubernetes/kubernetes/issues/31218\n        command:\n        - sh\n        - -exc\n        - |\n          export KAFKA_BROKER_ID=${HOSTNAME##*-} && \\\n          export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_NAME}.kafka-headless.${POD_NAMESPACE}:9092,EXTERNAL://${HOST_IP}:$((31090 + ${KAFKA_BROKER_ID})) && \\\n          exec /etc/confluent/docker/run\n        volumeMounts:\n          - name: datadir\n            mountPath: /opt/kafka/data-0\n      volumes:\n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  labels:\n    app: kafka\n    chart: kafka-0.1.0\n    release: kafka\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: kafka\n      release: kafka\n  serviceName: kafka-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: kafka\n        release: kafka\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - kafka\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - kafka\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: kafka-broker\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 9092\n          name: kafka\n        resources:\n          {}\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: KAFKA_ZOOKEEPER_CONNECT\n          value: \"usp-zookeeper:2181\"\n        - name: KAFKA_LOG_DIRS\n          value: \"/opt/kafka/data-0/logs\"\n#        - name: KAFKA_METRIC_REPORTERS\n#          value: \"io.confluent.metrics.reporter.ConfluentMetricsReporter\"\n        - name: CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS\n          value: \"PLAINTEXT://kafka-kafka-headless:9092\"\n        - name: \"KAFKA_LISTENER_SECURITY_PROTOCOL_MAP\"\n          value: \"PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT\"\n        - name: \"KAFKA_MESSAGE_MAX_BYTES\"\n          value: \"6291456\"\n        - name: \"KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR\"\n          value: \"3\"\n        - name: \"KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES\"\n          value: \"131072\"\n        - name: KAFKA_JMX_PORT\n          value: \"5555\"\n        # This is required because the Downward API does not yet support identification of\n        # pod numbering in statefulsets. Thus, we are required to specify a command which\n        # allows us to extract the pod ID for usage as the Kafka Broker ID.\n        # See: https://github.com/kubernetes/kubernetes/issues/31218\n        command:\n        - sh\n        - -exc\n        - |\n          export KAFKA_BROKER_ID=${HOSTNAME##*-} && \\\n          export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_NAME}.kafka-headless.${POD_NAMESPACE}:9092,EXTERNAL://${HOST_IP}:$((31090 + ${KAFKA_BROKER_ID})) && \\\n          exec /etc/confluent/docker/run\n        volumeMounts:\n          - name: datadir\n            mountPath: /opt/kafka/data-0\n      volumes:\n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  labels:\n    app: kafka\n    chart: kafka-0.1.0\n    release: kafka\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: kafka\n      release: kafka\n  serviceName: kafka-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: kafka\n        release: kafka\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - kafka\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - kafka\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: kafka-broker\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 9092\n          name: kafka\n        resources:\n          {}\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: KAFKA_ZOOKEEPER_CONNECT\n          value: \"usp-zookeeper:2181\"\n        - name: KAFKA_LOG_DIRS\n          value: \"/opt/kafka/data-0/logs\"\n#        - name: KAFKA_METRIC_REPORTERS\n#          value: \"io.confluent.metrics.reporter.ConfluentMetricsReporter\"\n        - name: CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS\n          value: \"PLAINTEXT://kafka-kafka-headless:9092\"\n        - name: \"KAFKA_LISTENER_SECURITY_PROTOCOL_MAP\"\n          value: \"PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT\"\n        - name: \"KAFKA_MESSAGE_MAX_BYTES\"\n          value: \"6291456\"\n        - name: \"KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR\"\n          value: \"3\"\n        - name: \"KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES\"\n          value: \"131072\"\n        - name: KAFKA_JMX_PORT\n          value: \"5555\"\n        # This is required because the Downward API does not yet support identification of\n        # pod numbering in statefulsets. Thus, we are required to specify a command which\n        # allows us to extract the pod ID for usage as the Kafka Broker ID.\n        # See: https://github.com/kubernetes/kubernetes/issues/31218\n        command:\n        - sh\n        - -exc\n        - |\n          export KAFKA_BROKER_ID=${HOSTNAME##*-} && \\\n          export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_NAME}.kafka-headless.${POD_NAMESPACE}:9092,EXTERNAL://${HOST_IP}:$((31090 + ${KAFKA_BROKER_ID})) && \\\n          exec /etc/confluent/docker/run\n        volumeMounts:\n          - name: datadir\n            mountPath: /opt/kafka/data-0\n      volumes:\n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  labels:\n    app: kafka\n    chart: kafka-0.1.0\n    release: kafka\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: kafka\n      release: kafka\n  serviceName: kafka-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: kafka\n        release: kafka\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - kafka\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - kafka\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: kafka-broker\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 9092\n          name: kafka\n        resources:\n          {}\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: KAFKA_ZOOKEEPER_CONNECT\n          value: \"usp-zookeeper:2181\"\n        - name: KAFKA_LOG_DIRS\n          value: \"/opt/kafka/data-0/logs\"\n#        - name: KAFKA_METRIC_REPORTERS\n#          value: \"io.confluent.metrics.reporter.ConfluentMetricsReporter\"\n        - name: CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS\n          value: \"PLAINTEXT://kafka-kafka-headless:9092\"\n        - name: \"KAFKA_LISTENER_SECURITY_PROTOCOL_MAP\"\n          value: \"PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT\"\n        - name: \"KAFKA_MESSAGE_MAX_BYTES\"\n          value: \"6291456\"\n        - name: \"KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR\"\n          value: \"3\"\n        - name: \"KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES\"\n          value: \"131072\"\n        - name: KAFKA_JMX_PORT\n          value: \"5555\"\n        # This is required because the Downward API does not yet support identification of\n        # pod numbering in statefulsets. Thus, we are required to specify a command which\n        # allows us to extract the pod ID for usage as the Kafka Broker ID.\n        # See: https://github.com/kubernetes/kubernetes/issues/31218\n        command:\n        - sh\n        - -exc\n        - |\n          export KAFKA_BROKER_ID=${HOSTNAME##*-} && \\\n          export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_NAME}.kafka-headless.${POD_NAMESPACE}:9092,EXTERNAL://${HOST_IP}:$((31090 + ${KAFKA_BROKER_ID})) && \\\n          exec /etc/confluent/docker/run\n        volumeMounts:\n          - name: datadir\n            mountPath: /opt/kafka/data-0\n      volumes:\n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/statefulset.yaml"}, "region": {"startLine": 3, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  labels:\n    app: kafka\n    chart: kafka-0.1.0\n    release: kafka\n    heritage: Helm\nspec:\n  selector:\n    matchLabels:\n      app: kafka\n      release: kafka\n  serviceName: kafka-headless\n  podManagementPolicy: OrderedReady\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: kafka\n        release: kafka\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - kafka\n                  - key: \"release\"\n                    operator: In\n                    values:\n                    - kafka\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: kafka-broker\n        image: \":\"\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext:\n          runAsUser: 0\n        ports:\n        - containerPort: 9092\n          name: kafka\n        resources:\n          {}\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KAFKA_HEAP_OPTS\n          value: -Xms512M -Xmx512M\n        - name: KAFKA_ZOOKEEPER_CONNECT\n          value: \"usp-zookeeper:2181\"\n        - name: KAFKA_LOG_DIRS\n          value: \"/opt/kafka/data-0/logs\"\n#        - name: KAFKA_METRIC_REPORTERS\n#          value: \"io.confluent.metrics.reporter.ConfluentMetricsReporter\"\n        - name: CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS\n          value: \"PLAINTEXT://kafka-kafka-headless:9092\"\n        - name: \"KAFKA_LISTENER_SECURITY_PROTOCOL_MAP\"\n          value: \"PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT\"\n        - name: \"KAFKA_MESSAGE_MAX_BYTES\"\n          value: \"6291456\"\n        - name: \"KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR\"\n          value: \"3\"\n        - name: \"KAFKA_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES\"\n          value: \"131072\"\n        - name: KAFKA_JMX_PORT\n          value: \"5555\"\n        # This is required because the Downward API does not yet support identification of\n        # pod numbering in statefulsets. Thus, we are required to specify a command which\n        # allows us to extract the pod ID for usage as the Kafka Broker ID.\n        # See: https://github.com/kubernetes/kubernetes/issues/31218\n        command:\n        - sh\n        - -exc\n        - |\n          export KAFKA_BROKER_ID=${HOSTNAME##*-} && \\\n          export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_NAME}.kafka-headless.${POD_NAMESPACE}:9092,EXTERNAL://${HOST_IP}:$((31090 + ${KAFKA_BROKER_ID})) && \\\n          exec /etc/confluent/docker/run\n        volumeMounts:\n          - name: datadir\n            mountPath: /opt/kafka/data-0\n      volumes:\n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: \"10Gi\"\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/tests/canary-pod.yaml"}, "region": {"startLine": 3, "endLine": 29, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"kafka-canary\"\n  annotations:\n    \"helm.sh/hook\": test-success\n    \"helm.sh/hook-delete-policy\": \"before-hook-creation,hook-succeeded\"\nspec:\n  containers:\n  - name: kafka-canary\n    image: \":\"\n    imagePullPolicy: \"IfNotPresent\"\n    command:\n    - sh\n    - -c\n    - |\n      # Delete the topic if it exists\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --delete --if-exists\n      # Create the topic\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --create --partitions 1 --replication-factor 1 --if-not-exists && \\\n      # Create a message\n      MESSAGE=\"`date -u`\" && \\\n      # Produce a test message to the topic\n      echo \"$MESSAGE\" | kafka-console-producer --broker-list kafka:9092 --topic kafka-canary-topic && \\\n      # Consume a test message from the topic\n      kafka-console-consumer --bootstrap-server kafka-headless:9092 --topic kafka-canary-topic --from-beginning --max-messages 1 --timeout-ms 400000 | grep \"$MESSAGE\"\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/tests/canary-pod.yaml"}, "region": {"startLine": 3, "endLine": 29, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"kafka-canary\"\n  annotations:\n    \"helm.sh/hook\": test-success\n    \"helm.sh/hook-delete-policy\": \"before-hook-creation,hook-succeeded\"\nspec:\n  containers:\n  - name: kafka-canary\n    image: \":\"\n    imagePullPolicy: \"IfNotPresent\"\n    command:\n    - sh\n    - -c\n    - |\n      # Delete the topic if it exists\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --delete --if-exists\n      # Create the topic\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --create --partitions 1 --replication-factor 1 --if-not-exists && \\\n      # Create a message\n      MESSAGE=\"`date -u`\" && \\\n      # Produce a test message to the topic\n      echo \"$MESSAGE\" | kafka-console-producer --broker-list kafka:9092 --topic kafka-canary-topic && \\\n      # Consume a test message from the topic\n      kafka-console-consumer --bootstrap-server kafka-headless:9092 --topic kafka-canary-topic --from-beginning --max-messages 1 --timeout-ms 400000 | grep \"$MESSAGE\"\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/tests/canary-pod.yaml"}, "region": {"startLine": 3, "endLine": 29, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"kafka-canary\"\n  annotations:\n    \"helm.sh/hook\": test-success\n    \"helm.sh/hook-delete-policy\": \"before-hook-creation,hook-succeeded\"\nspec:\n  containers:\n  - name: kafka-canary\n    image: \":\"\n    imagePullPolicy: \"IfNotPresent\"\n    command:\n    - sh\n    - -c\n    - |\n      # Delete the topic if it exists\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --delete --if-exists\n      # Create the topic\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --create --partitions 1 --replication-factor 1 --if-not-exists && \\\n      # Create a message\n      MESSAGE=\"`date -u`\" && \\\n      # Produce a test message to the topic\n      echo \"$MESSAGE\" | kafka-console-producer --broker-list kafka:9092 --topic kafka-canary-topic && \\\n      # Consume a test message from the topic\n      kafka-console-consumer --bootstrap-server kafka-headless:9092 --topic kafka-canary-topic --from-beginning --max-messages 1 --timeout-ms 400000 | grep \"$MESSAGE\"\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/tests/canary-pod.yaml"}, "region": {"startLine": 3, "endLine": 29, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"kafka-canary\"\n  annotations:\n    \"helm.sh/hook\": test-success\n    \"helm.sh/hook-delete-policy\": \"before-hook-creation,hook-succeeded\"\nspec:\n  containers:\n  - name: kafka-canary\n    image: \":\"\n    imagePullPolicy: \"IfNotPresent\"\n    command:\n    - sh\n    - -c\n    - |\n      # Delete the topic if it exists\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --delete --if-exists\n      # Create the topic\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --create --partitions 1 --replication-factor 1 --if-not-exists && \\\n      # Create a message\n      MESSAGE=\"`date -u`\" && \\\n      # Produce a test message to the topic\n      echo \"$MESSAGE\" | kafka-console-producer --broker-list kafka:9092 --topic kafka-canary-topic && \\\n      # Consume a test message from the topic\n      kafka-console-consumer --bootstrap-server kafka-headless:9092 --topic kafka-canary-topic --from-beginning --max-messages 1 --timeout-ms 400000 | grep \"$MESSAGE\"\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/tests/canary-pod.yaml"}, "region": {"startLine": 3, "endLine": 29, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"kafka-canary\"\n  annotations:\n    \"helm.sh/hook\": test-success\n    \"helm.sh/hook-delete-policy\": \"before-hook-creation,hook-succeeded\"\nspec:\n  containers:\n  - name: kafka-canary\n    image: \":\"\n    imagePullPolicy: \"IfNotPresent\"\n    command:\n    - sh\n    - -c\n    - |\n      # Delete the topic if it exists\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --delete --if-exists\n      # Create the topic\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --create --partitions 1 --replication-factor 1 --if-not-exists && \\\n      # Create a message\n      MESSAGE=\"`date -u`\" && \\\n      # Produce a test message to the topic\n      echo \"$MESSAGE\" | kafka-console-producer --broker-list kafka:9092 --topic kafka-canary-topic && \\\n      # Consume a test message from the topic\n      kafka-console-consumer --bootstrap-server kafka-headless:9092 --topic kafka-canary-topic --from-beginning --max-messages 1 --timeout-ms 400000 | grep \"$MESSAGE\"\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/tests/canary-pod.yaml"}, "region": {"startLine": 3, "endLine": 29, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"kafka-canary\"\n  annotations:\n    \"helm.sh/hook\": test-success\n    \"helm.sh/hook-delete-policy\": \"before-hook-creation,hook-succeeded\"\nspec:\n  containers:\n  - name: kafka-canary\n    image: \":\"\n    imagePullPolicy: \"IfNotPresent\"\n    command:\n    - sh\n    - -c\n    - |\n      # Delete the topic if it exists\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --delete --if-exists\n      # Create the topic\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --create --partitions 1 --replication-factor 1 --if-not-exists && \\\n      # Create a message\n      MESSAGE=\"`date -u`\" && \\\n      # Produce a test message to the topic\n      echo \"$MESSAGE\" | kafka-console-producer --broker-list kafka:9092 --topic kafka-canary-topic && \\\n      # Consume a test message from the topic\n      kafka-console-consumer --bootstrap-server kafka-headless:9092 --topic kafka-canary-topic --from-beginning --max-messages 1 --timeout-ms 400000 | grep \"$MESSAGE\"\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/tests/canary-pod.yaml"}, "region": {"startLine": 3, "endLine": 29, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"kafka-canary\"\n  annotations:\n    \"helm.sh/hook\": test-success\n    \"helm.sh/hook-delete-policy\": \"before-hook-creation,hook-succeeded\"\nspec:\n  containers:\n  - name: kafka-canary\n    image: \":\"\n    imagePullPolicy: \"IfNotPresent\"\n    command:\n    - sh\n    - -c\n    - |\n      # Delete the topic if it exists\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --delete --if-exists\n      # Create the topic\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --create --partitions 1 --replication-factor 1 --if-not-exists && \\\n      # Create a message\n      MESSAGE=\"`date -u`\" && \\\n      # Produce a test message to the topic\n      echo \"$MESSAGE\" | kafka-console-producer --broker-list kafka:9092 --topic kafka-canary-topic && \\\n      # Consume a test message from the topic\n      kafka-console-consumer --bootstrap-server kafka-headless:9092 --topic kafka-canary-topic --from-beginning --max-messages 1 --timeout-ms 400000 | grep \"$MESSAGE\"\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/tests/canary-pod.yaml"}, "region": {"startLine": 3, "endLine": 29, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"kafka-canary\"\n  annotations:\n    \"helm.sh/hook\": test-success\n    \"helm.sh/hook-delete-policy\": \"before-hook-creation,hook-succeeded\"\nspec:\n  containers:\n  - name: kafka-canary\n    image: \":\"\n    imagePullPolicy: \"IfNotPresent\"\n    command:\n    - sh\n    - -c\n    - |\n      # Delete the topic if it exists\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --delete --if-exists\n      # Create the topic\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --create --partitions 1 --replication-factor 1 --if-not-exists && \\\n      # Create a message\n      MESSAGE=\"`date -u`\" && \\\n      # Produce a test message to the topic\n      echo \"$MESSAGE\" | kafka-console-producer --broker-list kafka:9092 --topic kafka-canary-topic && \\\n      # Consume a test message from the topic\n      kafka-console-consumer --bootstrap-server kafka-headless:9092 --topic kafka-canary-topic --from-beginning --max-messages 1 --timeout-ms 400000 | grep \"$MESSAGE\"\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/tests/canary-pod.yaml"}, "region": {"startLine": 3, "endLine": 29, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"kafka-canary\"\n  annotations:\n    \"helm.sh/hook\": test-success\n    \"helm.sh/hook-delete-policy\": \"before-hook-creation,hook-succeeded\"\nspec:\n  containers:\n  - name: kafka-canary\n    image: \":\"\n    imagePullPolicy: \"IfNotPresent\"\n    command:\n    - sh\n    - -c\n    - |\n      # Delete the topic if it exists\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --delete --if-exists\n      # Create the topic\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --create --partitions 1 --replication-factor 1 --if-not-exists && \\\n      # Create a message\n      MESSAGE=\"`date -u`\" && \\\n      # Produce a test message to the topic\n      echo \"$MESSAGE\" | kafka-console-producer --broker-list kafka:9092 --topic kafka-canary-topic && \\\n      # Consume a test message from the topic\n      kafka-console-consumer --bootstrap-server kafka-headless:9092 --topic kafka-canary-topic --from-beginning --max-messages 1 --timeout-ms 400000 | grep \"$MESSAGE\"\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/tests/canary-pod.yaml"}, "region": {"startLine": 3, "endLine": 29, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"kafka-canary\"\n  annotations:\n    \"helm.sh/hook\": test-success\n    \"helm.sh/hook-delete-policy\": \"before-hook-creation,hook-succeeded\"\nspec:\n  containers:\n  - name: kafka-canary\n    image: \":\"\n    imagePullPolicy: \"IfNotPresent\"\n    command:\n    - sh\n    - -c\n    - |\n      # Delete the topic if it exists\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --delete --if-exists\n      # Create the topic\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --create --partitions 1 --replication-factor 1 --if-not-exists && \\\n      # Create a message\n      MESSAGE=\"`date -u`\" && \\\n      # Produce a test message to the topic\n      echo \"$MESSAGE\" | kafka-console-producer --broker-list kafka:9092 --topic kafka-canary-topic && \\\n      # Consume a test message from the topic\n      kafka-console-consumer --bootstrap-server kafka-headless:9092 --topic kafka-canary-topic --from-beginning --max-messages 1 --timeout-ms 400000 | grep \"$MESSAGE\"\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/tests/canary-pod.yaml"}, "region": {"startLine": 3, "endLine": 29, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"kafka-canary\"\n  annotations:\n    \"helm.sh/hook\": test-success\n    \"helm.sh/hook-delete-policy\": \"before-hook-creation,hook-succeeded\"\nspec:\n  containers:\n  - name: kafka-canary\n    image: \":\"\n    imagePullPolicy: \"IfNotPresent\"\n    command:\n    - sh\n    - -c\n    - |\n      # Delete the topic if it exists\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --delete --if-exists\n      # Create the topic\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --create --partitions 1 --replication-factor 1 --if-not-exists && \\\n      # Create a message\n      MESSAGE=\"`date -u`\" && \\\n      # Produce a test message to the topic\n      echo \"$MESSAGE\" | kafka-console-producer --broker-list kafka:9092 --topic kafka-canary-topic && \\\n      # Consume a test message from the topic\n      kafka-console-consumer --bootstrap-server kafka-headless:9092 --topic kafka-canary-topic --from-beginning --max-messages 1 --timeout-ms 400000 | grep \"$MESSAGE\"\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/tests/canary-pod.yaml"}, "region": {"startLine": 3, "endLine": 29, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"kafka-canary\"\n  annotations:\n    \"helm.sh/hook\": test-success\n    \"helm.sh/hook-delete-policy\": \"before-hook-creation,hook-succeeded\"\nspec:\n  containers:\n  - name: kafka-canary\n    image: \":\"\n    imagePullPolicy: \"IfNotPresent\"\n    command:\n    - sh\n    - -c\n    - |\n      # Delete the topic if it exists\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --delete --if-exists\n      # Create the topic\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --create --partitions 1 --replication-factor 1 --if-not-exists && \\\n      # Create a message\n      MESSAGE=\"`date -u`\" && \\\n      # Produce a test message to the topic\n      echo \"$MESSAGE\" | kafka-console-producer --broker-list kafka:9092 --topic kafka-canary-topic && \\\n      # Consume a test message from the topic\n      kafka-console-consumer --bootstrap-server kafka-headless:9092 --topic kafka-canary-topic --from-beginning --max-messages 1 --timeout-ms 400000 | grep \"$MESSAGE\"\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/tests/canary-pod.yaml"}, "region": {"startLine": 3, "endLine": 29, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"kafka-canary\"\n  annotations:\n    \"helm.sh/hook\": test-success\n    \"helm.sh/hook-delete-policy\": \"before-hook-creation,hook-succeeded\"\nspec:\n  containers:\n  - name: kafka-canary\n    image: \":\"\n    imagePullPolicy: \"IfNotPresent\"\n    command:\n    - sh\n    - -c\n    - |\n      # Delete the topic if it exists\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --delete --if-exists\n      # Create the topic\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --create --partitions 1 --replication-factor 1 --if-not-exists && \\\n      # Create a message\n      MESSAGE=\"`date -u`\" && \\\n      # Produce a test message to the topic\n      echo \"$MESSAGE\" | kafka-console-producer --broker-list kafka:9092 --topic kafka-canary-topic && \\\n      # Consume a test message from the topic\n      kafka-console-consumer --bootstrap-server kafka-headless:9092 --topic kafka-canary-topic --from-beginning --max-messages 1 --timeout-ms 400000 | grep \"$MESSAGE\"\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/tests/canary-pod.yaml"}, "region": {"startLine": 3, "endLine": 29, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"kafka-canary\"\n  annotations:\n    \"helm.sh/hook\": test-success\n    \"helm.sh/hook-delete-policy\": \"before-hook-creation,hook-succeeded\"\nspec:\n  containers:\n  - name: kafka-canary\n    image: \":\"\n    imagePullPolicy: \"IfNotPresent\"\n    command:\n    - sh\n    - -c\n    - |\n      # Delete the topic if it exists\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --delete --if-exists\n      # Create the topic\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --create --partitions 1 --replication-factor 1 --if-not-exists && \\\n      # Create a message\n      MESSAGE=\"`date -u`\" && \\\n      # Produce a test message to the topic\n      echo \"$MESSAGE\" | kafka-console-producer --broker-list kafka:9092 --topic kafka-canary-topic && \\\n      # Consume a test message from the topic\n      kafka-console-consumer --bootstrap-server kafka-headless:9092 --topic kafka-canary-topic --from-beginning --max-messages 1 --timeout-ms 400000 | grep \"$MESSAGE\"\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/tests/canary-pod.yaml"}, "region": {"startLine": 3, "endLine": 29, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"kafka-canary\"\n  annotations:\n    \"helm.sh/hook\": test-success\n    \"helm.sh/hook-delete-policy\": \"before-hook-creation,hook-succeeded\"\nspec:\n  containers:\n  - name: kafka-canary\n    image: \":\"\n    imagePullPolicy: \"IfNotPresent\"\n    command:\n    - sh\n    - -c\n    - |\n      # Delete the topic if it exists\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --delete --if-exists\n      # Create the topic\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --create --partitions 1 --replication-factor 1 --if-not-exists && \\\n      # Create a message\n      MESSAGE=\"`date -u`\" && \\\n      # Produce a test message to the topic\n      echo \"$MESSAGE\" | kafka-console-producer --broker-list kafka:9092 --topic kafka-canary-topic && \\\n      # Consume a test message from the topic\n      kafka-console-consumer --bootstrap-server kafka-headless:9092 --topic kafka-canary-topic --from-beginning --max-messages 1 --timeout-ms 400000 | grep \"$MESSAGE\"\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/tests/canary-pod.yaml"}, "region": {"startLine": 3, "endLine": 29, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"kafka-canary\"\n  annotations:\n    \"helm.sh/hook\": test-success\n    \"helm.sh/hook-delete-policy\": \"before-hook-creation,hook-succeeded\"\nspec:\n  containers:\n  - name: kafka-canary\n    image: \":\"\n    imagePullPolicy: \"IfNotPresent\"\n    command:\n    - sh\n    - -c\n    - |\n      # Delete the topic if it exists\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --delete --if-exists\n      # Create the topic\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --create --partitions 1 --replication-factor 1 --if-not-exists && \\\n      # Create a message\n      MESSAGE=\"`date -u`\" && \\\n      # Produce a test message to the topic\n      echo \"$MESSAGE\" | kafka-console-producer --broker-list kafka:9092 --topic kafka-canary-topic && \\\n      # Consume a test message from the topic\n      kafka-console-consumer --bootstrap-server kafka-headless:9092 --topic kafka-canary-topic --from-beginning --max-messages 1 --timeout-ms 400000 | grep \"$MESSAGE\"\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/tests/canary-pod.yaml"}, "region": {"startLine": 3, "endLine": 29, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"kafka-canary\"\n  annotations:\n    \"helm.sh/hook\": test-success\n    \"helm.sh/hook-delete-policy\": \"before-hook-creation,hook-succeeded\"\nspec:\n  containers:\n  - name: kafka-canary\n    image: \":\"\n    imagePullPolicy: \"IfNotPresent\"\n    command:\n    - sh\n    - -c\n    - |\n      # Delete the topic if it exists\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --delete --if-exists\n      # Create the topic\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --create --partitions 1 --replication-factor 1 --if-not-exists && \\\n      # Create a message\n      MESSAGE=\"`date -u`\" && \\\n      # Produce a test message to the topic\n      echo \"$MESSAGE\" | kafka-console-producer --broker-list kafka:9092 --topic kafka-canary-topic && \\\n      # Consume a test message from the topic\n      kafka-console-consumer --bootstrap-server kafka-headless:9092 --topic kafka-canary-topic --from-beginning --max-messages 1 --timeout-ms 400000 | grep \"$MESSAGE\"\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/tests/canary-pod.yaml"}, "region": {"startLine": 3, "endLine": 29, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"kafka-canary\"\n  annotations:\n    \"helm.sh/hook\": test-success\n    \"helm.sh/hook-delete-policy\": \"before-hook-creation,hook-succeeded\"\nspec:\n  containers:\n  - name: kafka-canary\n    image: \":\"\n    imagePullPolicy: \"IfNotPresent\"\n    command:\n    - sh\n    - -c\n    - |\n      # Delete the topic if it exists\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --delete --if-exists\n      # Create the topic\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --create --partitions 1 --replication-factor 1 --if-not-exists && \\\n      # Create a message\n      MESSAGE=\"`date -u`\" && \\\n      # Produce a test message to the topic\n      echo \"$MESSAGE\" | kafka-console-producer --broker-list kafka:9092 --topic kafka-canary-topic && \\\n      # Consume a test message from the topic\n      kafka-console-consumer --bootstrap-server kafka-headless:9092 --topic kafka-canary-topic --from-beginning --max-messages 1 --timeout-ms 400000 | grep \"$MESSAGE\"\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/kafka/templates/tests/canary-pod.yaml"}, "region": {"startLine": 3, "endLine": 29, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"kafka-canary\"\n  annotations:\n    \"helm.sh/hook\": test-success\n    \"helm.sh/hook-delete-policy\": \"before-hook-creation,hook-succeeded\"\nspec:\n  containers:\n  - name: kafka-canary\n    image: \":\"\n    imagePullPolicy: \"IfNotPresent\"\n    command:\n    - sh\n    - -c\n    - |\n      # Delete the topic if it exists\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --delete --if-exists\n      # Create the topic\n      kafka-topics --zookeeper usp-zookeeper:2181 --topic kafka-canary-topic --create --partitions 1 --replication-factor 1 --if-not-exists && \\\n      # Create a message\n      MESSAGE=\"`date -u`\" && \\\n      # Produce a test message to the topic\n      echo \"$MESSAGE\" | kafka-console-producer --broker-list kafka:9092 --topic kafka-canary-topic && \\\n      # Consume a test message from the topic\n      kafka-console-consumer --bootstrap-server kafka-headless:9092 --topic kafka-canary-topic --from-beginning --max-messages 1 --timeout-ms 400000 | grep \"$MESSAGE\"\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/controller-serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 9, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: efs-csi-controller-sa\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\n---\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/controller-deployment.yaml"}, "region": {"startLine": 4, "endLine": 88, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: efs-csi-controller-sa\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n            - --delete-access-point-root-dir=false\n            - --vol-metrics-opt-in=false\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n            - name: CSI_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n          ports:\n            - name: healthz\n              containerPort: 9909\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 10\n            failureThreshold: 5\n        - name: csi-provisioner\n          image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v3.3.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --v=2\n            - --feature-gates=Topology=true\n            - --extra-create-metadata\n            - --leader-election\n          env:\n            - name: ADDRESS\n              value: /var/lib/csi/sockets/pluginproxy/csi.sock\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9909\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /csi\n      volumes:\n        - name: socket-dir\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/controller-deployment.yaml"}, "region": {"startLine": 4, "endLine": 88, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: efs-csi-controller-sa\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n            - --delete-access-point-root-dir=false\n            - --vol-metrics-opt-in=false\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n            - name: CSI_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n          ports:\n            - name: healthz\n              containerPort: 9909\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 10\n            failureThreshold: 5\n        - name: csi-provisioner\n          image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v3.3.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --v=2\n            - --feature-gates=Topology=true\n            - --extra-create-metadata\n            - --leader-election\n          env:\n            - name: ADDRESS\n              value: /var/lib/csi/sockets/pluginproxy/csi.sock\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9909\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /csi\n      volumes:\n        - name: socket-dir\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/controller-deployment.yaml"}, "region": {"startLine": 4, "endLine": 88, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: efs-csi-controller-sa\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n            - --delete-access-point-root-dir=false\n            - --vol-metrics-opt-in=false\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n            - name: CSI_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n          ports:\n            - name: healthz\n              containerPort: 9909\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 10\n            failureThreshold: 5\n        - name: csi-provisioner\n          image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v3.3.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --v=2\n            - --feature-gates=Topology=true\n            - --extra-create-metadata\n            - --leader-election\n          env:\n            - name: ADDRESS\n              value: /var/lib/csi/sockets/pluginproxy/csi.sock\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9909\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /csi\n      volumes:\n        - name: socket-dir\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/controller-deployment.yaml"}, "region": {"startLine": 4, "endLine": 88, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: efs-csi-controller-sa\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n            - --delete-access-point-root-dir=false\n            - --vol-metrics-opt-in=false\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n            - name: CSI_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n          ports:\n            - name: healthz\n              containerPort: 9909\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 10\n            failureThreshold: 5\n        - name: csi-provisioner\n          image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v3.3.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --v=2\n            - --feature-gates=Topology=true\n            - --extra-create-metadata\n            - --leader-election\n          env:\n            - name: ADDRESS\n              value: /var/lib/csi/sockets/pluginproxy/csi.sock\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9909\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /csi\n      volumes:\n        - name: socket-dir\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/controller-deployment.yaml"}, "region": {"startLine": 4, "endLine": 88, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: efs-csi-controller-sa\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n            - --delete-access-point-root-dir=false\n            - --vol-metrics-opt-in=false\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n            - name: CSI_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n          ports:\n            - name: healthz\n              containerPort: 9909\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 10\n            failureThreshold: 5\n        - name: csi-provisioner\n          image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v3.3.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --v=2\n            - --feature-gates=Topology=true\n            - --extra-create-metadata\n            - --leader-election\n          env:\n            - name: ADDRESS\n              value: /var/lib/csi/sockets/pluginproxy/csi.sock\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9909\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /csi\n      volumes:\n        - name: socket-dir\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/controller-deployment.yaml"}, "region": {"startLine": 4, "endLine": 88, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: efs-csi-controller-sa\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n            - --delete-access-point-root-dir=false\n            - --vol-metrics-opt-in=false\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n            - name: CSI_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n          ports:\n            - name: healthz\n              containerPort: 9909\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 10\n            failureThreshold: 5\n        - name: csi-provisioner\n          image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v3.3.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --v=2\n            - --feature-gates=Topology=true\n            - --extra-create-metadata\n            - --leader-election\n          env:\n            - name: ADDRESS\n              value: /var/lib/csi/sockets/pluginproxy/csi.sock\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9909\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /csi\n      volumes:\n        - name: socket-dir\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/controller-deployment.yaml"}, "region": {"startLine": 4, "endLine": 88, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: efs-csi-controller-sa\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n            - --delete-access-point-root-dir=false\n            - --vol-metrics-opt-in=false\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n            - name: CSI_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n          ports:\n            - name: healthz\n              containerPort: 9909\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 10\n            failureThreshold: 5\n        - name: csi-provisioner\n          image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v3.3.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --v=2\n            - --feature-gates=Topology=true\n            - --extra-create-metadata\n            - --leader-election\n          env:\n            - name: ADDRESS\n              value: /var/lib/csi/sockets/pluginproxy/csi.sock\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9909\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /csi\n      volumes:\n        - name: socket-dir\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/controller-deployment.yaml"}, "region": {"startLine": 4, "endLine": 88, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: efs-csi-controller-sa\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n            - --delete-access-point-root-dir=false\n            - --vol-metrics-opt-in=false\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n            - name: CSI_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n          ports:\n            - name: healthz\n              containerPort: 9909\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 10\n            failureThreshold: 5\n        - name: csi-provisioner\n          image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v3.3.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --v=2\n            - --feature-gates=Topology=true\n            - --extra-create-metadata\n            - --leader-election\n          env:\n            - name: ADDRESS\n              value: /var/lib/csi/sockets/pluginproxy/csi.sock\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9909\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /csi\n      volumes:\n        - name: socket-dir\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/controller-deployment.yaml"}, "region": {"startLine": 4, "endLine": 88, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: efs-csi-controller-sa\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n            - --delete-access-point-root-dir=false\n            - --vol-metrics-opt-in=false\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n            - name: CSI_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n          ports:\n            - name: healthz\n              containerPort: 9909\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 10\n            failureThreshold: 5\n        - name: csi-provisioner\n          image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v3.3.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --v=2\n            - --feature-gates=Topology=true\n            - --extra-create-metadata\n            - --leader-election\n          env:\n            - name: ADDRESS\n              value: /var/lib/csi/sockets/pluginproxy/csi.sock\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9909\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /csi\n      volumes:\n        - name: socket-dir\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_16", "ruleIndex": 32, "level": "error", "attachments": [], "message": {"text": "Container should not be privileged"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/controller-deployment.yaml"}, "region": {"startLine": 4, "endLine": 88, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: efs-csi-controller-sa\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n            - --delete-access-point-root-dir=false\n            - --vol-metrics-opt-in=false\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n            - name: CSI_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n          ports:\n            - name: healthz\n              containerPort: 9909\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 10\n            failureThreshold: 5\n        - name: csi-provisioner\n          image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v3.3.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --v=2\n            - --feature-gates=Topology=true\n            - --extra-create-metadata\n            - --leader-election\n          env:\n            - name: ADDRESS\n              value: /var/lib/csi/sockets/pluginproxy/csi.sock\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9909\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /csi\n      volumes:\n        - name: socket-dir\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/controller-deployment.yaml"}, "region": {"startLine": 4, "endLine": 88, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: efs-csi-controller-sa\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n            - --delete-access-point-root-dir=false\n            - --vol-metrics-opt-in=false\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n            - name: CSI_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n          ports:\n            - name: healthz\n              containerPort: 9909\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 10\n            failureThreshold: 5\n        - name: csi-provisioner\n          image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v3.3.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --v=2\n            - --feature-gates=Topology=true\n            - --extra-create-metadata\n            - --leader-election\n          env:\n            - name: ADDRESS\n              value: /var/lib/csi/sockets/pluginproxy/csi.sock\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9909\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /csi\n      volumes:\n        - name: socket-dir\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/controller-deployment.yaml"}, "region": {"startLine": 4, "endLine": 88, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: efs-csi-controller-sa\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n            - --delete-access-point-root-dir=false\n            - --vol-metrics-opt-in=false\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n            - name: CSI_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n          ports:\n            - name: healthz\n              containerPort: 9909\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 10\n            failureThreshold: 5\n        - name: csi-provisioner\n          image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v3.3.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --v=2\n            - --feature-gates=Topology=true\n            - --extra-create-metadata\n            - --leader-election\n          env:\n            - name: ADDRESS\n              value: /var/lib/csi/sockets/pluginproxy/csi.sock\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9909\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /csi\n      volumes:\n        - name: socket-dir\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/controller-deployment.yaml"}, "region": {"startLine": 4, "endLine": 88, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: efs-csi-controller-sa\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n            - --delete-access-point-root-dir=false\n            - --vol-metrics-opt-in=false\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n            - name: CSI_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n          ports:\n            - name: healthz\n              containerPort: 9909\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 10\n            failureThreshold: 5\n        - name: csi-provisioner\n          image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v3.3.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --v=2\n            - --feature-gates=Topology=true\n            - --extra-create-metadata\n            - --leader-election\n          env:\n            - name: ADDRESS\n              value: /var/lib/csi/sockets/pluginproxy/csi.sock\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9909\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /csi\n      volumes:\n        - name: socket-dir\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/controller-deployment.yaml"}, "region": {"startLine": 4, "endLine": 88, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: efs-csi-controller-sa\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n            - --delete-access-point-root-dir=false\n            - --vol-metrics-opt-in=false\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n            - name: CSI_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n          ports:\n            - name: healthz\n              containerPort: 9909\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 10\n            failureThreshold: 5\n        - name: csi-provisioner\n          image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v3.3.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --v=2\n            - --feature-gates=Topology=true\n            - --extra-create-metadata\n            - --leader-election\n          env:\n            - name: ADDRESS\n              value: /var/lib/csi/sockets/pluginproxy/csi.sock\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9909\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /csi\n      volumes:\n        - name: socket-dir\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/controller-deployment.yaml"}, "region": {"startLine": 4, "endLine": 88, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: efs-csi-controller-sa\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n            - --delete-access-point-root-dir=false\n            - --vol-metrics-opt-in=false\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n            - name: CSI_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n          ports:\n            - name: healthz\n              containerPort: 9909\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 10\n            failureThreshold: 5\n        - name: csi-provisioner\n          image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v3.3.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --v=2\n            - --feature-gates=Topology=true\n            - --extra-create-metadata\n            - --leader-election\n          env:\n            - name: ADDRESS\n              value: /var/lib/csi/sockets/pluginproxy/csi.sock\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9909\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /csi\n      volumes:\n        - name: socket-dir\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/controller-deployment.yaml"}, "region": {"startLine": 4, "endLine": 88, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: efs-csi-controller-sa\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n            - --delete-access-point-root-dir=false\n            - --vol-metrics-opt-in=false\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n            - name: CSI_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n          ports:\n            - name: healthz\n              containerPort: 9909\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 10\n            failureThreshold: 5\n        - name: csi-provisioner\n          image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v3.3.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --v=2\n            - --feature-gates=Topology=true\n            - --extra-create-metadata\n            - --leader-election\n          env:\n            - name: ADDRESS\n              value: /var/lib/csi/sockets/pluginproxy/csi.sock\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9909\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /csi\n      volumes:\n        - name: socket-dir\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/controller-deployment.yaml"}, "region": {"startLine": 4, "endLine": 88, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: efs-csi-controller-sa\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n            - --delete-access-point-root-dir=false\n            - --vol-metrics-opt-in=false\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n            - name: CSI_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n          ports:\n            - name: healthz\n              containerPort: 9909\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 10\n            failureThreshold: 5\n        - name: csi-provisioner\n          image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v3.3.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --v=2\n            - --feature-gates=Topology=true\n            - --extra-create-metadata\n            - --leader-election\n          env:\n            - name: ADDRESS\n              value: /var/lib/csi/sockets/pluginproxy/csi.sock\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9909\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /csi\n      volumes:\n        - name: socket-dir\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/controller-deployment.yaml"}, "region": {"startLine": 4, "endLine": 88, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: efs-csi-controller-sa\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n            - --delete-access-point-root-dir=false\n            - --vol-metrics-opt-in=false\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n            - name: CSI_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n          ports:\n            - name: healthz\n              containerPort: 9909\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 10\n            failureThreshold: 5\n        - name: csi-provisioner\n          image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v3.3.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --v=2\n            - --feature-gates=Topology=true\n            - --extra-create-metadata\n            - --leader-election\n          env:\n            - name: ADDRESS\n              value: /var/lib/csi/sockets/pluginproxy/csi.sock\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9909\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /csi\n      volumes:\n        - name: socket-dir\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/controller-deployment.yaml"}, "region": {"startLine": 4, "endLine": 88, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: efs-csi-controller-sa\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n            - --delete-access-point-root-dir=false\n            - --vol-metrics-opt-in=false\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n            - name: CSI_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n          ports:\n            - name: healthz\n              containerPort: 9909\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 10\n            failureThreshold: 5\n        - name: csi-provisioner\n          image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v3.3.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --v=2\n            - --feature-gates=Topology=true\n            - --extra-create-metadata\n            - --leader-election\n          env:\n            - name: ADDRESS\n              value: /var/lib/csi/sockets/pluginproxy/csi.sock\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9909\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /csi\n      volumes:\n        - name: socket-dir\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/controller-deployment.yaml"}, "region": {"startLine": 4, "endLine": 88, "snippet": {"text": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-controller\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: efs-csi-controller\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-controller\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: efs-csi-controller-sa\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n            - --delete-access-point-root-dir=false\n            - --vol-metrics-opt-in=false\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n            - name: CSI_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n          ports:\n            - name: healthz\n              containerPort: 9909\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 10\n            failureThreshold: 5\n        - name: csi-provisioner\n          image: public.ecr.aws/eks-distro/kubernetes-csi/external-provisioner:v3.3.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --v=2\n            - --feature-gates=Topology=true\n            - --extra-create-metadata\n            - --leader-election\n          env:\n            - name: ADDRESS\n              value: /var/lib/csi/sockets/pluginproxy/csi.sock\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /var/lib/csi/sockets/pluginproxy/\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9909\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /csi\n      volumes:\n        - name: socket-dir\n          emptyDir: {}\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/node-daemonset.yaml"}, "region": {"startLine": 4, "endLine": 132, "snippet": {"text": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-node\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: efs-csi-node\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-node\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      hostNetwork: true\n      dnsPolicy: ClusterFirst\n      serviceAccountName: efs-csi-node-sa\n      priorityClassName: system-node-critical\n      tolerations:\n        - operator: Exists\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:/csi/csi.sock\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /csi\n            - name: efs-state-dir\n              mountPath: /var/run/efs\n            - name: efs-utils-config\n              mountPath: /var/amazon/efs\n            - name: efs-utils-config-legacy\n              mountPath: /etc/amazon/efs-legacy\n          ports:\n            - name: healthz\n              containerPort: 9809\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 2\n            failureThreshold: 5\n        - name: csi-driver-registrar\n          image: public.ecr.aws/eks-distro/kubernetes-csi/node-driver-registrar:v2.6.2-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n            - --v=2\n          env:\n            - name: ADDRESS\n              value: /csi/csi.sock\n            - name: DRIVER_REG_SOCK_PATH\n              value: /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n            - name: registration-dir\n              mountPath: /registration\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9809\n            - --v=2\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/efs.csi.aws.com/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: efs-state-dir\n          hostPath:\n            path: /var/run/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config\n          hostPath:\n            path: /var/amazon/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config-legacy\n          hostPath:\n            path: /etc/amazon/efs\n            type: DirectoryOrCreate\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/node-daemonset.yaml"}, "region": {"startLine": 4, "endLine": 132, "snippet": {"text": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-node\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: efs-csi-node\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-node\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      hostNetwork: true\n      dnsPolicy: ClusterFirst\n      serviceAccountName: efs-csi-node-sa\n      priorityClassName: system-node-critical\n      tolerations:\n        - operator: Exists\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:/csi/csi.sock\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /csi\n            - name: efs-state-dir\n              mountPath: /var/run/efs\n            - name: efs-utils-config\n              mountPath: /var/amazon/efs\n            - name: efs-utils-config-legacy\n              mountPath: /etc/amazon/efs-legacy\n          ports:\n            - name: healthz\n              containerPort: 9809\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 2\n            failureThreshold: 5\n        - name: csi-driver-registrar\n          image: public.ecr.aws/eks-distro/kubernetes-csi/node-driver-registrar:v2.6.2-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n            - --v=2\n          env:\n            - name: ADDRESS\n              value: /csi/csi.sock\n            - name: DRIVER_REG_SOCK_PATH\n              value: /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n            - name: registration-dir\n              mountPath: /registration\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9809\n            - --v=2\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/efs.csi.aws.com/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: efs-state-dir\n          hostPath:\n            path: /var/run/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config\n          hostPath:\n            path: /var/amazon/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config-legacy\n          hostPath:\n            path: /etc/amazon/efs\n            type: DirectoryOrCreate\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/node-daemonset.yaml"}, "region": {"startLine": 4, "endLine": 132, "snippet": {"text": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-node\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: efs-csi-node\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-node\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      hostNetwork: true\n      dnsPolicy: ClusterFirst\n      serviceAccountName: efs-csi-node-sa\n      priorityClassName: system-node-critical\n      tolerations:\n        - operator: Exists\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:/csi/csi.sock\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /csi\n            - name: efs-state-dir\n              mountPath: /var/run/efs\n            - name: efs-utils-config\n              mountPath: /var/amazon/efs\n            - name: efs-utils-config-legacy\n              mountPath: /etc/amazon/efs-legacy\n          ports:\n            - name: healthz\n              containerPort: 9809\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 2\n            failureThreshold: 5\n        - name: csi-driver-registrar\n          image: public.ecr.aws/eks-distro/kubernetes-csi/node-driver-registrar:v2.6.2-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n            - --v=2\n          env:\n            - name: ADDRESS\n              value: /csi/csi.sock\n            - name: DRIVER_REG_SOCK_PATH\n              value: /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n            - name: registration-dir\n              mountPath: /registration\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9809\n            - --v=2\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/efs.csi.aws.com/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: efs-state-dir\n          hostPath:\n            path: /var/run/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config\n          hostPath:\n            path: /var/amazon/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config-legacy\n          hostPath:\n            path: /etc/amazon/efs\n            type: DirectoryOrCreate\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/node-daemonset.yaml"}, "region": {"startLine": 4, "endLine": 132, "snippet": {"text": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-node\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: efs-csi-node\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-node\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      hostNetwork: true\n      dnsPolicy: ClusterFirst\n      serviceAccountName: efs-csi-node-sa\n      priorityClassName: system-node-critical\n      tolerations:\n        - operator: Exists\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:/csi/csi.sock\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /csi\n            - name: efs-state-dir\n              mountPath: /var/run/efs\n            - name: efs-utils-config\n              mountPath: /var/amazon/efs\n            - name: efs-utils-config-legacy\n              mountPath: /etc/amazon/efs-legacy\n          ports:\n            - name: healthz\n              containerPort: 9809\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 2\n            failureThreshold: 5\n        - name: csi-driver-registrar\n          image: public.ecr.aws/eks-distro/kubernetes-csi/node-driver-registrar:v2.6.2-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n            - --v=2\n          env:\n            - name: ADDRESS\n              value: /csi/csi.sock\n            - name: DRIVER_REG_SOCK_PATH\n              value: /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n            - name: registration-dir\n              mountPath: /registration\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9809\n            - --v=2\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/efs.csi.aws.com/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: efs-state-dir\n          hostPath:\n            path: /var/run/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config\n          hostPath:\n            path: /var/amazon/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config-legacy\n          hostPath:\n            path: /etc/amazon/efs\n            type: DirectoryOrCreate\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/node-daemonset.yaml"}, "region": {"startLine": 4, "endLine": 132, "snippet": {"text": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-node\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: efs-csi-node\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-node\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      hostNetwork: true\n      dnsPolicy: ClusterFirst\n      serviceAccountName: efs-csi-node-sa\n      priorityClassName: system-node-critical\n      tolerations:\n        - operator: Exists\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:/csi/csi.sock\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /csi\n            - name: efs-state-dir\n              mountPath: /var/run/efs\n            - name: efs-utils-config\n              mountPath: /var/amazon/efs\n            - name: efs-utils-config-legacy\n              mountPath: /etc/amazon/efs-legacy\n          ports:\n            - name: healthz\n              containerPort: 9809\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 2\n            failureThreshold: 5\n        - name: csi-driver-registrar\n          image: public.ecr.aws/eks-distro/kubernetes-csi/node-driver-registrar:v2.6.2-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n            - --v=2\n          env:\n            - name: ADDRESS\n              value: /csi/csi.sock\n            - name: DRIVER_REG_SOCK_PATH\n              value: /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n            - name: registration-dir\n              mountPath: /registration\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9809\n            - --v=2\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/efs.csi.aws.com/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: efs-state-dir\n          hostPath:\n            path: /var/run/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config\n          hostPath:\n            path: /var/amazon/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config-legacy\n          hostPath:\n            path: /etc/amazon/efs\n            type: DirectoryOrCreate\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/node-daemonset.yaml"}, "region": {"startLine": 4, "endLine": 132, "snippet": {"text": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-node\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: efs-csi-node\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-node\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      hostNetwork: true\n      dnsPolicy: ClusterFirst\n      serviceAccountName: efs-csi-node-sa\n      priorityClassName: system-node-critical\n      tolerations:\n        - operator: Exists\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:/csi/csi.sock\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /csi\n            - name: efs-state-dir\n              mountPath: /var/run/efs\n            - name: efs-utils-config\n              mountPath: /var/amazon/efs\n            - name: efs-utils-config-legacy\n              mountPath: /etc/amazon/efs-legacy\n          ports:\n            - name: healthz\n              containerPort: 9809\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 2\n            failureThreshold: 5\n        - name: csi-driver-registrar\n          image: public.ecr.aws/eks-distro/kubernetes-csi/node-driver-registrar:v2.6.2-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n            - --v=2\n          env:\n            - name: ADDRESS\n              value: /csi/csi.sock\n            - name: DRIVER_REG_SOCK_PATH\n              value: /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n            - name: registration-dir\n              mountPath: /registration\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9809\n            - --v=2\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/efs.csi.aws.com/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: efs-state-dir\n          hostPath:\n            path: /var/run/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config\n          hostPath:\n            path: /var/amazon/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config-legacy\n          hostPath:\n            path: /etc/amazon/efs\n            type: DirectoryOrCreate\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/node-daemonset.yaml"}, "region": {"startLine": 4, "endLine": 132, "snippet": {"text": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-node\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: efs-csi-node\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-node\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      hostNetwork: true\n      dnsPolicy: ClusterFirst\n      serviceAccountName: efs-csi-node-sa\n      priorityClassName: system-node-critical\n      tolerations:\n        - operator: Exists\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:/csi/csi.sock\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /csi\n            - name: efs-state-dir\n              mountPath: /var/run/efs\n            - name: efs-utils-config\n              mountPath: /var/amazon/efs\n            - name: efs-utils-config-legacy\n              mountPath: /etc/amazon/efs-legacy\n          ports:\n            - name: healthz\n              containerPort: 9809\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 2\n            failureThreshold: 5\n        - name: csi-driver-registrar\n          image: public.ecr.aws/eks-distro/kubernetes-csi/node-driver-registrar:v2.6.2-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n            - --v=2\n          env:\n            - name: ADDRESS\n              value: /csi/csi.sock\n            - name: DRIVER_REG_SOCK_PATH\n              value: /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n            - name: registration-dir\n              mountPath: /registration\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9809\n            - --v=2\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/efs.csi.aws.com/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: efs-state-dir\n          hostPath:\n            path: /var/run/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config\n          hostPath:\n            path: /var/amazon/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config-legacy\n          hostPath:\n            path: /etc/amazon/efs\n            type: DirectoryOrCreate\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/node-daemonset.yaml"}, "region": {"startLine": 4, "endLine": 132, "snippet": {"text": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-node\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: efs-csi-node\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-node\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      hostNetwork: true\n      dnsPolicy: ClusterFirst\n      serviceAccountName: efs-csi-node-sa\n      priorityClassName: system-node-critical\n      tolerations:\n        - operator: Exists\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:/csi/csi.sock\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /csi\n            - name: efs-state-dir\n              mountPath: /var/run/efs\n            - name: efs-utils-config\n              mountPath: /var/amazon/efs\n            - name: efs-utils-config-legacy\n              mountPath: /etc/amazon/efs-legacy\n          ports:\n            - name: healthz\n              containerPort: 9809\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 2\n            failureThreshold: 5\n        - name: csi-driver-registrar\n          image: public.ecr.aws/eks-distro/kubernetes-csi/node-driver-registrar:v2.6.2-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n            - --v=2\n          env:\n            - name: ADDRESS\n              value: /csi/csi.sock\n            - name: DRIVER_REG_SOCK_PATH\n              value: /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n            - name: registration-dir\n              mountPath: /registration\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9809\n            - --v=2\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/efs.csi.aws.com/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: efs-state-dir\n          hostPath:\n            path: /var/run/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config\n          hostPath:\n            path: /var/amazon/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config-legacy\n          hostPath:\n            path: /etc/amazon/efs\n            type: DirectoryOrCreate\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/node-daemonset.yaml"}, "region": {"startLine": 4, "endLine": 132, "snippet": {"text": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-node\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: efs-csi-node\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-node\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      hostNetwork: true\n      dnsPolicy: ClusterFirst\n      serviceAccountName: efs-csi-node-sa\n      priorityClassName: system-node-critical\n      tolerations:\n        - operator: Exists\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:/csi/csi.sock\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /csi\n            - name: efs-state-dir\n              mountPath: /var/run/efs\n            - name: efs-utils-config\n              mountPath: /var/amazon/efs\n            - name: efs-utils-config-legacy\n              mountPath: /etc/amazon/efs-legacy\n          ports:\n            - name: healthz\n              containerPort: 9809\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 2\n            failureThreshold: 5\n        - name: csi-driver-registrar\n          image: public.ecr.aws/eks-distro/kubernetes-csi/node-driver-registrar:v2.6.2-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n            - --v=2\n          env:\n            - name: ADDRESS\n              value: /csi/csi.sock\n            - name: DRIVER_REG_SOCK_PATH\n              value: /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n            - name: registration-dir\n              mountPath: /registration\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9809\n            - --v=2\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/efs.csi.aws.com/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: efs-state-dir\n          hostPath:\n            path: /var/run/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config\n          hostPath:\n            path: /var/amazon/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config-legacy\n          hostPath:\n            path: /etc/amazon/efs\n            type: DirectoryOrCreate\n"}}}}]}, {"ruleId": "CKV_K8S_16", "ruleIndex": 32, "level": "error", "attachments": [], "message": {"text": "Container should not be privileged"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/node-daemonset.yaml"}, "region": {"startLine": 4, "endLine": 132, "snippet": {"text": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-node\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: efs-csi-node\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-node\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      hostNetwork: true\n      dnsPolicy: ClusterFirst\n      serviceAccountName: efs-csi-node-sa\n      priorityClassName: system-node-critical\n      tolerations:\n        - operator: Exists\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:/csi/csi.sock\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /csi\n            - name: efs-state-dir\n              mountPath: /var/run/efs\n            - name: efs-utils-config\n              mountPath: /var/amazon/efs\n            - name: efs-utils-config-legacy\n              mountPath: /etc/amazon/efs-legacy\n          ports:\n            - name: healthz\n              containerPort: 9809\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 2\n            failureThreshold: 5\n        - name: csi-driver-registrar\n          image: public.ecr.aws/eks-distro/kubernetes-csi/node-driver-registrar:v2.6.2-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n            - --v=2\n          env:\n            - name: ADDRESS\n              value: /csi/csi.sock\n            - name: DRIVER_REG_SOCK_PATH\n              value: /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n            - name: registration-dir\n              mountPath: /registration\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9809\n            - --v=2\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/efs.csi.aws.com/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: efs-state-dir\n          hostPath:\n            path: /var/run/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config\n          hostPath:\n            path: /var/amazon/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config-legacy\n          hostPath:\n            path: /etc/amazon/efs\n            type: DirectoryOrCreate\n"}}}}]}, {"ruleId": "CKV_K8S_19", "ruleIndex": 31, "level": "error", "attachments": [], "message": {"text": "Containers should not share the host network namespace"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/node-daemonset.yaml"}, "region": {"startLine": 4, "endLine": 132, "snippet": {"text": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-node\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: efs-csi-node\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-node\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      hostNetwork: true\n      dnsPolicy: ClusterFirst\n      serviceAccountName: efs-csi-node-sa\n      priorityClassName: system-node-critical\n      tolerations:\n        - operator: Exists\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:/csi/csi.sock\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /csi\n            - name: efs-state-dir\n              mountPath: /var/run/efs\n            - name: efs-utils-config\n              mountPath: /var/amazon/efs\n            - name: efs-utils-config-legacy\n              mountPath: /etc/amazon/efs-legacy\n          ports:\n            - name: healthz\n              containerPort: 9809\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 2\n            failureThreshold: 5\n        - name: csi-driver-registrar\n          image: public.ecr.aws/eks-distro/kubernetes-csi/node-driver-registrar:v2.6.2-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n            - --v=2\n          env:\n            - name: ADDRESS\n              value: /csi/csi.sock\n            - name: DRIVER_REG_SOCK_PATH\n              value: /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n            - name: registration-dir\n              mountPath: /registration\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9809\n            - --v=2\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/efs.csi.aws.com/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: efs-state-dir\n          hostPath:\n            path: /var/run/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config\n          hostPath:\n            path: /var/amazon/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config-legacy\n          hostPath:\n            path: /etc/amazon/efs\n            type: DirectoryOrCreate\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/node-daemonset.yaml"}, "region": {"startLine": 4, "endLine": 132, "snippet": {"text": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-node\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: efs-csi-node\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-node\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      hostNetwork: true\n      dnsPolicy: ClusterFirst\n      serviceAccountName: efs-csi-node-sa\n      priorityClassName: system-node-critical\n      tolerations:\n        - operator: Exists\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:/csi/csi.sock\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /csi\n            - name: efs-state-dir\n              mountPath: /var/run/efs\n            - name: efs-utils-config\n              mountPath: /var/amazon/efs\n            - name: efs-utils-config-legacy\n              mountPath: /etc/amazon/efs-legacy\n          ports:\n            - name: healthz\n              containerPort: 9809\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 2\n            failureThreshold: 5\n        - name: csi-driver-registrar\n          image: public.ecr.aws/eks-distro/kubernetes-csi/node-driver-registrar:v2.6.2-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n            - --v=2\n          env:\n            - name: ADDRESS\n              value: /csi/csi.sock\n            - name: DRIVER_REG_SOCK_PATH\n              value: /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n            - name: registration-dir\n              mountPath: /registration\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9809\n            - --v=2\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/efs.csi.aws.com/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: efs-state-dir\n          hostPath:\n            path: /var/run/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config\n          hostPath:\n            path: /var/amazon/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config-legacy\n          hostPath:\n            path: /etc/amazon/efs\n            type: DirectoryOrCreate\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/node-daemonset.yaml"}, "region": {"startLine": 4, "endLine": 132, "snippet": {"text": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-node\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: efs-csi-node\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-node\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      hostNetwork: true\n      dnsPolicy: ClusterFirst\n      serviceAccountName: efs-csi-node-sa\n      priorityClassName: system-node-critical\n      tolerations:\n        - operator: Exists\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:/csi/csi.sock\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /csi\n            - name: efs-state-dir\n              mountPath: /var/run/efs\n            - name: efs-utils-config\n              mountPath: /var/amazon/efs\n            - name: efs-utils-config-legacy\n              mountPath: /etc/amazon/efs-legacy\n          ports:\n            - name: healthz\n              containerPort: 9809\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 2\n            failureThreshold: 5\n        - name: csi-driver-registrar\n          image: public.ecr.aws/eks-distro/kubernetes-csi/node-driver-registrar:v2.6.2-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n            - --v=2\n          env:\n            - name: ADDRESS\n              value: /csi/csi.sock\n            - name: DRIVER_REG_SOCK_PATH\n              value: /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n            - name: registration-dir\n              mountPath: /registration\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9809\n            - --v=2\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/efs.csi.aws.com/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: efs-state-dir\n          hostPath:\n            path: /var/run/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config\n          hostPath:\n            path: /var/amazon/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config-legacy\n          hostPath:\n            path: /etc/amazon/efs\n            type: DirectoryOrCreate\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/node-daemonset.yaml"}, "region": {"startLine": 4, "endLine": 132, "snippet": {"text": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-node\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: efs-csi-node\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-node\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      hostNetwork: true\n      dnsPolicy: ClusterFirst\n      serviceAccountName: efs-csi-node-sa\n      priorityClassName: system-node-critical\n      tolerations:\n        - operator: Exists\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:/csi/csi.sock\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /csi\n            - name: efs-state-dir\n              mountPath: /var/run/efs\n            - name: efs-utils-config\n              mountPath: /var/amazon/efs\n            - name: efs-utils-config-legacy\n              mountPath: /etc/amazon/efs-legacy\n          ports:\n            - name: healthz\n              containerPort: 9809\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 2\n            failureThreshold: 5\n        - name: csi-driver-registrar\n          image: public.ecr.aws/eks-distro/kubernetes-csi/node-driver-registrar:v2.6.2-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n            - --v=2\n          env:\n            - name: ADDRESS\n              value: /csi/csi.sock\n            - name: DRIVER_REG_SOCK_PATH\n              value: /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n            - name: registration-dir\n              mountPath: /registration\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9809\n            - --v=2\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/efs.csi.aws.com/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: efs-state-dir\n          hostPath:\n            path: /var/run/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config\n          hostPath:\n            path: /var/amazon/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config-legacy\n          hostPath:\n            path: /etc/amazon/efs\n            type: DirectoryOrCreate\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/node-daemonset.yaml"}, "region": {"startLine": 4, "endLine": 132, "snippet": {"text": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-node\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: efs-csi-node\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-node\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      hostNetwork: true\n      dnsPolicy: ClusterFirst\n      serviceAccountName: efs-csi-node-sa\n      priorityClassName: system-node-critical\n      tolerations:\n        - operator: Exists\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:/csi/csi.sock\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /csi\n            - name: efs-state-dir\n              mountPath: /var/run/efs\n            - name: efs-utils-config\n              mountPath: /var/amazon/efs\n            - name: efs-utils-config-legacy\n              mountPath: /etc/amazon/efs-legacy\n          ports:\n            - name: healthz\n              containerPort: 9809\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 2\n            failureThreshold: 5\n        - name: csi-driver-registrar\n          image: public.ecr.aws/eks-distro/kubernetes-csi/node-driver-registrar:v2.6.2-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n            - --v=2\n          env:\n            - name: ADDRESS\n              value: /csi/csi.sock\n            - name: DRIVER_REG_SOCK_PATH\n              value: /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n            - name: registration-dir\n              mountPath: /registration\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9809\n            - --v=2\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/efs.csi.aws.com/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: efs-state-dir\n          hostPath:\n            path: /var/run/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config\n          hostPath:\n            path: /var/amazon/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config-legacy\n          hostPath:\n            path: /etc/amazon/efs\n            type: DirectoryOrCreate\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/node-daemonset.yaml"}, "region": {"startLine": 4, "endLine": 132, "snippet": {"text": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-node\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: efs-csi-node\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-node\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      hostNetwork: true\n      dnsPolicy: ClusterFirst\n      serviceAccountName: efs-csi-node-sa\n      priorityClassName: system-node-critical\n      tolerations:\n        - operator: Exists\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:/csi/csi.sock\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /csi\n            - name: efs-state-dir\n              mountPath: /var/run/efs\n            - name: efs-utils-config\n              mountPath: /var/amazon/efs\n            - name: efs-utils-config-legacy\n              mountPath: /etc/amazon/efs-legacy\n          ports:\n            - name: healthz\n              containerPort: 9809\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 2\n            failureThreshold: 5\n        - name: csi-driver-registrar\n          image: public.ecr.aws/eks-distro/kubernetes-csi/node-driver-registrar:v2.6.2-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n            - --v=2\n          env:\n            - name: ADDRESS\n              value: /csi/csi.sock\n            - name: DRIVER_REG_SOCK_PATH\n              value: /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n            - name: registration-dir\n              mountPath: /registration\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9809\n            - --v=2\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/efs.csi.aws.com/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: efs-state-dir\n          hostPath:\n            path: /var/run/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config\n          hostPath:\n            path: /var/amazon/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config-legacy\n          hostPath:\n            path: /etc/amazon/efs\n            type: DirectoryOrCreate\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/node-daemonset.yaml"}, "region": {"startLine": 4, "endLine": 132, "snippet": {"text": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-node\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: efs-csi-node\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-node\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      hostNetwork: true\n      dnsPolicy: ClusterFirst\n      serviceAccountName: efs-csi-node-sa\n      priorityClassName: system-node-critical\n      tolerations:\n        - operator: Exists\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:/csi/csi.sock\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /csi\n            - name: efs-state-dir\n              mountPath: /var/run/efs\n            - name: efs-utils-config\n              mountPath: /var/amazon/efs\n            - name: efs-utils-config-legacy\n              mountPath: /etc/amazon/efs-legacy\n          ports:\n            - name: healthz\n              containerPort: 9809\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 2\n            failureThreshold: 5\n        - name: csi-driver-registrar\n          image: public.ecr.aws/eks-distro/kubernetes-csi/node-driver-registrar:v2.6.2-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n            - --v=2\n          env:\n            - name: ADDRESS\n              value: /csi/csi.sock\n            - name: DRIVER_REG_SOCK_PATH\n              value: /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n            - name: registration-dir\n              mountPath: /registration\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9809\n            - --v=2\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/efs.csi.aws.com/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: efs-state-dir\n          hostPath:\n            path: /var/run/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config\n          hostPath:\n            path: /var/amazon/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config-legacy\n          hostPath:\n            path: /etc/amazon/efs\n            type: DirectoryOrCreate\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/node-daemonset.yaml"}, "region": {"startLine": 4, "endLine": 132, "snippet": {"text": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-node\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: efs-csi-node\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-node\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      hostNetwork: true\n      dnsPolicy: ClusterFirst\n      serviceAccountName: efs-csi-node-sa\n      priorityClassName: system-node-critical\n      tolerations:\n        - operator: Exists\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:/csi/csi.sock\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /csi\n            - name: efs-state-dir\n              mountPath: /var/run/efs\n            - name: efs-utils-config\n              mountPath: /var/amazon/efs\n            - name: efs-utils-config-legacy\n              mountPath: /etc/amazon/efs-legacy\n          ports:\n            - name: healthz\n              containerPort: 9809\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 2\n            failureThreshold: 5\n        - name: csi-driver-registrar\n          image: public.ecr.aws/eks-distro/kubernetes-csi/node-driver-registrar:v2.6.2-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n            - --v=2\n          env:\n            - name: ADDRESS\n              value: /csi/csi.sock\n            - name: DRIVER_REG_SOCK_PATH\n              value: /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n            - name: registration-dir\n              mountPath: /registration\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9809\n            - --v=2\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/efs.csi.aws.com/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: efs-state-dir\n          hostPath:\n            path: /var/run/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config\n          hostPath:\n            path: /var/amazon/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config-legacy\n          hostPath:\n            path: /etc/amazon/efs\n            type: DirectoryOrCreate\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/node-daemonset.yaml"}, "region": {"startLine": 4, "endLine": 132, "snippet": {"text": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-node\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: efs-csi-node\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-node\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      hostNetwork: true\n      dnsPolicy: ClusterFirst\n      serviceAccountName: efs-csi-node-sa\n      priorityClassName: system-node-critical\n      tolerations:\n        - operator: Exists\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:/csi/csi.sock\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /csi\n            - name: efs-state-dir\n              mountPath: /var/run/efs\n            - name: efs-utils-config\n              mountPath: /var/amazon/efs\n            - name: efs-utils-config-legacy\n              mountPath: /etc/amazon/efs-legacy\n          ports:\n            - name: healthz\n              containerPort: 9809\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 2\n            failureThreshold: 5\n        - name: csi-driver-registrar\n          image: public.ecr.aws/eks-distro/kubernetes-csi/node-driver-registrar:v2.6.2-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n            - --v=2\n          env:\n            - name: ADDRESS\n              value: /csi/csi.sock\n            - name: DRIVER_REG_SOCK_PATH\n              value: /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n            - name: registration-dir\n              mountPath: /registration\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9809\n            - --v=2\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/efs.csi.aws.com/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: efs-state-dir\n          hostPath:\n            path: /var/run/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config\n          hostPath:\n            path: /var/amazon/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config-legacy\n          hostPath:\n            path: /etc/amazon/efs\n            type: DirectoryOrCreate\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/node-daemonset.yaml"}, "region": {"startLine": 4, "endLine": 132, "snippet": {"text": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-node\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: efs-csi-node\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-node\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      hostNetwork: true\n      dnsPolicy: ClusterFirst\n      serviceAccountName: efs-csi-node-sa\n      priorityClassName: system-node-critical\n      tolerations:\n        - operator: Exists\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:/csi/csi.sock\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /csi\n            - name: efs-state-dir\n              mountPath: /var/run/efs\n            - name: efs-utils-config\n              mountPath: /var/amazon/efs\n            - name: efs-utils-config-legacy\n              mountPath: /etc/amazon/efs-legacy\n          ports:\n            - name: healthz\n              containerPort: 9809\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 2\n            failureThreshold: 5\n        - name: csi-driver-registrar\n          image: public.ecr.aws/eks-distro/kubernetes-csi/node-driver-registrar:v2.6.2-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n            - --v=2\n          env:\n            - name: ADDRESS\n              value: /csi/csi.sock\n            - name: DRIVER_REG_SOCK_PATH\n              value: /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n            - name: registration-dir\n              mountPath: /registration\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9809\n            - --v=2\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/efs.csi.aws.com/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: efs-state-dir\n          hostPath:\n            path: /var/run/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config\n          hostPath:\n            path: /var/amazon/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config-legacy\n          hostPath:\n            path: /etc/amazon/efs\n            type: DirectoryOrCreate\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/node-daemonset.yaml"}, "region": {"startLine": 4, "endLine": 132, "snippet": {"text": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: efs-csi-node\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: efs-csi-node\n      app.kubernetes.io/name: aws-efs-csi-driver\n      app.kubernetes.io/instance: aws-efs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: efs-csi-node\n        app.kubernetes.io/name: aws-efs-csi-driver\n        app.kubernetes.io/instance: aws-efs-csi-driver\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      hostNetwork: true\n      dnsPolicy: ClusterFirst\n      serviceAccountName: efs-csi-node-sa\n      priorityClassName: system-node-critical\n      tolerations:\n        - operator: Exists\n      containers:\n        - name: efs-plugin\n          securityContext:\n            privileged: true\n          image: amazon/aws-efs-csi-driver:v1.5.1\n          imagePullPolicy: IfNotPresent\n          args:\n            - --endpoint=$(CSI_ENDPOINT)\n            - --logtostderr\n            - --v=2\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:/csi/csi.sock\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /csi\n            - name: efs-state-dir\n              mountPath: /var/run/efs\n            - name: efs-utils-config\n              mountPath: /var/amazon/efs\n            - name: efs-utils-config-legacy\n              mountPath: /etc/amazon/efs-legacy\n          ports:\n            - name: healthz\n              containerPort: 9809\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: healthz\n            initialDelaySeconds: 10\n            timeoutSeconds: 3\n            periodSeconds: 2\n            failureThreshold: 5\n        - name: csi-driver-registrar\n          image: public.ecr.aws/eks-distro/kubernetes-csi/node-driver-registrar:v2.6.2-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=$(ADDRESS)\n            - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n            - --v=2\n          env:\n            - name: ADDRESS\n              value: /csi/csi.sock\n            - name: DRIVER_REG_SOCK_PATH\n              value: /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n            - name: registration-dir\n              mountPath: /registration\n        - name: liveness-probe\n          image: public.ecr.aws/eks-distro/kubernetes-csi/livenessprobe:v2.8.0-eks-1-25-latest\n          imagePullPolicy: IfNotPresent\n          args:\n            - --csi-address=/csi/csi.sock\n            - --health-port=9809\n            - --v=2\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /csi\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/efs.csi.aws.com/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: efs-state-dir\n          hostPath:\n            path: /var/run/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config\n          hostPath:\n            path: /var/amazon/efs\n            type: DirectoryOrCreate\n        - name: efs-utils-config-legacy\n          hostPath:\n            path: /etc/amazon/efs\n            type: DirectoryOrCreate\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/aws-efs-csi-driver/templates/node-serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 8, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: efs-csi-node-sa\n  labels:\n    app.kubernetes.io/name: aws-efs-csi-driver\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 62, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smtp\n  labels:\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: smtp\n      app.kubernetes.io/instance: smtp\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: smtp\n        app.kubernetes.io/instance: smtp\n      annotations:\n        checksum/config: c9a2e600f27e111b8aaebd15cf8680047165572c381d4087ec44fb1e54869d1a\n    spec:\n      serviceAccountName: smtp\n      securityContext:\n        {}\n      containers:\n        - name: smtp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: IfNotPresent\n          envFrom:\n          - secretRef:\n              name: smtp-secrets    \n          ports:\n            - name: smtp\n              containerPort: 25\n              protocol: TCP\n            - name: submission\n              containerPort: 587\n              protocol: TCP\n          readinessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 5\n            periodSeconds: 30\n          livenessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 15\n            periodSeconds: 60\n          resources:\n            limits:\n              cpu: 200m\n              memory: 50Mi\n            requests:\n              cpu: 10m\n              memory: 10Mi\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 62, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smtp\n  labels:\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: smtp\n      app.kubernetes.io/instance: smtp\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: smtp\n        app.kubernetes.io/instance: smtp\n      annotations:\n        checksum/config: c9a2e600f27e111b8aaebd15cf8680047165572c381d4087ec44fb1e54869d1a\n    spec:\n      serviceAccountName: smtp\n      securityContext:\n        {}\n      containers:\n        - name: smtp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: IfNotPresent\n          envFrom:\n          - secretRef:\n              name: smtp-secrets    \n          ports:\n            - name: smtp\n              containerPort: 25\n              protocol: TCP\n            - name: submission\n              containerPort: 587\n              protocol: TCP\n          readinessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 5\n            periodSeconds: 30\n          livenessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 15\n            periodSeconds: 60\n          resources:\n            limits:\n              cpu: 200m\n              memory: 50Mi\n            requests:\n              cpu: 10m\n              memory: 10Mi\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 62, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smtp\n  labels:\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: smtp\n      app.kubernetes.io/instance: smtp\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: smtp\n        app.kubernetes.io/instance: smtp\n      annotations:\n        checksum/config: c9a2e600f27e111b8aaebd15cf8680047165572c381d4087ec44fb1e54869d1a\n    spec:\n      serviceAccountName: smtp\n      securityContext:\n        {}\n      containers:\n        - name: smtp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: IfNotPresent\n          envFrom:\n          - secretRef:\n              name: smtp-secrets    \n          ports:\n            - name: smtp\n              containerPort: 25\n              protocol: TCP\n            - name: submission\n              containerPort: 587\n              protocol: TCP\n          readinessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 5\n            periodSeconds: 30\n          livenessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 15\n            periodSeconds: 60\n          resources:\n            limits:\n              cpu: 200m\n              memory: 50Mi\n            requests:\n              cpu: 10m\n              memory: 10Mi\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 62, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smtp\n  labels:\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: smtp\n      app.kubernetes.io/instance: smtp\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: smtp\n        app.kubernetes.io/instance: smtp\n      annotations:\n        checksum/config: c9a2e600f27e111b8aaebd15cf8680047165572c381d4087ec44fb1e54869d1a\n    spec:\n      serviceAccountName: smtp\n      securityContext:\n        {}\n      containers:\n        - name: smtp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: IfNotPresent\n          envFrom:\n          - secretRef:\n              name: smtp-secrets    \n          ports:\n            - name: smtp\n              containerPort: 25\n              protocol: TCP\n            - name: submission\n              containerPort: 587\n              protocol: TCP\n          readinessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 5\n            periodSeconds: 30\n          livenessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 15\n            periodSeconds: 60\n          resources:\n            limits:\n              cpu: 200m\n              memory: 50Mi\n            requests:\n              cpu: 10m\n              memory: 10Mi\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 62, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smtp\n  labels:\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: smtp\n      app.kubernetes.io/instance: smtp\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: smtp\n        app.kubernetes.io/instance: smtp\n      annotations:\n        checksum/config: c9a2e600f27e111b8aaebd15cf8680047165572c381d4087ec44fb1e54869d1a\n    spec:\n      serviceAccountName: smtp\n      securityContext:\n        {}\n      containers:\n        - name: smtp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: IfNotPresent\n          envFrom:\n          - secretRef:\n              name: smtp-secrets    \n          ports:\n            - name: smtp\n              containerPort: 25\n              protocol: TCP\n            - name: submission\n              containerPort: 587\n              protocol: TCP\n          readinessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 5\n            periodSeconds: 30\n          livenessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 15\n            periodSeconds: 60\n          resources:\n            limits:\n              cpu: 200m\n              memory: 50Mi\n            requests:\n              cpu: 10m\n              memory: 10Mi\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 62, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smtp\n  labels:\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: smtp\n      app.kubernetes.io/instance: smtp\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: smtp\n        app.kubernetes.io/instance: smtp\n      annotations:\n        checksum/config: c9a2e600f27e111b8aaebd15cf8680047165572c381d4087ec44fb1e54869d1a\n    spec:\n      serviceAccountName: smtp\n      securityContext:\n        {}\n      containers:\n        - name: smtp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: IfNotPresent\n          envFrom:\n          - secretRef:\n              name: smtp-secrets    \n          ports:\n            - name: smtp\n              containerPort: 25\n              protocol: TCP\n            - name: submission\n              containerPort: 587\n              protocol: TCP\n          readinessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 5\n            periodSeconds: 30\n          livenessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 15\n            periodSeconds: 60\n          resources:\n            limits:\n              cpu: 200m\n              memory: 50Mi\n            requests:\n              cpu: 10m\n              memory: 10Mi\n"}}}}]}, {"ruleId": "CKV_K8S_35", "ruleIndex": 6, "level": "error", "attachments": [], "message": {"text": "Prefer using secrets as files over secrets as environment variables"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 62, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smtp\n  labels:\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: smtp\n      app.kubernetes.io/instance: smtp\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: smtp\n        app.kubernetes.io/instance: smtp\n      annotations:\n        checksum/config: c9a2e600f27e111b8aaebd15cf8680047165572c381d4087ec44fb1e54869d1a\n    spec:\n      serviceAccountName: smtp\n      securityContext:\n        {}\n      containers:\n        - name: smtp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: IfNotPresent\n          envFrom:\n          - secretRef:\n              name: smtp-secrets    \n          ports:\n            - name: smtp\n              containerPort: 25\n              protocol: TCP\n            - name: submission\n              containerPort: 587\n              protocol: TCP\n          readinessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 5\n            periodSeconds: 30\n          livenessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 15\n            periodSeconds: 60\n          resources:\n            limits:\n              cpu: 200m\n              memory: 50Mi\n            requests:\n              cpu: 10m\n              memory: 10Mi\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 62, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smtp\n  labels:\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: smtp\n      app.kubernetes.io/instance: smtp\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: smtp\n        app.kubernetes.io/instance: smtp\n      annotations:\n        checksum/config: c9a2e600f27e111b8aaebd15cf8680047165572c381d4087ec44fb1e54869d1a\n    spec:\n      serviceAccountName: smtp\n      securityContext:\n        {}\n      containers:\n        - name: smtp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: IfNotPresent\n          envFrom:\n          - secretRef:\n              name: smtp-secrets    \n          ports:\n            - name: smtp\n              containerPort: 25\n              protocol: TCP\n            - name: submission\n              containerPort: 587\n              protocol: TCP\n          readinessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 5\n            periodSeconds: 30\n          livenessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 15\n            periodSeconds: 60\n          resources:\n            limits:\n              cpu: 200m\n              memory: 50Mi\n            requests:\n              cpu: 10m\n              memory: 10Mi\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 62, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smtp\n  labels:\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: smtp\n      app.kubernetes.io/instance: smtp\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: smtp\n        app.kubernetes.io/instance: smtp\n      annotations:\n        checksum/config: c9a2e600f27e111b8aaebd15cf8680047165572c381d4087ec44fb1e54869d1a\n    spec:\n      serviceAccountName: smtp\n      securityContext:\n        {}\n      containers:\n        - name: smtp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: IfNotPresent\n          envFrom:\n          - secretRef:\n              name: smtp-secrets    \n          ports:\n            - name: smtp\n              containerPort: 25\n              protocol: TCP\n            - name: submission\n              containerPort: 587\n              protocol: TCP\n          readinessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 5\n            periodSeconds: 30\n          livenessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 15\n            periodSeconds: 60\n          resources:\n            limits:\n              cpu: 200m\n              memory: 50Mi\n            requests:\n              cpu: 10m\n              memory: 10Mi\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 62, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smtp\n  labels:\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: smtp\n      app.kubernetes.io/instance: smtp\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: smtp\n        app.kubernetes.io/instance: smtp\n      annotations:\n        checksum/config: c9a2e600f27e111b8aaebd15cf8680047165572c381d4087ec44fb1e54869d1a\n    spec:\n      serviceAccountName: smtp\n      securityContext:\n        {}\n      containers:\n        - name: smtp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: IfNotPresent\n          envFrom:\n          - secretRef:\n              name: smtp-secrets    \n          ports:\n            - name: smtp\n              containerPort: 25\n              protocol: TCP\n            - name: submission\n              containerPort: 587\n              protocol: TCP\n          readinessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 5\n            periodSeconds: 30\n          livenessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 15\n            periodSeconds: 60\n          resources:\n            limits:\n              cpu: 200m\n              memory: 50Mi\n            requests:\n              cpu: 10m\n              memory: 10Mi\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 62, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smtp\n  labels:\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: smtp\n      app.kubernetes.io/instance: smtp\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: smtp\n        app.kubernetes.io/instance: smtp\n      annotations:\n        checksum/config: c9a2e600f27e111b8aaebd15cf8680047165572c381d4087ec44fb1e54869d1a\n    spec:\n      serviceAccountName: smtp\n      securityContext:\n        {}\n      containers:\n        - name: smtp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: IfNotPresent\n          envFrom:\n          - secretRef:\n              name: smtp-secrets    \n          ports:\n            - name: smtp\n              containerPort: 25\n              protocol: TCP\n            - name: submission\n              containerPort: 587\n              protocol: TCP\n          readinessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 5\n            periodSeconds: 30\n          livenessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 15\n            periodSeconds: 60\n          resources:\n            limits:\n              cpu: 200m\n              memory: 50Mi\n            requests:\n              cpu: 10m\n              memory: 10Mi\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 62, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smtp\n  labels:\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: smtp\n      app.kubernetes.io/instance: smtp\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: smtp\n        app.kubernetes.io/instance: smtp\n      annotations:\n        checksum/config: c9a2e600f27e111b8aaebd15cf8680047165572c381d4087ec44fb1e54869d1a\n    spec:\n      serviceAccountName: smtp\n      securityContext:\n        {}\n      containers:\n        - name: smtp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: IfNotPresent\n          envFrom:\n          - secretRef:\n              name: smtp-secrets    \n          ports:\n            - name: smtp\n              containerPort: 25\n              protocol: TCP\n            - name: submission\n              containerPort: 587\n              protocol: TCP\n          readinessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 5\n            periodSeconds: 30\n          livenessProbe:\n            tcpSocket:\n              port: smtp\n            initialDelaySeconds: 15\n            periodSeconds: 60\n          resources:\n            limits:\n              cpu: 200m\n              memory: 50Mi\n            requests:\n              cpu: 10m\n              memory: 10Mi\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/config.yaml"}, "region": {"startLine": 3, "endLine": 16, "snippet": {"text": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: smtp-secrets\n  labels:\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\ntype: Opaque\ndata:\n  MAILNAME: \"bWFpbA==\"\n  RELAY_NETWORKS: \"MTAuMC4wLjAvOCwgMTcyLjE2LjAuMC8xMiwgMTkyLjE2OC4wLjAvMTYsIDEyNy4wLjAuMC84\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/service.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: smtp\n  labels:\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ClusterIP\n  ports:\n    - port: 25\n      targetPort: smtp\n      protocol: TCP\n      name: tcp-smtp\n  selector:\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 13, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: smtp\n  labels:\n\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"smtp-test-connection\"\n  labels:\n\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args:  ['smtp:25']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"smtp-test-connection\"\n  labels:\n\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args:  ['smtp:25']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"smtp-test-connection\"\n  labels:\n\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args:  ['smtp:25']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"smtp-test-connection\"\n  labels:\n\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args:  ['smtp:25']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"smtp-test-connection\"\n  labels:\n\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args:  ['smtp:25']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"smtp-test-connection\"\n  labels:\n\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args:  ['smtp:25']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"smtp-test-connection\"\n  labels:\n\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args:  ['smtp:25']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"smtp-test-connection\"\n  labels:\n\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args:  ['smtp:25']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"smtp-test-connection\"\n  labels:\n\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args:  ['smtp:25']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"smtp-test-connection\"\n  labels:\n\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args:  ['smtp:25']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"smtp-test-connection\"\n  labels:\n\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args:  ['smtp:25']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"smtp-test-connection\"\n  labels:\n\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args:  ['smtp:25']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"smtp-test-connection\"\n  labels:\n\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args:  ['smtp:25']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"smtp-test-connection\"\n  labels:\n\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args:  ['smtp:25']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"smtp-test-connection\"\n  labels:\n\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args:  ['smtp:25']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"smtp-test-connection\"\n  labels:\n\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args:  ['smtp:25']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"smtp-test-connection\"\n  labels:\n\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args:  ['smtp:25']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"smtp-test-connection\"\n  labels:\n\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args:  ['smtp:25']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/smtp/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"smtp-test-connection\"\n  labels:\n\n    helm.sh/chart: smtp-1.0.1\n    app.kubernetes.io/name: smtp\n    app.kubernetes.io/instance: smtp\n    app.kubernetes.io/version: \"latest\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args:  ['smtp:25']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-tigera-operator.yaml"}, "region": {"startLine": 3, "endLine": 60, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tigera-operator\n  namespace: default\n  labels:\n    k8s-app: tigera-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tigera-operator\n  template:\n    metadata:\n      labels:\n        name: tigera-operator\n        k8s-app: tigera-operator\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      serviceAccountName: tigera-operator\n      hostNetwork: true\n      # This must be set when hostNetwork is true or else the cluster services won't resolve\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n        - name: tigera-operator\n          image: quay.io/tigera/operator:v1.27.12\n          imagePullPolicy: IfNotPresent\n          command:\n            - operator\n          volumeMounts:\n            - name: var-lib-calico\n              readOnly: true\n              mountPath: /var/lib/calico\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"tigera-operator\"\n            - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION\n              value: v1.27.12\n          envFrom:\n            - configMapRef:\n                name: kubernetes-services-endpoint\n                optional: true\n      volumes:\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-tigera-operator.yaml"}, "region": {"startLine": 3, "endLine": 60, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tigera-operator\n  namespace: default\n  labels:\n    k8s-app: tigera-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tigera-operator\n  template:\n    metadata:\n      labels:\n        name: tigera-operator\n        k8s-app: tigera-operator\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      serviceAccountName: tigera-operator\n      hostNetwork: true\n      # This must be set when hostNetwork is true or else the cluster services won't resolve\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n        - name: tigera-operator\n          image: quay.io/tigera/operator:v1.27.12\n          imagePullPolicy: IfNotPresent\n          command:\n            - operator\n          volumeMounts:\n            - name: var-lib-calico\n              readOnly: true\n              mountPath: /var/lib/calico\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"tigera-operator\"\n            - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION\n              value: v1.27.12\n          envFrom:\n            - configMapRef:\n                name: kubernetes-services-endpoint\n                optional: true\n      volumes:\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-tigera-operator.yaml"}, "region": {"startLine": 3, "endLine": 60, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tigera-operator\n  namespace: default\n  labels:\n    k8s-app: tigera-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tigera-operator\n  template:\n    metadata:\n      labels:\n        name: tigera-operator\n        k8s-app: tigera-operator\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      serviceAccountName: tigera-operator\n      hostNetwork: true\n      # This must be set when hostNetwork is true or else the cluster services won't resolve\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n        - name: tigera-operator\n          image: quay.io/tigera/operator:v1.27.12\n          imagePullPolicy: IfNotPresent\n          command:\n            - operator\n          volumeMounts:\n            - name: var-lib-calico\n              readOnly: true\n              mountPath: /var/lib/calico\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"tigera-operator\"\n            - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION\n              value: v1.27.12\n          envFrom:\n            - configMapRef:\n                name: kubernetes-services-endpoint\n                optional: true\n      volumes:\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-tigera-operator.yaml"}, "region": {"startLine": 3, "endLine": 60, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tigera-operator\n  namespace: default\n  labels:\n    k8s-app: tigera-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tigera-operator\n  template:\n    metadata:\n      labels:\n        name: tigera-operator\n        k8s-app: tigera-operator\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      serviceAccountName: tigera-operator\n      hostNetwork: true\n      # This must be set when hostNetwork is true or else the cluster services won't resolve\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n        - name: tigera-operator\n          image: quay.io/tigera/operator:v1.27.12\n          imagePullPolicy: IfNotPresent\n          command:\n            - operator\n          volumeMounts:\n            - name: var-lib-calico\n              readOnly: true\n              mountPath: /var/lib/calico\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"tigera-operator\"\n            - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION\n              value: v1.27.12\n          envFrom:\n            - configMapRef:\n                name: kubernetes-services-endpoint\n                optional: true\n      volumes:\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-tigera-operator.yaml"}, "region": {"startLine": 3, "endLine": 60, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tigera-operator\n  namespace: default\n  labels:\n    k8s-app: tigera-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tigera-operator\n  template:\n    metadata:\n      labels:\n        name: tigera-operator\n        k8s-app: tigera-operator\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      serviceAccountName: tigera-operator\n      hostNetwork: true\n      # This must be set when hostNetwork is true or else the cluster services won't resolve\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n        - name: tigera-operator\n          image: quay.io/tigera/operator:v1.27.12\n          imagePullPolicy: IfNotPresent\n          command:\n            - operator\n          volumeMounts:\n            - name: var-lib-calico\n              readOnly: true\n              mountPath: /var/lib/calico\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"tigera-operator\"\n            - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION\n              value: v1.27.12\n          envFrom:\n            - configMapRef:\n                name: kubernetes-services-endpoint\n                optional: true\n      volumes:\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-tigera-operator.yaml"}, "region": {"startLine": 3, "endLine": 60, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tigera-operator\n  namespace: default\n  labels:\n    k8s-app: tigera-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tigera-operator\n  template:\n    metadata:\n      labels:\n        name: tigera-operator\n        k8s-app: tigera-operator\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      serviceAccountName: tigera-operator\n      hostNetwork: true\n      # This must be set when hostNetwork is true or else the cluster services won't resolve\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n        - name: tigera-operator\n          image: quay.io/tigera/operator:v1.27.12\n          imagePullPolicy: IfNotPresent\n          command:\n            - operator\n          volumeMounts:\n            - name: var-lib-calico\n              readOnly: true\n              mountPath: /var/lib/calico\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"tigera-operator\"\n            - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION\n              value: v1.27.12\n          envFrom:\n            - configMapRef:\n                name: kubernetes-services-endpoint\n                optional: true\n      volumes:\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-tigera-operator.yaml"}, "region": {"startLine": 3, "endLine": 60, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tigera-operator\n  namespace: default\n  labels:\n    k8s-app: tigera-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tigera-operator\n  template:\n    metadata:\n      labels:\n        name: tigera-operator\n        k8s-app: tigera-operator\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      serviceAccountName: tigera-operator\n      hostNetwork: true\n      # This must be set when hostNetwork is true or else the cluster services won't resolve\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n        - name: tigera-operator\n          image: quay.io/tigera/operator:v1.27.12\n          imagePullPolicy: IfNotPresent\n          command:\n            - operator\n          volumeMounts:\n            - name: var-lib-calico\n              readOnly: true\n              mountPath: /var/lib/calico\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"tigera-operator\"\n            - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION\n              value: v1.27.12\n          envFrom:\n            - configMapRef:\n                name: kubernetes-services-endpoint\n                optional: true\n      volumes:\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-tigera-operator.yaml"}, "region": {"startLine": 3, "endLine": 60, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tigera-operator\n  namespace: default\n  labels:\n    k8s-app: tigera-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tigera-operator\n  template:\n    metadata:\n      labels:\n        name: tigera-operator\n        k8s-app: tigera-operator\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      serviceAccountName: tigera-operator\n      hostNetwork: true\n      # This must be set when hostNetwork is true or else the cluster services won't resolve\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n        - name: tigera-operator\n          image: quay.io/tigera/operator:v1.27.12\n          imagePullPolicy: IfNotPresent\n          command:\n            - operator\n          volumeMounts:\n            - name: var-lib-calico\n              readOnly: true\n              mountPath: /var/lib/calico\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"tigera-operator\"\n            - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION\n              value: v1.27.12\n          envFrom:\n            - configMapRef:\n                name: kubernetes-services-endpoint\n                optional: true\n      volumes:\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-tigera-operator.yaml"}, "region": {"startLine": 3, "endLine": 60, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tigera-operator\n  namespace: default\n  labels:\n    k8s-app: tigera-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tigera-operator\n  template:\n    metadata:\n      labels:\n        name: tigera-operator\n        k8s-app: tigera-operator\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      serviceAccountName: tigera-operator\n      hostNetwork: true\n      # This must be set when hostNetwork is true or else the cluster services won't resolve\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n        - name: tigera-operator\n          image: quay.io/tigera/operator:v1.27.12\n          imagePullPolicy: IfNotPresent\n          command:\n            - operator\n          volumeMounts:\n            - name: var-lib-calico\n              readOnly: true\n              mountPath: /var/lib/calico\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"tigera-operator\"\n            - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION\n              value: v1.27.12\n          envFrom:\n            - configMapRef:\n                name: kubernetes-services-endpoint\n                optional: true\n      volumes:\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n"}}}}]}, {"ruleId": "CKV_K8S_19", "ruleIndex": 31, "level": "error", "attachments": [], "message": {"text": "Containers should not share the host network namespace"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-tigera-operator.yaml"}, "region": {"startLine": 3, "endLine": 60, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tigera-operator\n  namespace: default\n  labels:\n    k8s-app: tigera-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tigera-operator\n  template:\n    metadata:\n      labels:\n        name: tigera-operator\n        k8s-app: tigera-operator\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      serviceAccountName: tigera-operator\n      hostNetwork: true\n      # This must be set when hostNetwork is true or else the cluster services won't resolve\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n        - name: tigera-operator\n          image: quay.io/tigera/operator:v1.27.12\n          imagePullPolicy: IfNotPresent\n          command:\n            - operator\n          volumeMounts:\n            - name: var-lib-calico\n              readOnly: true\n              mountPath: /var/lib/calico\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"tigera-operator\"\n            - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION\n              value: v1.27.12\n          envFrom:\n            - configMapRef:\n                name: kubernetes-services-endpoint\n                optional: true\n      volumes:\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-tigera-operator.yaml"}, "region": {"startLine": 3, "endLine": 60, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tigera-operator\n  namespace: default\n  labels:\n    k8s-app: tigera-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tigera-operator\n  template:\n    metadata:\n      labels:\n        name: tigera-operator\n        k8s-app: tigera-operator\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      serviceAccountName: tigera-operator\n      hostNetwork: true\n      # This must be set when hostNetwork is true or else the cluster services won't resolve\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n        - name: tigera-operator\n          image: quay.io/tigera/operator:v1.27.12\n          imagePullPolicy: IfNotPresent\n          command:\n            - operator\n          volumeMounts:\n            - name: var-lib-calico\n              readOnly: true\n              mountPath: /var/lib/calico\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"tigera-operator\"\n            - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION\n              value: v1.27.12\n          envFrom:\n            - configMapRef:\n                name: kubernetes-services-endpoint\n                optional: true\n      volumes:\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-tigera-operator.yaml"}, "region": {"startLine": 3, "endLine": 60, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tigera-operator\n  namespace: default\n  labels:\n    k8s-app: tigera-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tigera-operator\n  template:\n    metadata:\n      labels:\n        name: tigera-operator\n        k8s-app: tigera-operator\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      serviceAccountName: tigera-operator\n      hostNetwork: true\n      # This must be set when hostNetwork is true or else the cluster services won't resolve\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n        - name: tigera-operator\n          image: quay.io/tigera/operator:v1.27.12\n          imagePullPolicy: IfNotPresent\n          command:\n            - operator\n          volumeMounts:\n            - name: var-lib-calico\n              readOnly: true\n              mountPath: /var/lib/calico\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"tigera-operator\"\n            - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION\n              value: v1.27.12\n          envFrom:\n            - configMapRef:\n                name: kubernetes-services-endpoint\n                optional: true\n      volumes:\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-tigera-operator.yaml"}, "region": {"startLine": 3, "endLine": 60, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tigera-operator\n  namespace: default\n  labels:\n    k8s-app: tigera-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tigera-operator\n  template:\n    metadata:\n      labels:\n        name: tigera-operator\n        k8s-app: tigera-operator\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      serviceAccountName: tigera-operator\n      hostNetwork: true\n      # This must be set when hostNetwork is true or else the cluster services won't resolve\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n        - name: tigera-operator\n          image: quay.io/tigera/operator:v1.27.12\n          imagePullPolicy: IfNotPresent\n          command:\n            - operator\n          volumeMounts:\n            - name: var-lib-calico\n              readOnly: true\n              mountPath: /var/lib/calico\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"tigera-operator\"\n            - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION\n              value: v1.27.12\n          envFrom:\n            - configMapRef:\n                name: kubernetes-services-endpoint\n                optional: true\n      volumes:\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-tigera-operator.yaml"}, "region": {"startLine": 3, "endLine": 60, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tigera-operator\n  namespace: default\n  labels:\n    k8s-app: tigera-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tigera-operator\n  template:\n    metadata:\n      labels:\n        name: tigera-operator\n        k8s-app: tigera-operator\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      serviceAccountName: tigera-operator\n      hostNetwork: true\n      # This must be set when hostNetwork is true or else the cluster services won't resolve\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n        - name: tigera-operator\n          image: quay.io/tigera/operator:v1.27.12\n          imagePullPolicy: IfNotPresent\n          command:\n            - operator\n          volumeMounts:\n            - name: var-lib-calico\n              readOnly: true\n              mountPath: /var/lib/calico\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"tigera-operator\"\n            - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION\n              value: v1.27.12\n          envFrom:\n            - configMapRef:\n                name: kubernetes-services-endpoint\n                optional: true\n      volumes:\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-tigera-operator.yaml"}, "region": {"startLine": 3, "endLine": 60, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tigera-operator\n  namespace: default\n  labels:\n    k8s-app: tigera-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tigera-operator\n  template:\n    metadata:\n      labels:\n        name: tigera-operator\n        k8s-app: tigera-operator\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      serviceAccountName: tigera-operator\n      hostNetwork: true\n      # This must be set when hostNetwork is true or else the cluster services won't resolve\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n        - name: tigera-operator\n          image: quay.io/tigera/operator:v1.27.12\n          imagePullPolicy: IfNotPresent\n          command:\n            - operator\n          volumeMounts:\n            - name: var-lib-calico\n              readOnly: true\n              mountPath: /var/lib/calico\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"tigera-operator\"\n            - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION\n              value: v1.27.12\n          envFrom:\n            - configMapRef:\n                name: kubernetes-services-endpoint\n                optional: true\n      volumes:\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-tigera-operator.yaml"}, "region": {"startLine": 3, "endLine": 60, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tigera-operator\n  namespace: default\n  labels:\n    k8s-app: tigera-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tigera-operator\n  template:\n    metadata:\n      labels:\n        name: tigera-operator\n        k8s-app: tigera-operator\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      serviceAccountName: tigera-operator\n      hostNetwork: true\n      # This must be set when hostNetwork is true or else the cluster services won't resolve\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n        - name: tigera-operator\n          image: quay.io/tigera/operator:v1.27.12\n          imagePullPolicy: IfNotPresent\n          command:\n            - operator\n          volumeMounts:\n            - name: var-lib-calico\n              readOnly: true\n              mountPath: /var/lib/calico\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"tigera-operator\"\n            - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION\n              value: v1.27.12\n          envFrom:\n            - configMapRef:\n                name: kubernetes-services-endpoint\n                optional: true\n      volumes:\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-tigera-operator.yaml"}, "region": {"startLine": 3, "endLine": 60, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tigera-operator\n  namespace: default\n  labels:\n    k8s-app: tigera-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tigera-operator\n  template:\n    metadata:\n      labels:\n        name: tigera-operator\n        k8s-app: tigera-operator\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      serviceAccountName: tigera-operator\n      hostNetwork: true\n      # This must be set when hostNetwork is true or else the cluster services won't resolve\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n        - name: tigera-operator\n          image: quay.io/tigera/operator:v1.27.12\n          imagePullPolicy: IfNotPresent\n          command:\n            - operator\n          volumeMounts:\n            - name: var-lib-calico\n              readOnly: true\n              mountPath: /var/lib/calico\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"tigera-operator\"\n            - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION\n              value: v1.27.12\n          envFrom:\n            - configMapRef:\n                name: kubernetes-services-endpoint\n                optional: true\n      volumes:\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-tigera-operator.yaml"}, "region": {"startLine": 3, "endLine": 60, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tigera-operator\n  namespace: default\n  labels:\n    k8s-app: tigera-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tigera-operator\n  template:\n    metadata:\n      labels:\n        name: tigera-operator\n        k8s-app: tigera-operator\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      serviceAccountName: tigera-operator\n      hostNetwork: true\n      # This must be set when hostNetwork is true or else the cluster services won't resolve\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n        - name: tigera-operator\n          image: quay.io/tigera/operator:v1.27.12\n          imagePullPolicy: IfNotPresent\n          command:\n            - operator\n          volumeMounts:\n            - name: var-lib-calico\n              readOnly: true\n              mountPath: /var/lib/calico\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"tigera-operator\"\n            - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION\n              value: v1.27.12\n          envFrom:\n            - configMapRef:\n                name: kubernetes-services-endpoint\n                optional: true\n      volumes:\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-tigera-operator.yaml"}, "region": {"startLine": 3, "endLine": 60, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tigera-operator\n  namespace: default\n  labels:\n    k8s-app: tigera-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tigera-operator\n  template:\n    metadata:\n      labels:\n        name: tigera-operator\n        k8s-app: tigera-operator\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      serviceAccountName: tigera-operator\n      hostNetwork: true\n      # This must be set when hostNetwork is true or else the cluster services won't resolve\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n        - name: tigera-operator\n          image: quay.io/tigera/operator:v1.27.12\n          imagePullPolicy: IfNotPresent\n          command:\n            - operator\n          volumeMounts:\n            - name: var-lib-calico\n              readOnly: true\n              mountPath: /var/lib/calico\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"tigera-operator\"\n            - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION\n              value: v1.27.12\n          envFrom:\n            - configMapRef:\n                name: kubernetes-services-endpoint\n                optional: true\n      volumes:\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-tigera-operator.yaml"}, "region": {"startLine": 3, "endLine": 60, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tigera-operator\n  namespace: default\n  labels:\n    k8s-app: tigera-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: tigera-operator\n  template:\n    metadata:\n      labels:\n        name: tigera-operator\n        k8s-app: tigera-operator\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      serviceAccountName: tigera-operator\n      hostNetwork: true\n      # This must be set when hostNetwork is true or else the cluster services won't resolve\n      dnsPolicy: ClusterFirstWithHostNet\n      containers:\n        - name: tigera-operator\n          image: quay.io/tigera/operator:v1.27.12\n          imagePullPolicy: IfNotPresent\n          command:\n            - operator\n          volumeMounts:\n            - name: var-lib-calico\n              readOnly: true\n              mountPath: /var/lib/calico\n          env:\n            - name: WATCH_NAMESPACE\n              value: \"\"\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: OPERATOR_NAME\n              value: \"tigera-operator\"\n            - name: TIGERA_OPERATOR_INIT_IMAGE_VERSION\n              value: v1.27.12\n          envFrom:\n            - configMapRef:\n                name: kubernetes-services-endpoint\n                optional: true\n      volumes:\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n"}}}}]}, {"ruleId": "CKV_K8S_4", "ruleIndex": 33, "level": "error", "attachments": [], "message": {"text": "Do not admit containers wishing to share the host network namespace"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-podsecuritypolicy-tigera-operator.yaml"}, "region": {"startLine": 5, "endLine": 47, "snippet": {"text": "apiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: tigera-operator\n  annotations:\n    seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\nspec:\n  privileged: false\n  allowPrivilegeEscalation: false\n  requiredDropCapabilities:\n  - ALL\n  volumes:\n  - 'hostPath'\n  - 'configMap'\n  - 'emptyDir'\n  - 'projected'\n  - 'secret'\n  - 'downwardAPI'\n  # Assume that persistentVolumes set up by the cluster admin are safe to use.\n  - 'persistentVolumeClaim'\n  hostNetwork: true\n  hostPorts:\n  - min: 0\n    max: 65535\n  hostIPC: false\n  hostPID: false\n  runAsUser:\n    rule: 'MustRunAsNonRoot'\n  seLinux:\n    rule: 'RunAsAny'\n  supplementalGroups:\n    rule: 'MustRunAs'\n    ranges:\n      # Forbid adding the root group.\n      - min: 1\n        max: 65535\n  fsGroup:\n    rule: 'MustRunAs'\n    ranges:\n      # Forbid adding the root group.\n      - min: 1\n        max: 65535\n  readOnlyRootFilesystem: false\n"}}}}]}, {"ruleId": "CKV_K8S_32", "ruleIndex": 29, "level": "error", "attachments": [], "message": {"text": "Ensure default seccomp profile set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-podsecuritypolicy-tigera-operator.yaml"}, "region": {"startLine": 5, "endLine": 47, "snippet": {"text": "apiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: tigera-operator\n  annotations:\n    seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\nspec:\n  privileged: false\n  allowPrivilegeEscalation: false\n  requiredDropCapabilities:\n  - ALL\n  volumes:\n  - 'hostPath'\n  - 'configMap'\n  - 'emptyDir'\n  - 'projected'\n  - 'secret'\n  - 'downwardAPI'\n  # Assume that persistentVolumes set up by the cluster admin are safe to use.\n  - 'persistentVolumeClaim'\n  hostNetwork: true\n  hostPorts:\n  - min: 0\n    max: 65535\n  hostIPC: false\n  hostPID: false\n  runAsUser:\n    rule: 'MustRunAsNonRoot'\n  seLinux:\n    rule: 'RunAsAny'\n  supplementalGroups:\n    rule: 'MustRunAs'\n    ranges:\n      # Forbid adding the root group.\n      - min: 1\n        max: 65535\n  fsGroup:\n    rule: 'MustRunAs'\n    ranges:\n      # Forbid adding the root group.\n      - min: 1\n        max: 65535\n  readOnlyRootFilesystem: false\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-serviceaccount-tigera-operator.yaml"}, "region": {"startLine": 3, "endLine": 7, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: tigera-operator\n  namespace: default\n"}}}}]}, {"ruleId": "CKV_K8S_158", "ruleIndex": 22, "level": "error", "attachments": [], "message": {"text": "Minimize Roles and ClusterRoles that grant permissions to escalate Roles or ClusterRoles"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-role-tigera-operator.yaml"}, "region": {"startLine": 4, "endLine": 211, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: tigera-operator\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - namespaces\n      - pods\n      - podtemplates\n      - services\n      - endpoints\n      - events\n      - configmaps\n      - secrets\n      - serviceaccounts\n    verbs:\n      - create\n      - get\n      - list\n      - update\n      - delete\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - resourcequotas\n    verbs:\n      - list\n      - get\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - resourcequotas\n    verbs:\n      - create\n      - get\n      - list\n      - update\n      - delete\n      - watch\n    resourceNames:\n      - calico-critical-pods\n      - tigera-critical-pods\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\n    verbs:\n      # Need to update node labels when migrating nodes.\n      - get\n      - patch\n      - list\n      # We need this for Typha autoscaling\n      - watch\n  - apiGroups:\n      - rbac.authorization.k8s.io\n    resources:\n      - clusterroles\n      - clusterrolebindings\n      - rolebindings\n      - roles\n    verbs:\n      - create\n      - get\n      - list\n      - update\n      - delete\n      - watch\n      - bind\n      - escalate\n  - apiGroups:\n      - apps\n    resources:\n      - deployments\n      - daemonsets\n      - statefulsets\n    verbs:\n      - create\n      - get\n      - list\n      - patch\n      - update\n      - delete\n      - watch\n  - apiGroups:\n      - apps\n    resourceNames:\n      - tigera-operator\n    resources:\n      - deployments/finalizers\n    verbs:\n      - update\n  - apiGroups:\n      - operator.tigera.io\n    resources:\n      - '*'\n    verbs:\n      - create\n      - get\n      - list\n      - update\n      - patch\n      - delete\n      - watch\n  - apiGroups:\n    - networking.k8s.io\n    resources:\n    - networkpolicies\n    verbs:\n      - create\n      - update\n      - delete\n      - get\n      - list\n      - watch\n  - apiGroups:\n    - crd.projectcalico.org\n    resources:\n    - felixconfigurations\n    verbs:\n    - create\n    - patch\n    - list\n    - get\n    - watch\n  - apiGroups:\n    - crd.projectcalico.org\n    resources:\n    - ippools\n    - kubecontrollersconfigurations\n    - bgpconfigurations\n    verbs:\n    - get\n    - list\n    - watch\n  - apiGroups:\n      - scheduling.k8s.io\n    resources:\n      - priorityclasses\n    verbs:\n      - create\n      - get\n      - list\n      - update\n      - delete\n      - watch\n  - apiGroups:\n      - policy\n    resources:\n      - poddisruptionbudgets\n    verbs:\n      - create\n      - get\n      - list\n      - update\n      - delete\n      - watch\n  - apiGroups:\n      - apiregistration.k8s.io\n    resources:\n      - apiservices\n    verbs:\n      - list\n      - watch\n      - create \n      - update\n  # Needed for operator lock\n  - apiGroups:\n      - coordination.k8s.io\n    resources:\n      - leases\n    verbs:\n      - create\n      - get\n      - list\n      - update\n      - delete\n      - watch\n  # Add the appropriate pod security policy permissions\n  - apiGroups:\n      - policy\n    resources:\n      - podsecuritypolicies\n    resourceNames:\n      - tigera-operator\n    verbs:\n      - use\n  - apiGroups:\n      - policy\n    resources:\n      - podsecuritypolicies\n    verbs:\n      - get\n      - list\n      - watch\n      - create\n      - update\n      - delete\n# Add the permissions to monitor the status of certificatesigningrequests when certificate management is enabled.\n  - apiGroups:\n      - certificates.k8s.io\n    resources:\n      - certificatesigningrequests\n    verbs:\n      - list\n"}}}}]}, {"ruleId": "CKV_K8S_157", "ruleIndex": 23, "level": "error", "attachments": [], "message": {"text": "Minimize Roles and ClusterRoles that grant permissions to bind RoleBindings or ClusterRoleBindings"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/tigera-operator/templates/tigera-operator/02-role-tigera-operator.yaml"}, "region": {"startLine": 4, "endLine": 211, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: tigera-operator\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - namespaces\n      - pods\n      - podtemplates\n      - services\n      - endpoints\n      - events\n      - configmaps\n      - secrets\n      - serviceaccounts\n    verbs:\n      - create\n      - get\n      - list\n      - update\n      - delete\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - resourcequotas\n    verbs:\n      - list\n      - get\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - resourcequotas\n    verbs:\n      - create\n      - get\n      - list\n      - update\n      - delete\n      - watch\n    resourceNames:\n      - calico-critical-pods\n      - tigera-critical-pods\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\n    verbs:\n      # Need to update node labels when migrating nodes.\n      - get\n      - patch\n      - list\n      # We need this for Typha autoscaling\n      - watch\n  - apiGroups:\n      - rbac.authorization.k8s.io\n    resources:\n      - clusterroles\n      - clusterrolebindings\n      - rolebindings\n      - roles\n    verbs:\n      - create\n      - get\n      - list\n      - update\n      - delete\n      - watch\n      - bind\n      - escalate\n  - apiGroups:\n      - apps\n    resources:\n      - deployments\n      - daemonsets\n      - statefulsets\n    verbs:\n      - create\n      - get\n      - list\n      - patch\n      - update\n      - delete\n      - watch\n  - apiGroups:\n      - apps\n    resourceNames:\n      - tigera-operator\n    resources:\n      - deployments/finalizers\n    verbs:\n      - update\n  - apiGroups:\n      - operator.tigera.io\n    resources:\n      - '*'\n    verbs:\n      - create\n      - get\n      - list\n      - update\n      - patch\n      - delete\n      - watch\n  - apiGroups:\n    - networking.k8s.io\n    resources:\n    - networkpolicies\n    verbs:\n      - create\n      - update\n      - delete\n      - get\n      - list\n      - watch\n  - apiGroups:\n    - crd.projectcalico.org\n    resources:\n    - felixconfigurations\n    verbs:\n    - create\n    - patch\n    - list\n    - get\n    - watch\n  - apiGroups:\n    - crd.projectcalico.org\n    resources:\n    - ippools\n    - kubecontrollersconfigurations\n    - bgpconfigurations\n    verbs:\n    - get\n    - list\n    - watch\n  - apiGroups:\n      - scheduling.k8s.io\n    resources:\n      - priorityclasses\n    verbs:\n      - create\n      - get\n      - list\n      - update\n      - delete\n      - watch\n  - apiGroups:\n      - policy\n    resources:\n      - poddisruptionbudgets\n    verbs:\n      - create\n      - get\n      - list\n      - update\n      - delete\n      - watch\n  - apiGroups:\n      - apiregistration.k8s.io\n    resources:\n      - apiservices\n    verbs:\n      - list\n      - watch\n      - create \n      - update\n  # Needed for operator lock\n  - apiGroups:\n      - coordination.k8s.io\n    resources:\n      - leases\n    verbs:\n      - create\n      - get\n      - list\n      - update\n      - delete\n      - watch\n  # Add the appropriate pod security policy permissions\n  - apiGroups:\n      - policy\n    resources:\n      - podsecuritypolicies\n    resourceNames:\n      - tigera-operator\n    verbs:\n      - use\n  - apiGroups:\n      - policy\n    resources:\n      - podsecuritypolicies\n    verbs:\n      - get\n      - list\n      - watch\n      - create\n      - update\n      - delete\n# Add the permissions to monitor the status of certificatesigningrequests when certificate management is enabled.\n  - apiGroups:\n      - certificates.k8s.io\n    resources:\n      - certificatesigningrequests\n    verbs:\n      - list\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wrlinux-ota\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  strategy:\n    type: Recreate\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: wrlinux-ota\n      app.kubernetes.io/instance: wrlinux-ota\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: wrlinux-ota\n        app.kubernetes.io/instance: wrlinux-ota\n    spec:\n      serviceAccountName: default\n      securityContext:\n        fsGroup: 10000\n        fsGroupChangePolicy: Always\n      containers:\n        - name: wrlinux-ota-httpd\n          securityContext:\n            {}\n          image: \"httpd:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          volumeMounts:\n            - mountPath: /usr/local/apache2/htdocs\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n        - name: wrlinux-ota-rsyncd\n          securityContext:\n            runAsGroup: 100\n            runAsNonRoot: true\n            runAsUser: 10000\n          image: \"apnar/rsync-server:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: rsync\n              containerPort: 873\n              protocol: TCP\n          volumeMounts:\n            - mountPath: /data\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n          env:\n          - name: ALLOW\n            value: 10.0.0.0/8 172.16.0.0/16 172.16.0.0/12 127.0.0.1/24\n      volumes:\n      - name: wrlinux-ota-pvc\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wrlinux-ota\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  strategy:\n    type: Recreate\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: wrlinux-ota\n      app.kubernetes.io/instance: wrlinux-ota\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: wrlinux-ota\n        app.kubernetes.io/instance: wrlinux-ota\n    spec:\n      serviceAccountName: default\n      securityContext:\n        fsGroup: 10000\n        fsGroupChangePolicy: Always\n      containers:\n        - name: wrlinux-ota-httpd\n          securityContext:\n            {}\n          image: \"httpd:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          volumeMounts:\n            - mountPath: /usr/local/apache2/htdocs\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n        - name: wrlinux-ota-rsyncd\n          securityContext:\n            runAsGroup: 100\n            runAsNonRoot: true\n            runAsUser: 10000\n          image: \"apnar/rsync-server:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: rsync\n              containerPort: 873\n              protocol: TCP\n          volumeMounts:\n            - mountPath: /data\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n          env:\n          - name: ALLOW\n            value: 10.0.0.0/8 172.16.0.0/16 172.16.0.0/12 127.0.0.1/24\n      volumes:\n      - name: wrlinux-ota-pvc\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wrlinux-ota\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  strategy:\n    type: Recreate\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: wrlinux-ota\n      app.kubernetes.io/instance: wrlinux-ota\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: wrlinux-ota\n        app.kubernetes.io/instance: wrlinux-ota\n    spec:\n      serviceAccountName: default\n      securityContext:\n        fsGroup: 10000\n        fsGroupChangePolicy: Always\n      containers:\n        - name: wrlinux-ota-httpd\n          securityContext:\n            {}\n          image: \"httpd:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          volumeMounts:\n            - mountPath: /usr/local/apache2/htdocs\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n        - name: wrlinux-ota-rsyncd\n          securityContext:\n            runAsGroup: 100\n            runAsNonRoot: true\n            runAsUser: 10000\n          image: \"apnar/rsync-server:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: rsync\n              containerPort: 873\n              protocol: TCP\n          volumeMounts:\n            - mountPath: /data\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n          env:\n          - name: ALLOW\n            value: 10.0.0.0/8 172.16.0.0/16 172.16.0.0/12 127.0.0.1/24\n      volumes:\n      - name: wrlinux-ota-pvc\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wrlinux-ota\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  strategy:\n    type: Recreate\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: wrlinux-ota\n      app.kubernetes.io/instance: wrlinux-ota\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: wrlinux-ota\n        app.kubernetes.io/instance: wrlinux-ota\n    spec:\n      serviceAccountName: default\n      securityContext:\n        fsGroup: 10000\n        fsGroupChangePolicy: Always\n      containers:\n        - name: wrlinux-ota-httpd\n          securityContext:\n            {}\n          image: \"httpd:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          volumeMounts:\n            - mountPath: /usr/local/apache2/htdocs\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n        - name: wrlinux-ota-rsyncd\n          securityContext:\n            runAsGroup: 100\n            runAsNonRoot: true\n            runAsUser: 10000\n          image: \"apnar/rsync-server:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: rsync\n              containerPort: 873\n              protocol: TCP\n          volumeMounts:\n            - mountPath: /data\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n          env:\n          - name: ALLOW\n            value: 10.0.0.0/8 172.16.0.0/16 172.16.0.0/12 127.0.0.1/24\n      volumes:\n      - name: wrlinux-ota-pvc\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wrlinux-ota\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  strategy:\n    type: Recreate\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: wrlinux-ota\n      app.kubernetes.io/instance: wrlinux-ota\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: wrlinux-ota\n        app.kubernetes.io/instance: wrlinux-ota\n    spec:\n      serviceAccountName: default\n      securityContext:\n        fsGroup: 10000\n        fsGroupChangePolicy: Always\n      containers:\n        - name: wrlinux-ota-httpd\n          securityContext:\n            {}\n          image: \"httpd:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          volumeMounts:\n            - mountPath: /usr/local/apache2/htdocs\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n        - name: wrlinux-ota-rsyncd\n          securityContext:\n            runAsGroup: 100\n            runAsNonRoot: true\n            runAsUser: 10000\n          image: \"apnar/rsync-server:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: rsync\n              containerPort: 873\n              protocol: TCP\n          volumeMounts:\n            - mountPath: /data\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n          env:\n          - name: ALLOW\n            value: 10.0.0.0/8 172.16.0.0/16 172.16.0.0/12 127.0.0.1/24\n      volumes:\n      - name: wrlinux-ota-pvc\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wrlinux-ota\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  strategy:\n    type: Recreate\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: wrlinux-ota\n      app.kubernetes.io/instance: wrlinux-ota\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: wrlinux-ota\n        app.kubernetes.io/instance: wrlinux-ota\n    spec:\n      serviceAccountName: default\n      securityContext:\n        fsGroup: 10000\n        fsGroupChangePolicy: Always\n      containers:\n        - name: wrlinux-ota-httpd\n          securityContext:\n            {}\n          image: \"httpd:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          volumeMounts:\n            - mountPath: /usr/local/apache2/htdocs\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n        - name: wrlinux-ota-rsyncd\n          securityContext:\n            runAsGroup: 100\n            runAsNonRoot: true\n            runAsUser: 10000\n          image: \"apnar/rsync-server:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: rsync\n              containerPort: 873\n              protocol: TCP\n          volumeMounts:\n            - mountPath: /data\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n          env:\n          - name: ALLOW\n            value: 10.0.0.0/8 172.16.0.0/16 172.16.0.0/12 127.0.0.1/24\n      volumes:\n      - name: wrlinux-ota-pvc\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wrlinux-ota\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  strategy:\n    type: Recreate\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: wrlinux-ota\n      app.kubernetes.io/instance: wrlinux-ota\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: wrlinux-ota\n        app.kubernetes.io/instance: wrlinux-ota\n    spec:\n      serviceAccountName: default\n      securityContext:\n        fsGroup: 10000\n        fsGroupChangePolicy: Always\n      containers:\n        - name: wrlinux-ota-httpd\n          securityContext:\n            {}\n          image: \"httpd:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          volumeMounts:\n            - mountPath: /usr/local/apache2/htdocs\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n        - name: wrlinux-ota-rsyncd\n          securityContext:\n            runAsGroup: 100\n            runAsNonRoot: true\n            runAsUser: 10000\n          image: \"apnar/rsync-server:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: rsync\n              containerPort: 873\n              protocol: TCP\n          volumeMounts:\n            - mountPath: /data\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n          env:\n          - name: ALLOW\n            value: 10.0.0.0/8 172.16.0.0/16 172.16.0.0/12 127.0.0.1/24\n      volumes:\n      - name: wrlinux-ota-pvc\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wrlinux-ota\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  strategy:\n    type: Recreate\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: wrlinux-ota\n      app.kubernetes.io/instance: wrlinux-ota\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: wrlinux-ota\n        app.kubernetes.io/instance: wrlinux-ota\n    spec:\n      serviceAccountName: default\n      securityContext:\n        fsGroup: 10000\n        fsGroupChangePolicy: Always\n      containers:\n        - name: wrlinux-ota-httpd\n          securityContext:\n            {}\n          image: \"httpd:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          volumeMounts:\n            - mountPath: /usr/local/apache2/htdocs\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n        - name: wrlinux-ota-rsyncd\n          securityContext:\n            runAsGroup: 100\n            runAsNonRoot: true\n            runAsUser: 10000\n          image: \"apnar/rsync-server:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: rsync\n              containerPort: 873\n              protocol: TCP\n          volumeMounts:\n            - mountPath: /data\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n          env:\n          - name: ALLOW\n            value: 10.0.0.0/8 172.16.0.0/16 172.16.0.0/12 127.0.0.1/24\n      volumes:\n      - name: wrlinux-ota-pvc\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wrlinux-ota\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  strategy:\n    type: Recreate\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: wrlinux-ota\n      app.kubernetes.io/instance: wrlinux-ota\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: wrlinux-ota\n        app.kubernetes.io/instance: wrlinux-ota\n    spec:\n      serviceAccountName: default\n      securityContext:\n        fsGroup: 10000\n        fsGroupChangePolicy: Always\n      containers:\n        - name: wrlinux-ota-httpd\n          securityContext:\n            {}\n          image: \"httpd:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          volumeMounts:\n            - mountPath: /usr/local/apache2/htdocs\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n        - name: wrlinux-ota-rsyncd\n          securityContext:\n            runAsGroup: 100\n            runAsNonRoot: true\n            runAsUser: 10000\n          image: \"apnar/rsync-server:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: rsync\n              containerPort: 873\n              protocol: TCP\n          volumeMounts:\n            - mountPath: /data\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n          env:\n          - name: ALLOW\n            value: 10.0.0.0/8 172.16.0.0/16 172.16.0.0/12 127.0.0.1/24\n      volumes:\n      - name: wrlinux-ota-pvc\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wrlinux-ota\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  strategy:\n    type: Recreate\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: wrlinux-ota\n      app.kubernetes.io/instance: wrlinux-ota\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: wrlinux-ota\n        app.kubernetes.io/instance: wrlinux-ota\n    spec:\n      serviceAccountName: default\n      securityContext:\n        fsGroup: 10000\n        fsGroupChangePolicy: Always\n      containers:\n        - name: wrlinux-ota-httpd\n          securityContext:\n            {}\n          image: \"httpd:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          volumeMounts:\n            - mountPath: /usr/local/apache2/htdocs\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n        - name: wrlinux-ota-rsyncd\n          securityContext:\n            runAsGroup: 100\n            runAsNonRoot: true\n            runAsUser: 10000\n          image: \"apnar/rsync-server:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: rsync\n              containerPort: 873\n              protocol: TCP\n          volumeMounts:\n            - mountPath: /data\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n          env:\n          - name: ALLOW\n            value: 10.0.0.0/8 172.16.0.0/16 172.16.0.0/12 127.0.0.1/24\n      volumes:\n      - name: wrlinux-ota-pvc\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wrlinux-ota\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  strategy:\n    type: Recreate\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: wrlinux-ota\n      app.kubernetes.io/instance: wrlinux-ota\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: wrlinux-ota\n        app.kubernetes.io/instance: wrlinux-ota\n    spec:\n      serviceAccountName: default\n      securityContext:\n        fsGroup: 10000\n        fsGroupChangePolicy: Always\n      containers:\n        - name: wrlinux-ota-httpd\n          securityContext:\n            {}\n          image: \"httpd:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          volumeMounts:\n            - mountPath: /usr/local/apache2/htdocs\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n        - name: wrlinux-ota-rsyncd\n          securityContext:\n            runAsGroup: 100\n            runAsNonRoot: true\n            runAsUser: 10000\n          image: \"apnar/rsync-server:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: rsync\n              containerPort: 873\n              protocol: TCP\n          volumeMounts:\n            - mountPath: /data\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n          env:\n          - name: ALLOW\n            value: 10.0.0.0/8 172.16.0.0/16 172.16.0.0/12 127.0.0.1/24\n      volumes:\n      - name: wrlinux-ota-pvc\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wrlinux-ota\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  strategy:\n    type: Recreate\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: wrlinux-ota\n      app.kubernetes.io/instance: wrlinux-ota\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: wrlinux-ota\n        app.kubernetes.io/instance: wrlinux-ota\n    spec:\n      serviceAccountName: default\n      securityContext:\n        fsGroup: 10000\n        fsGroupChangePolicy: Always\n      containers:\n        - name: wrlinux-ota-httpd\n          securityContext:\n            {}\n          image: \"httpd:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          volumeMounts:\n            - mountPath: /usr/local/apache2/htdocs\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n        - name: wrlinux-ota-rsyncd\n          securityContext:\n            runAsGroup: 100\n            runAsNonRoot: true\n            runAsUser: 10000\n          image: \"apnar/rsync-server:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: rsync\n              containerPort: 873\n              protocol: TCP\n          volumeMounts:\n            - mountPath: /data\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n          env:\n          - name: ALLOW\n            value: 10.0.0.0/8 172.16.0.0/16 172.16.0.0/12 127.0.0.1/24\n      volumes:\n      - name: wrlinux-ota-pvc\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wrlinux-ota\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  strategy:\n    type: Recreate\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: wrlinux-ota\n      app.kubernetes.io/instance: wrlinux-ota\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: wrlinux-ota\n        app.kubernetes.io/instance: wrlinux-ota\n    spec:\n      serviceAccountName: default\n      securityContext:\n        fsGroup: 10000\n        fsGroupChangePolicy: Always\n      containers:\n        - name: wrlinux-ota-httpd\n          securityContext:\n            {}\n          image: \"httpd:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          volumeMounts:\n            - mountPath: /usr/local/apache2/htdocs\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n        - name: wrlinux-ota-rsyncd\n          securityContext:\n            runAsGroup: 100\n            runAsNonRoot: true\n            runAsUser: 10000\n          image: \"apnar/rsync-server:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: rsync\n              containerPort: 873\n              protocol: TCP\n          volumeMounts:\n            - mountPath: /data\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n          env:\n          - name: ALLOW\n            value: 10.0.0.0/8 172.16.0.0/16 172.16.0.0/12 127.0.0.1/24\n      volumes:\n      - name: wrlinux-ota-pvc\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wrlinux-ota\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  strategy:\n    type: Recreate\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: wrlinux-ota\n      app.kubernetes.io/instance: wrlinux-ota\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: wrlinux-ota\n        app.kubernetes.io/instance: wrlinux-ota\n    spec:\n      serviceAccountName: default\n      securityContext:\n        fsGroup: 10000\n        fsGroupChangePolicy: Always\n      containers:\n        - name: wrlinux-ota-httpd\n          securityContext:\n            {}\n          image: \"httpd:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          volumeMounts:\n            - mountPath: /usr/local/apache2/htdocs\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n        - name: wrlinux-ota-rsyncd\n          securityContext:\n            runAsGroup: 100\n            runAsNonRoot: true\n            runAsUser: 10000\n          image: \"apnar/rsync-server:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: rsync\n              containerPort: 873\n              protocol: TCP\n          volumeMounts:\n            - mountPath: /data\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n          env:\n          - name: ALLOW\n            value: 10.0.0.0/8 172.16.0.0/16 172.16.0.0/12 127.0.0.1/24\n      volumes:\n      - name: wrlinux-ota-pvc\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wrlinux-ota\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  strategy:\n    type: Recreate\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: wrlinux-ota\n      app.kubernetes.io/instance: wrlinux-ota\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: wrlinux-ota\n        app.kubernetes.io/instance: wrlinux-ota\n    spec:\n      serviceAccountName: default\n      securityContext:\n        fsGroup: 10000\n        fsGroupChangePolicy: Always\n      containers:\n        - name: wrlinux-ota-httpd\n          securityContext:\n            {}\n          image: \"httpd:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          volumeMounts:\n            - mountPath: /usr/local/apache2/htdocs\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n        - name: wrlinux-ota-rsyncd\n          securityContext:\n            runAsGroup: 100\n            runAsNonRoot: true\n            runAsUser: 10000\n          image: \"apnar/rsync-server:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: rsync\n              containerPort: 873\n              protocol: TCP\n          volumeMounts:\n            - mountPath: /data\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n          env:\n          - name: ALLOW\n            value: 10.0.0.0/8 172.16.0.0/16 172.16.0.0/12 127.0.0.1/24\n      volumes:\n      - name: wrlinux-ota-pvc\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wrlinux-ota\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  strategy:\n    type: Recreate\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: wrlinux-ota\n      app.kubernetes.io/instance: wrlinux-ota\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: wrlinux-ota\n        app.kubernetes.io/instance: wrlinux-ota\n    spec:\n      serviceAccountName: default\n      securityContext:\n        fsGroup: 10000\n        fsGroupChangePolicy: Always\n      containers:\n        - name: wrlinux-ota-httpd\n          securityContext:\n            {}\n          image: \"httpd:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          volumeMounts:\n            - mountPath: /usr/local/apache2/htdocs\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n        - name: wrlinux-ota-rsyncd\n          securityContext:\n            runAsGroup: 100\n            runAsNonRoot: true\n            runAsUser: 10000\n          image: \"apnar/rsync-server:latest\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: rsync\n              containerPort: 873\n              protocol: TCP\n          volumeMounts:\n            - mountPath: /data\n              name: wrlinux-ota-pvc\n          resources:\n            {}\n          env:\n          - name: ALLOW\n            value: 10.0.0.0/8 172.16.0.0/16 172.16.0.0/12 127.0.0.1/24\n      volumes:\n      - name: wrlinux-ota-pvc\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/service.yaml"}, "region": {"startLine": 3, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: wrlinux-ota\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n    - port: 873\n      targetPort: 8873\n      protocol: TCP\n      name: rsyncd\n  selector:\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"wrlinux-ota-test-connection\"\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['wrlinux-ota:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"wrlinux-ota-test-connection\"\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['wrlinux-ota:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"wrlinux-ota-test-connection\"\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['wrlinux-ota:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"wrlinux-ota-test-connection\"\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['wrlinux-ota:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"wrlinux-ota-test-connection\"\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['wrlinux-ota:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"wrlinux-ota-test-connection\"\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['wrlinux-ota:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"wrlinux-ota-test-connection\"\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['wrlinux-ota:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"wrlinux-ota-test-connection\"\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['wrlinux-ota:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"wrlinux-ota-test-connection\"\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['wrlinux-ota:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"wrlinux-ota-test-connection\"\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['wrlinux-ota:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"wrlinux-ota-test-connection\"\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['wrlinux-ota:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"wrlinux-ota-test-connection\"\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['wrlinux-ota:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"wrlinux-ota-test-connection\"\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['wrlinux-ota:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"wrlinux-ota-test-connection\"\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['wrlinux-ota:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"wrlinux-ota-test-connection\"\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['wrlinux-ota:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"wrlinux-ota-test-connection\"\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['wrlinux-ota:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"wrlinux-ota-test-connection\"\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['wrlinux-ota:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"wrlinux-ota-test-connection\"\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['wrlinux-ota:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/wrlinux-ota/templates/tests/test-connection.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"wrlinux-ota-test-connection\"\n  labels:\n    helm.sh/chart: wrlinux-ota-0.1.1\n    app.kubernetes.io/name: wrlinux-ota\n    app.kubernetes.io/instance: wrlinux-ota\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['wrlinux-ota:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: prometheus\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: prometheus\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: prometheus-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: prometheus\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: prometheus\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: prometheus-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: prometheus\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: prometheus\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: prometheus-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: prometheus\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: prometheus\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: prometheus-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: prometheus\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: prometheus\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: prometheus-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: prometheus\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: prometheus\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: prometheus-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: prometheus\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: prometheus\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: prometheus-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: prometheus\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: prometheus\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: prometheus-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: prometheus\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: prometheus\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: prometheus-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: prometheus\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: prometheus\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: prometheus-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: prometheus\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: prometheus\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: prometheus-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: prometheus\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: prometheus\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: prometheus-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: prometheus\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: prometheus\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: prometheus-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/charts/kube-state-metrics/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 61, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: prometheus\n    app.kubernetes.io/version: \"2.2.4\"\nspec:\n  selector:\n    matchLabels:      \n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:        \n        helm.sh/chart: kube-state-metrics-4.0.2\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: prometheus\n        app.kubernetes.io/version: \"2.2.4\"\n    spec:\n      hostNetwork: false\n      serviceAccountName: prometheus-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsUser: 65534\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        - --telemetry-port=8081\n        imagePullPolicy: IfNotPresent\n        image: \"k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.2.4\"\n        ports:\n        - containerPort: 8080\n          name: \"http\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/charts/kube-state-metrics/templates/service.yaml"}, "region": {"startLine": 3, "endLine": 28, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: prometheus-kube-state-metrics\n  namespace: default\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: prometheus\n    app.kubernetes.io/version: \"2.2.4\"\n  annotations:\n    prometheus.io/scrape: 'true'\nspec:\n  type: \"ClusterIP\"\n  ports:\n  - name: \"http\"\n    protocol: TCP\n    port: 8080\n    targetPort: 8080\n  \n  selector:    \n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: prometheus\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/charts/kube-state-metrics/templates/serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 17, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:    \n    helm.sh/chart: kube-state-metrics-4.0.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: prometheus\n    app.kubernetes.io/version: \"2.2.4\"\n  name: prometheus-kube-state-metrics\n  namespace: default\nimagePullSecrets:\n  []\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/pushgateway/deploy.yaml"}, "region": {"startLine": 3, "endLine": 54, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"pushgateway\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-pushgateway\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"pushgateway\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"pushgateway\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-pushgateway\n      containers:\n        - name: prometheus-pushgateway\n          image: \"prom/pushgateway:v1.4.2\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n          ports:\n            - containerPort: 9091\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          resources:\n            {}\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/pushgateway/deploy.yaml"}, "region": {"startLine": 3, "endLine": 54, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"pushgateway\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-pushgateway\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"pushgateway\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"pushgateway\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-pushgateway\n      containers:\n        - name: prometheus-pushgateway\n          image: \"prom/pushgateway:v1.4.2\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n          ports:\n            - containerPort: 9091\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          resources:\n            {}\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/pushgateway/deploy.yaml"}, "region": {"startLine": 3, "endLine": 54, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"pushgateway\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-pushgateway\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"pushgateway\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"pushgateway\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-pushgateway\n      containers:\n        - name: prometheus-pushgateway\n          image: \"prom/pushgateway:v1.4.2\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n          ports:\n            - containerPort: 9091\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          resources:\n            {}\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/pushgateway/deploy.yaml"}, "region": {"startLine": 3, "endLine": 54, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"pushgateway\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-pushgateway\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"pushgateway\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"pushgateway\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-pushgateway\n      containers:\n        - name: prometheus-pushgateway\n          image: \"prom/pushgateway:v1.4.2\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n          ports:\n            - containerPort: 9091\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          resources:\n            {}\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/pushgateway/deploy.yaml"}, "region": {"startLine": 3, "endLine": 54, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"pushgateway\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-pushgateway\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"pushgateway\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"pushgateway\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-pushgateway\n      containers:\n        - name: prometheus-pushgateway\n          image: \"prom/pushgateway:v1.4.2\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n          ports:\n            - containerPort: 9091\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          resources:\n            {}\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/pushgateway/deploy.yaml"}, "region": {"startLine": 3, "endLine": 54, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"pushgateway\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-pushgateway\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"pushgateway\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"pushgateway\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-pushgateway\n      containers:\n        - name: prometheus-pushgateway\n          image: \"prom/pushgateway:v1.4.2\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n          ports:\n            - containerPort: 9091\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          resources:\n            {}\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/pushgateway/deploy.yaml"}, "region": {"startLine": 3, "endLine": 54, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"pushgateway\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-pushgateway\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"pushgateway\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"pushgateway\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-pushgateway\n      containers:\n        - name: prometheus-pushgateway\n          image: \"prom/pushgateway:v1.4.2\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n          ports:\n            - containerPort: 9091\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          resources:\n            {}\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/pushgateway/deploy.yaml"}, "region": {"startLine": 3, "endLine": 54, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"pushgateway\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-pushgateway\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"pushgateway\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"pushgateway\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-pushgateway\n      containers:\n        - name: prometheus-pushgateway\n          image: \"prom/pushgateway:v1.4.2\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n          ports:\n            - containerPort: 9091\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          resources:\n            {}\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/pushgateway/deploy.yaml"}, "region": {"startLine": 3, "endLine": 54, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"pushgateway\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-pushgateway\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"pushgateway\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"pushgateway\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-pushgateway\n      containers:\n        - name: prometheus-pushgateway\n          image: \"prom/pushgateway:v1.4.2\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n          ports:\n            - containerPort: 9091\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          resources:\n            {}\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/pushgateway/deploy.yaml"}, "region": {"startLine": 3, "endLine": 54, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"pushgateway\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-pushgateway\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"pushgateway\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"pushgateway\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-pushgateway\n      containers:\n        - name: prometheus-pushgateway\n          image: \"prom/pushgateway:v1.4.2\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n          ports:\n            - containerPort: 9091\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          resources:\n            {}\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/pushgateway/deploy.yaml"}, "region": {"startLine": 3, "endLine": 54, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"pushgateway\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-pushgateway\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"pushgateway\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"pushgateway\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-pushgateway\n      containers:\n        - name: prometheus-pushgateway\n          image: \"prom/pushgateway:v1.4.2\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n          ports:\n            - containerPort: 9091\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          resources:\n            {}\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/pushgateway/deploy.yaml"}, "region": {"startLine": 3, "endLine": 54, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"pushgateway\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-pushgateway\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"pushgateway\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"pushgateway\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-pushgateway\n      containers:\n        - name: prometheus-pushgateway\n          image: \"prom/pushgateway:v1.4.2\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n          ports:\n            - containerPort: 9091\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9091\n            initialDelaySeconds: 10\n            timeoutSeconds: 10\n          resources:\n            {}\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/pushgateway/service.yaml"}, "region": {"startLine": 3, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    prometheus.io/probe: pushgateway\n  labels:\n    component: \"pushgateway\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-pushgateway\n  namespace: default\nspec:\n  ports:\n    - name: http\n      port: 9091\n      protocol: TCP\n      targetPort: 9091\n  selector:\n    component: \"pushgateway\"\n    app: prometheus\n    release: prometheus\n  type: \"ClusterIP\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/pushgateway/serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 15, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    component: \"pushgateway\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-pushgateway\n  namespace: default\n  annotations:\n    {}\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/alertmanager/deploy.yaml"}, "region": {"startLine": 3, "endLine": 86, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"alertmanager\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-alertmanager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"alertmanager\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"alertmanager\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-alertmanager\n      containers:\n        - name: prometheus-alertmanager\n          image: \"quay.io/prometheus/alertmanager:v0.23.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          env:\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: status.podIP\n          args:\n            - --config.file=/etc/config/alertmanager.yml\n            - --storage.path=/data\n            - --cluster.listen-address=\n            - --web.external-url=http://localhost:9093\n\n          ports:\n            - containerPort: 9093\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9093\n            initialDelaySeconds: 30\n            timeoutSeconds: 30\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: \"/data\"\n              subPath: \"\"\n        - name: prometheus-alertmanager-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9093/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-alertmanager\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-alertmanager\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/alertmanager/deploy.yaml"}, "region": {"startLine": 3, "endLine": 86, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"alertmanager\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-alertmanager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"alertmanager\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"alertmanager\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-alertmanager\n      containers:\n        - name: prometheus-alertmanager\n          image: \"quay.io/prometheus/alertmanager:v0.23.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          env:\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: status.podIP\n          args:\n            - --config.file=/etc/config/alertmanager.yml\n            - --storage.path=/data\n            - --cluster.listen-address=\n            - --web.external-url=http://localhost:9093\n\n          ports:\n            - containerPort: 9093\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9093\n            initialDelaySeconds: 30\n            timeoutSeconds: 30\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: \"/data\"\n              subPath: \"\"\n        - name: prometheus-alertmanager-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9093/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-alertmanager\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-alertmanager\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/alertmanager/deploy.yaml"}, "region": {"startLine": 3, "endLine": 86, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"alertmanager\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-alertmanager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"alertmanager\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"alertmanager\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-alertmanager\n      containers:\n        - name: prometheus-alertmanager\n          image: \"quay.io/prometheus/alertmanager:v0.23.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          env:\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: status.podIP\n          args:\n            - --config.file=/etc/config/alertmanager.yml\n            - --storage.path=/data\n            - --cluster.listen-address=\n            - --web.external-url=http://localhost:9093\n\n          ports:\n            - containerPort: 9093\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9093\n            initialDelaySeconds: 30\n            timeoutSeconds: 30\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: \"/data\"\n              subPath: \"\"\n        - name: prometheus-alertmanager-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9093/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-alertmanager\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-alertmanager\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/alertmanager/deploy.yaml"}, "region": {"startLine": 3, "endLine": 86, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"alertmanager\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-alertmanager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"alertmanager\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"alertmanager\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-alertmanager\n      containers:\n        - name: prometheus-alertmanager\n          image: \"quay.io/prometheus/alertmanager:v0.23.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          env:\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: status.podIP\n          args:\n            - --config.file=/etc/config/alertmanager.yml\n            - --storage.path=/data\n            - --cluster.listen-address=\n            - --web.external-url=http://localhost:9093\n\n          ports:\n            - containerPort: 9093\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9093\n            initialDelaySeconds: 30\n            timeoutSeconds: 30\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: \"/data\"\n              subPath: \"\"\n        - name: prometheus-alertmanager-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9093/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-alertmanager\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-alertmanager\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/alertmanager/deploy.yaml"}, "region": {"startLine": 3, "endLine": 86, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"alertmanager\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-alertmanager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"alertmanager\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"alertmanager\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-alertmanager\n      containers:\n        - name: prometheus-alertmanager\n          image: \"quay.io/prometheus/alertmanager:v0.23.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          env:\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: status.podIP\n          args:\n            - --config.file=/etc/config/alertmanager.yml\n            - --storage.path=/data\n            - --cluster.listen-address=\n            - --web.external-url=http://localhost:9093\n\n          ports:\n            - containerPort: 9093\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9093\n            initialDelaySeconds: 30\n            timeoutSeconds: 30\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: \"/data\"\n              subPath: \"\"\n        - name: prometheus-alertmanager-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9093/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-alertmanager\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-alertmanager\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/alertmanager/deploy.yaml"}, "region": {"startLine": 3, "endLine": 86, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"alertmanager\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-alertmanager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"alertmanager\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"alertmanager\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-alertmanager\n      containers:\n        - name: prometheus-alertmanager\n          image: \"quay.io/prometheus/alertmanager:v0.23.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          env:\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: status.podIP\n          args:\n            - --config.file=/etc/config/alertmanager.yml\n            - --storage.path=/data\n            - --cluster.listen-address=\n            - --web.external-url=http://localhost:9093\n\n          ports:\n            - containerPort: 9093\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9093\n            initialDelaySeconds: 30\n            timeoutSeconds: 30\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: \"/data\"\n              subPath: \"\"\n        - name: prometheus-alertmanager-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9093/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-alertmanager\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-alertmanager\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/alertmanager/deploy.yaml"}, "region": {"startLine": 3, "endLine": 86, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"alertmanager\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-alertmanager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"alertmanager\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"alertmanager\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-alertmanager\n      containers:\n        - name: prometheus-alertmanager\n          image: \"quay.io/prometheus/alertmanager:v0.23.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          env:\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: status.podIP\n          args:\n            - --config.file=/etc/config/alertmanager.yml\n            - --storage.path=/data\n            - --cluster.listen-address=\n            - --web.external-url=http://localhost:9093\n\n          ports:\n            - containerPort: 9093\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9093\n            initialDelaySeconds: 30\n            timeoutSeconds: 30\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: \"/data\"\n              subPath: \"\"\n        - name: prometheus-alertmanager-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9093/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-alertmanager\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-alertmanager\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/alertmanager/deploy.yaml"}, "region": {"startLine": 3, "endLine": 86, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"alertmanager\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-alertmanager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"alertmanager\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"alertmanager\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-alertmanager\n      containers:\n        - name: prometheus-alertmanager\n          image: \"quay.io/prometheus/alertmanager:v0.23.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          env:\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: status.podIP\n          args:\n            - --config.file=/etc/config/alertmanager.yml\n            - --storage.path=/data\n            - --cluster.listen-address=\n            - --web.external-url=http://localhost:9093\n\n          ports:\n            - containerPort: 9093\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9093\n            initialDelaySeconds: 30\n            timeoutSeconds: 30\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: \"/data\"\n              subPath: \"\"\n        - name: prometheus-alertmanager-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9093/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-alertmanager\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-alertmanager\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/alertmanager/deploy.yaml"}, "region": {"startLine": 3, "endLine": 86, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"alertmanager\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-alertmanager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"alertmanager\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"alertmanager\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-alertmanager\n      containers:\n        - name: prometheus-alertmanager\n          image: \"quay.io/prometheus/alertmanager:v0.23.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          env:\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: status.podIP\n          args:\n            - --config.file=/etc/config/alertmanager.yml\n            - --storage.path=/data\n            - --cluster.listen-address=\n            - --web.external-url=http://localhost:9093\n\n          ports:\n            - containerPort: 9093\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9093\n            initialDelaySeconds: 30\n            timeoutSeconds: 30\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: \"/data\"\n              subPath: \"\"\n        - name: prometheus-alertmanager-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9093/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-alertmanager\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-alertmanager\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/alertmanager/deploy.yaml"}, "region": {"startLine": 3, "endLine": 86, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"alertmanager\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-alertmanager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"alertmanager\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"alertmanager\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-alertmanager\n      containers:\n        - name: prometheus-alertmanager\n          image: \"quay.io/prometheus/alertmanager:v0.23.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          env:\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: status.podIP\n          args:\n            - --config.file=/etc/config/alertmanager.yml\n            - --storage.path=/data\n            - --cluster.listen-address=\n            - --web.external-url=http://localhost:9093\n\n          ports:\n            - containerPort: 9093\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9093\n            initialDelaySeconds: 30\n            timeoutSeconds: 30\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: \"/data\"\n              subPath: \"\"\n        - name: prometheus-alertmanager-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9093/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-alertmanager\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-alertmanager\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/alertmanager/deploy.yaml"}, "region": {"startLine": 3, "endLine": 86, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"alertmanager\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-alertmanager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"alertmanager\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"alertmanager\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-alertmanager\n      containers:\n        - name: prometheus-alertmanager\n          image: \"quay.io/prometheus/alertmanager:v0.23.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          env:\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: status.podIP\n          args:\n            - --config.file=/etc/config/alertmanager.yml\n            - --storage.path=/data\n            - --cluster.listen-address=\n            - --web.external-url=http://localhost:9093\n\n          ports:\n            - containerPort: 9093\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9093\n            initialDelaySeconds: 30\n            timeoutSeconds: 30\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: \"/data\"\n              subPath: \"\"\n        - name: prometheus-alertmanager-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9093/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-alertmanager\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-alertmanager\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/alertmanager/deploy.yaml"}, "region": {"startLine": 3, "endLine": 86, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"alertmanager\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-alertmanager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"alertmanager\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"alertmanager\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-alertmanager\n      containers:\n        - name: prometheus-alertmanager\n          image: \"quay.io/prometheus/alertmanager:v0.23.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          env:\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: status.podIP\n          args:\n            - --config.file=/etc/config/alertmanager.yml\n            - --storage.path=/data\n            - --cluster.listen-address=\n            - --web.external-url=http://localhost:9093\n\n          ports:\n            - containerPort: 9093\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9093\n            initialDelaySeconds: 30\n            timeoutSeconds: 30\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: \"/data\"\n              subPath: \"\"\n        - name: prometheus-alertmanager-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9093/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-alertmanager\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-alertmanager\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/alertmanager/deploy.yaml"}, "region": {"startLine": 3, "endLine": 86, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"alertmanager\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-alertmanager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"alertmanager\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"alertmanager\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-alertmanager\n      containers:\n        - name: prometheus-alertmanager\n          image: \"quay.io/prometheus/alertmanager:v0.23.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          env:\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: status.podIP\n          args:\n            - --config.file=/etc/config/alertmanager.yml\n            - --storage.path=/data\n            - --cluster.listen-address=\n            - --web.external-url=http://localhost:9093\n\n          ports:\n            - containerPort: 9093\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9093\n            initialDelaySeconds: 30\n            timeoutSeconds: 30\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: \"/data\"\n              subPath: \"\"\n        - name: prometheus-alertmanager-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9093/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-alertmanager\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-alertmanager\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/alertmanager/deploy.yaml"}, "region": {"startLine": 3, "endLine": 86, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"alertmanager\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-alertmanager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"alertmanager\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"alertmanager\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-alertmanager\n      containers:\n        - name: prometheus-alertmanager\n          image: \"quay.io/prometheus/alertmanager:v0.23.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          env:\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: status.podIP\n          args:\n            - --config.file=/etc/config/alertmanager.yml\n            - --storage.path=/data\n            - --cluster.listen-address=\n            - --web.external-url=http://localhost:9093\n\n          ports:\n            - containerPort: 9093\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9093\n            initialDelaySeconds: 30\n            timeoutSeconds: 30\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: \"/data\"\n              subPath: \"\"\n        - name: prometheus-alertmanager-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9093/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-alertmanager\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-alertmanager\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/alertmanager/service.yaml"}, "region": {"startLine": 3, "endLine": 25, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    component: \"alertmanager\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-alertmanager\n  namespace: default\nspec:\n  ports:\n    - name: http\n      port: 80\n      protocol: TCP\n      targetPort: 9093\n  selector:\n    component: \"alertmanager\"\n    app: prometheus\n    release: prometheus\n  sessionAffinity: None\n  type: \"ClusterIP\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/alertmanager/cm.yaml"}, "region": {"startLine": 3, "endLine": 23, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  labels:\n    component: \"alertmanager\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-alertmanager\n  namespace: default\ndata:\n  alertmanager.yml: |\n    global: {}\n    receivers:\n    - name: default-receiver\n    route:\n      group_interval: 5m\n      group_wait: 10s\n      receiver: default-receiver\n      repeat_interval: 3h\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/alertmanager/serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 15, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    component: \"alertmanager\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-alertmanager\n  namespace: default\n  annotations:\n    {}\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/server/deploy.yaml"}, "region": {"startLine": 3, "endLine": 100, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"server\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"server\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"server\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: prometheus-server\n      containers:\n        - name: prometheus-server-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9090/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n\n        - name: prometheus-server\n          image: \"quay.io/prometheus/prometheus:v2.31.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --storage.tsdb.retention.time=15d\n            - --config.file=/etc/config/prometheus.yml\n            - --storage.tsdb.path=/data\n            - --web.console.libraries=/etc/prometheus/console_libraries\n            - --web.console.templates=/etc/prometheus/consoles\n            - --web.enable-lifecycle\n          ports:\n            - containerPort: 9090\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 5\n            timeoutSeconds: 4\n            failureThreshold: 3\n            successThreshold: 1\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 15\n            timeoutSeconds: 10\n            failureThreshold: 3\n            successThreshold: 1\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: /data\n              subPath: \"\"\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-server\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-server\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/server/deploy.yaml"}, "region": {"startLine": 3, "endLine": 100, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"server\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"server\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"server\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: prometheus-server\n      containers:\n        - name: prometheus-server-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9090/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n\n        - name: prometheus-server\n          image: \"quay.io/prometheus/prometheus:v2.31.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --storage.tsdb.retention.time=15d\n            - --config.file=/etc/config/prometheus.yml\n            - --storage.tsdb.path=/data\n            - --web.console.libraries=/etc/prometheus/console_libraries\n            - --web.console.templates=/etc/prometheus/consoles\n            - --web.enable-lifecycle\n          ports:\n            - containerPort: 9090\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 5\n            timeoutSeconds: 4\n            failureThreshold: 3\n            successThreshold: 1\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 15\n            timeoutSeconds: 10\n            failureThreshold: 3\n            successThreshold: 1\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: /data\n              subPath: \"\"\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-server\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-server\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/server/deploy.yaml"}, "region": {"startLine": 3, "endLine": 100, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"server\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"server\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"server\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: prometheus-server\n      containers:\n        - name: prometheus-server-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9090/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n\n        - name: prometheus-server\n          image: \"quay.io/prometheus/prometheus:v2.31.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --storage.tsdb.retention.time=15d\n            - --config.file=/etc/config/prometheus.yml\n            - --storage.tsdb.path=/data\n            - --web.console.libraries=/etc/prometheus/console_libraries\n            - --web.console.templates=/etc/prometheus/consoles\n            - --web.enable-lifecycle\n          ports:\n            - containerPort: 9090\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 5\n            timeoutSeconds: 4\n            failureThreshold: 3\n            successThreshold: 1\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 15\n            timeoutSeconds: 10\n            failureThreshold: 3\n            successThreshold: 1\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: /data\n              subPath: \"\"\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-server\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-server\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/server/deploy.yaml"}, "region": {"startLine": 3, "endLine": 100, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"server\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"server\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"server\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: prometheus-server\n      containers:\n        - name: prometheus-server-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9090/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n\n        - name: prometheus-server\n          image: \"quay.io/prometheus/prometheus:v2.31.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --storage.tsdb.retention.time=15d\n            - --config.file=/etc/config/prometheus.yml\n            - --storage.tsdb.path=/data\n            - --web.console.libraries=/etc/prometheus/console_libraries\n            - --web.console.templates=/etc/prometheus/consoles\n            - --web.enable-lifecycle\n          ports:\n            - containerPort: 9090\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 5\n            timeoutSeconds: 4\n            failureThreshold: 3\n            successThreshold: 1\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 15\n            timeoutSeconds: 10\n            failureThreshold: 3\n            successThreshold: 1\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: /data\n              subPath: \"\"\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-server\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-server\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/server/deploy.yaml"}, "region": {"startLine": 3, "endLine": 100, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"server\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"server\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"server\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: prometheus-server\n      containers:\n        - name: prometheus-server-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9090/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n\n        - name: prometheus-server\n          image: \"quay.io/prometheus/prometheus:v2.31.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --storage.tsdb.retention.time=15d\n            - --config.file=/etc/config/prometheus.yml\n            - --storage.tsdb.path=/data\n            - --web.console.libraries=/etc/prometheus/console_libraries\n            - --web.console.templates=/etc/prometheus/consoles\n            - --web.enable-lifecycle\n          ports:\n            - containerPort: 9090\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 5\n            timeoutSeconds: 4\n            failureThreshold: 3\n            successThreshold: 1\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 15\n            timeoutSeconds: 10\n            failureThreshold: 3\n            successThreshold: 1\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: /data\n              subPath: \"\"\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-server\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-server\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/server/deploy.yaml"}, "region": {"startLine": 3, "endLine": 100, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"server\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"server\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"server\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: prometheus-server\n      containers:\n        - name: prometheus-server-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9090/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n\n        - name: prometheus-server\n          image: \"quay.io/prometheus/prometheus:v2.31.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --storage.tsdb.retention.time=15d\n            - --config.file=/etc/config/prometheus.yml\n            - --storage.tsdb.path=/data\n            - --web.console.libraries=/etc/prometheus/console_libraries\n            - --web.console.templates=/etc/prometheus/consoles\n            - --web.enable-lifecycle\n          ports:\n            - containerPort: 9090\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 5\n            timeoutSeconds: 4\n            failureThreshold: 3\n            successThreshold: 1\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 15\n            timeoutSeconds: 10\n            failureThreshold: 3\n            successThreshold: 1\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: /data\n              subPath: \"\"\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-server\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-server\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/server/deploy.yaml"}, "region": {"startLine": 3, "endLine": 100, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"server\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"server\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"server\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: prometheus-server\n      containers:\n        - name: prometheus-server-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9090/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n\n        - name: prometheus-server\n          image: \"quay.io/prometheus/prometheus:v2.31.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --storage.tsdb.retention.time=15d\n            - --config.file=/etc/config/prometheus.yml\n            - --storage.tsdb.path=/data\n            - --web.console.libraries=/etc/prometheus/console_libraries\n            - --web.console.templates=/etc/prometheus/consoles\n            - --web.enable-lifecycle\n          ports:\n            - containerPort: 9090\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 5\n            timeoutSeconds: 4\n            failureThreshold: 3\n            successThreshold: 1\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 15\n            timeoutSeconds: 10\n            failureThreshold: 3\n            successThreshold: 1\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: /data\n              subPath: \"\"\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-server\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-server\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/server/deploy.yaml"}, "region": {"startLine": 3, "endLine": 100, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"server\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"server\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"server\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: prometheus-server\n      containers:\n        - name: prometheus-server-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9090/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n\n        - name: prometheus-server\n          image: \"quay.io/prometheus/prometheus:v2.31.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --storage.tsdb.retention.time=15d\n            - --config.file=/etc/config/prometheus.yml\n            - --storage.tsdb.path=/data\n            - --web.console.libraries=/etc/prometheus/console_libraries\n            - --web.console.templates=/etc/prometheus/consoles\n            - --web.enable-lifecycle\n          ports:\n            - containerPort: 9090\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 5\n            timeoutSeconds: 4\n            failureThreshold: 3\n            successThreshold: 1\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 15\n            timeoutSeconds: 10\n            failureThreshold: 3\n            successThreshold: 1\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: /data\n              subPath: \"\"\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-server\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-server\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/server/deploy.yaml"}, "region": {"startLine": 3, "endLine": 100, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"server\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"server\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"server\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: prometheus-server\n      containers:\n        - name: prometheus-server-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9090/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n\n        - name: prometheus-server\n          image: \"quay.io/prometheus/prometheus:v2.31.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --storage.tsdb.retention.time=15d\n            - --config.file=/etc/config/prometheus.yml\n            - --storage.tsdb.path=/data\n            - --web.console.libraries=/etc/prometheus/console_libraries\n            - --web.console.templates=/etc/prometheus/consoles\n            - --web.enable-lifecycle\n          ports:\n            - containerPort: 9090\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 5\n            timeoutSeconds: 4\n            failureThreshold: 3\n            successThreshold: 1\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 15\n            timeoutSeconds: 10\n            failureThreshold: 3\n            successThreshold: 1\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: /data\n              subPath: \"\"\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-server\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-server\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/server/deploy.yaml"}, "region": {"startLine": 3, "endLine": 100, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"server\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"server\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"server\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: prometheus-server\n      containers:\n        - name: prometheus-server-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9090/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n\n        - name: prometheus-server\n          image: \"quay.io/prometheus/prometheus:v2.31.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --storage.tsdb.retention.time=15d\n            - --config.file=/etc/config/prometheus.yml\n            - --storage.tsdb.path=/data\n            - --web.console.libraries=/etc/prometheus/console_libraries\n            - --web.console.templates=/etc/prometheus/consoles\n            - --web.enable-lifecycle\n          ports:\n            - containerPort: 9090\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 5\n            timeoutSeconds: 4\n            failureThreshold: 3\n            successThreshold: 1\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 15\n            timeoutSeconds: 10\n            failureThreshold: 3\n            successThreshold: 1\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: /data\n              subPath: \"\"\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-server\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-server\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/server/deploy.yaml"}, "region": {"startLine": 3, "endLine": 100, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"server\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"server\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"server\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: prometheus-server\n      containers:\n        - name: prometheus-server-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9090/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n\n        - name: prometheus-server\n          image: \"quay.io/prometheus/prometheus:v2.31.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --storage.tsdb.retention.time=15d\n            - --config.file=/etc/config/prometheus.yml\n            - --storage.tsdb.path=/data\n            - --web.console.libraries=/etc/prometheus/console_libraries\n            - --web.console.templates=/etc/prometheus/consoles\n            - --web.enable-lifecycle\n          ports:\n            - containerPort: 9090\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 5\n            timeoutSeconds: 4\n            failureThreshold: 3\n            successThreshold: 1\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 15\n            timeoutSeconds: 10\n            failureThreshold: 3\n            successThreshold: 1\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: /data\n              subPath: \"\"\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-server\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-server\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/server/deploy.yaml"}, "region": {"startLine": 3, "endLine": 100, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"server\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"server\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"server\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: prometheus-server\n      containers:\n        - name: prometheus-server-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9090/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n\n        - name: prometheus-server\n          image: \"quay.io/prometheus/prometheus:v2.31.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --storage.tsdb.retention.time=15d\n            - --config.file=/etc/config/prometheus.yml\n            - --storage.tsdb.path=/data\n            - --web.console.libraries=/etc/prometheus/console_libraries\n            - --web.console.templates=/etc/prometheus/consoles\n            - --web.enable-lifecycle\n          ports:\n            - containerPort: 9090\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 5\n            timeoutSeconds: 4\n            failureThreshold: 3\n            successThreshold: 1\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 15\n            timeoutSeconds: 10\n            failureThreshold: 3\n            successThreshold: 1\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: /data\n              subPath: \"\"\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-server\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-server\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/server/deploy.yaml"}, "region": {"startLine": 3, "endLine": 100, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"server\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"server\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"server\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: prometheus-server\n      containers:\n        - name: prometheus-server-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9090/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n\n        - name: prometheus-server\n          image: \"quay.io/prometheus/prometheus:v2.31.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --storage.tsdb.retention.time=15d\n            - --config.file=/etc/config/prometheus.yml\n            - --storage.tsdb.path=/data\n            - --web.console.libraries=/etc/prometheus/console_libraries\n            - --web.console.templates=/etc/prometheus/consoles\n            - --web.enable-lifecycle\n          ports:\n            - containerPort: 9090\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 5\n            timeoutSeconds: 4\n            failureThreshold: 3\n            successThreshold: 1\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 15\n            timeoutSeconds: 10\n            failureThreshold: 3\n            successThreshold: 1\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: /data\n              subPath: \"\"\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-server\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-server\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/server/deploy.yaml"}, "region": {"startLine": 3, "endLine": 100, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: \"server\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"server\"\n      app: prometheus\n      release: prometheus\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: \"server\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: prometheus-server\n      containers:\n        - name: prometheus-server-configmap-reload\n          image: \"jimmidyson/configmap-reload:v0.5.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://127.0.0.1:9090/-/reload\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n\n        - name: prometheus-server\n          image: \"quay.io/prometheus/prometheus:v2.31.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --storage.tsdb.retention.time=15d\n            - --config.file=/etc/config/prometheus.yml\n            - --storage.tsdb.path=/data\n            - --web.console.libraries=/etc/prometheus/console_libraries\n            - --web.console.templates=/etc/prometheus/consoles\n            - --web.enable-lifecycle\n          ports:\n            - containerPort: 9090\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 5\n            timeoutSeconds: 4\n            failureThreshold: 3\n            successThreshold: 1\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9090\n              scheme: HTTP\n            initialDelaySeconds: 30\n            periodSeconds: 15\n            timeoutSeconds: 10\n            failureThreshold: 3\n            successThreshold: 1\n          resources:\n            {}\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: storage-volume\n              mountPath: /data\n              subPath: \"\"\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-server\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: prometheus-server\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/server/service.yaml"}, "region": {"startLine": 3, "endLine": 25, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    component: \"server\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-server\n  namespace: default\nspec:\n  ports:\n    - name: http\n      port: 80\n      protocol: TCP\n      targetPort: 9090\n  selector:\n    component: \"server\"\n    app: prometheus\n    release: prometheus\n  sessionAffinity: None\n  type: \"ClusterIP\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/server/cm.yaml"}, "region": {"startLine": 3, "endLine": 328, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  labels:\n    component: \"server\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-server\n  namespace: default\ndata:\n  alerting_rules.yml: |\n    {}\n  alerts: |\n    {}\n  prometheus.yml: |\n    global:\n      evaluation_interval: 1m\n      scrape_interval: 1m\n      scrape_timeout: 10s\n    rule_files:\n    - /etc/config/recording_rules.yml\n    - /etc/config/alerting_rules.yml\n    - /etc/config/rules\n    - /etc/config/alerts\n    scrape_configs:\n    - job_name: prometheus\n      static_configs:\n      - targets:\n        - localhost:9090\n    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      job_name: kubernetes-apiservers\n      kubernetes_sd_configs:\n      - role: endpoints\n      relabel_configs:\n      - action: keep\n        regex: default;kubernetes;https\n        source_labels:\n        - __meta_kubernetes_namespace\n        - __meta_kubernetes_service_name\n        - __meta_kubernetes_endpoint_port_name\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n        insecure_skip_verify: true\n    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      job_name: kubernetes-nodes\n      kubernetes_sd_configs:\n      - role: node\n      relabel_configs:\n      - action: labelmap\n        regex: __meta_kubernetes_node_label_(.+)\n      - replacement: kubernetes.default.svc:443\n        target_label: __address__\n      - regex: (.+)\n        replacement: /api/v1/nodes/$1/proxy/metrics\n        source_labels:\n        - __meta_kubernetes_node_name\n        target_label: __metrics_path__\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n        insecure_skip_verify: true\n    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      job_name: kubernetes-nodes-cadvisor\n      kubernetes_sd_configs:\n      - role: node\n      relabel_configs:\n      - action: labelmap\n        regex: __meta_kubernetes_node_label_(.+)\n      - replacement: kubernetes.default.svc:443\n        target_label: __address__\n      - regex: (.+)\n        replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor\n        source_labels:\n        - __meta_kubernetes_node_name\n        target_label: __metrics_path__\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n        insecure_skip_verify: true\n    - job_name: kubernetes-service-endpoints\n      kubernetes_sd_configs:\n      - role: endpoints\n      relabel_configs:\n      - action: keep\n        regex: true\n        source_labels:\n        - __meta_kubernetes_service_annotation_prometheus_io_scrape\n      - action: drop\n        regex: true\n        source_labels:\n        - __meta_kubernetes_service_annotation_prometheus_io_scrape_slow\n      - action: replace\n        regex: (https?)\n        source_labels:\n        - __meta_kubernetes_service_annotation_prometheus_io_scheme\n        target_label: __scheme__\n      - action: replace\n        regex: (.+)\n        source_labels:\n        - __meta_kubernetes_service_annotation_prometheus_io_path\n        target_label: __metrics_path__\n      - action: replace\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n        source_labels:\n        - __address__\n        - __meta_kubernetes_service_annotation_prometheus_io_port\n        target_label: __address__\n      - action: labelmap\n        regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)\n        replacement: __param_$1\n      - action: labelmap\n        regex: __meta_kubernetes_service_label_(.+)\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_namespace\n        target_label: namespace\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_service_name\n        target_label: service\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_node_name\n        target_label: node\n    - job_name: kubernetes-service-endpoints-slow\n      kubernetes_sd_configs:\n      - role: endpoints\n      relabel_configs:\n      - action: keep\n        regex: true\n        source_labels:\n        - __meta_kubernetes_service_annotation_prometheus_io_scrape_slow\n      - action: replace\n        regex: (https?)\n        source_labels:\n        - __meta_kubernetes_service_annotation_prometheus_io_scheme\n        target_label: __scheme__\n      - action: replace\n        regex: (.+)\n        source_labels:\n        - __meta_kubernetes_service_annotation_prometheus_io_path\n        target_label: __metrics_path__\n      - action: replace\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n        source_labels:\n        - __address__\n        - __meta_kubernetes_service_annotation_prometheus_io_port\n        target_label: __address__\n      - action: labelmap\n        regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)\n        replacement: __param_$1\n      - action: labelmap\n        regex: __meta_kubernetes_service_label_(.+)\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_namespace\n        target_label: namespace\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_service_name\n        target_label: service\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_node_name\n        target_label: node\n      scrape_interval: 5m\n      scrape_timeout: 30s\n    - honor_labels: true\n      job_name: prometheus-pushgateway\n      kubernetes_sd_configs:\n      - role: service\n      relabel_configs:\n      - action: keep\n        regex: pushgateway\n        source_labels:\n        - __meta_kubernetes_service_annotation_prometheus_io_probe\n    - job_name: kubernetes-services\n      kubernetes_sd_configs:\n      - role: service\n      metrics_path: /probe\n      params:\n        module:\n        - http_2xx\n      relabel_configs:\n      - action: keep\n        regex: true\n        source_labels:\n        - __meta_kubernetes_service_annotation_prometheus_io_probe\n      - source_labels:\n        - __address__\n        target_label: __param_target\n      - replacement: blackbox\n        target_label: __address__\n      - source_labels:\n        - __param_target\n        target_label: instance\n      - action: labelmap\n        regex: __meta_kubernetes_service_label_(.+)\n      - source_labels:\n        - __meta_kubernetes_namespace\n        target_label: namespace\n      - source_labels:\n        - __meta_kubernetes_service_name\n        target_label: service\n    - job_name: kubernetes-pods\n      kubernetes_sd_configs:\n      - role: pod\n      relabel_configs:\n      - action: keep\n        regex: true\n        source_labels:\n        - __meta_kubernetes_pod_annotation_prometheus_io_scrape\n      - action: drop\n        regex: true\n        source_labels:\n        - __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow\n      - action: replace\n        regex: (https?)\n        source_labels:\n        - __meta_kubernetes_pod_annotation_prometheus_io_scheme\n        target_label: __scheme__\n      - action: replace\n        regex: (.+)\n        source_labels:\n        - __meta_kubernetes_pod_annotation_prometheus_io_path\n        target_label: __metrics_path__\n      - action: replace\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n        source_labels:\n        - __address__\n        - __meta_kubernetes_pod_annotation_prometheus_io_port\n        target_label: __address__\n      - action: labelmap\n        regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)\n        replacement: __param_$1\n      - action: labelmap\n        regex: __meta_kubernetes_pod_label_(.+)\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_namespace\n        target_label: namespace\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_name\n        target_label: pod\n      - action: drop\n        regex: Pending|Succeeded|Failed|Completed\n        source_labels:\n        - __meta_kubernetes_pod_phase\n    - job_name: kubernetes-pods-slow\n      kubernetes_sd_configs:\n      - role: pod\n      relabel_configs:\n      - action: keep\n        regex: true\n        source_labels:\n        - __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow\n      - action: replace\n        regex: (https?)\n        source_labels:\n        - __meta_kubernetes_pod_annotation_prometheus_io_scheme\n        target_label: __scheme__\n      - action: replace\n        regex: (.+)\n        source_labels:\n        - __meta_kubernetes_pod_annotation_prometheus_io_path\n        target_label: __metrics_path__\n      - action: replace\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n        source_labels:\n        - __address__\n        - __meta_kubernetes_pod_annotation_prometheus_io_port\n        target_label: __address__\n      - action: labelmap\n        regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)\n        replacement: __param_$1\n      - action: labelmap\n        regex: __meta_kubernetes_pod_label_(.+)\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_namespace\n        target_label: namespace\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_name\n        target_label: pod\n      - action: drop\n        regex: Pending|Succeeded|Failed|Completed\n        source_labels:\n        - __meta_kubernetes_pod_phase\n      scrape_interval: 5m\n      scrape_timeout: 30s\n    alerting:\n      alertmanagers:\n      - kubernetes_sd_configs:\n          - role: pod\n        tls_config:\n          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n        relabel_configs:\n        - source_labels: [__meta_kubernetes_namespace]\n          regex: default\n          action: keep\n        - source_labels: [__meta_kubernetes_pod_label_app]\n          regex: prometheus\n          action: keep\n        - source_labels: [__meta_kubernetes_pod_label_component]\n          regex: alertmanager\n          action: keep\n        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_probe]\n          regex: .*\n          action: keep\n        - source_labels: [__meta_kubernetes_pod_container_port_number]\n          regex: \"9093\"\n          action: keep\n  recording_rules.yml: |\n    {}\n  rules: |\n    {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/server/serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 15, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    component: \"server\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-server\n  namespace: default\n  annotations:\n    {}\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/node-exporter/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    component: \"node-exporter\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-node-exporter\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"node-exporter\"\n      app: prometheus\n      release: prometheus\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        component: \"node-exporter\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-node-exporter\n      containers:\n        - name: prometheus-node-exporter\n          image: \"quay.io/prometheus/node-exporter:v1.3.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --path.procfs=/host/proc\n            - --path.sysfs=/host/sys\n            - --path.rootfs=/host/root\n            - --web.listen-address=:9100\n          ports:\n            - name: metrics\n              containerPort: 9100\n              hostPort: 9100\n          resources:\n            {}\n          volumeMounts:\n            - name: proc\n              mountPath: /host/proc\n              readOnly:  true\n            - name: sys\n              mountPath: /host/sys\n              readOnly: true\n            - name: root\n              mountPath: /host/root\n              mountPropagation: HostToContainer\n              readOnly: true\n      hostNetwork: true\n      hostPID: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: proc\n          hostPath:\n            path: /proc\n        - name: sys\n          hostPath:\n            path: /sys\n        - name: root\n          hostPath:\n            path: /\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/node-exporter/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    component: \"node-exporter\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-node-exporter\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"node-exporter\"\n      app: prometheus\n      release: prometheus\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        component: \"node-exporter\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-node-exporter\n      containers:\n        - name: prometheus-node-exporter\n          image: \"quay.io/prometheus/node-exporter:v1.3.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --path.procfs=/host/proc\n            - --path.sysfs=/host/sys\n            - --path.rootfs=/host/root\n            - --web.listen-address=:9100\n          ports:\n            - name: metrics\n              containerPort: 9100\n              hostPort: 9100\n          resources:\n            {}\n          volumeMounts:\n            - name: proc\n              mountPath: /host/proc\n              readOnly:  true\n            - name: sys\n              mountPath: /host/sys\n              readOnly: true\n            - name: root\n              mountPath: /host/root\n              mountPropagation: HostToContainer\n              readOnly: true\n      hostNetwork: true\n      hostPID: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: proc\n          hostPath:\n            path: /proc\n        - name: sys\n          hostPath:\n            path: /sys\n        - name: root\n          hostPath:\n            path: /\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/node-exporter/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    component: \"node-exporter\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-node-exporter\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"node-exporter\"\n      app: prometheus\n      release: prometheus\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        component: \"node-exporter\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-node-exporter\n      containers:\n        - name: prometheus-node-exporter\n          image: \"quay.io/prometheus/node-exporter:v1.3.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --path.procfs=/host/proc\n            - --path.sysfs=/host/sys\n            - --path.rootfs=/host/root\n            - --web.listen-address=:9100\n          ports:\n            - name: metrics\n              containerPort: 9100\n              hostPort: 9100\n          resources:\n            {}\n          volumeMounts:\n            - name: proc\n              mountPath: /host/proc\n              readOnly:  true\n            - name: sys\n              mountPath: /host/sys\n              readOnly: true\n            - name: root\n              mountPath: /host/root\n              mountPropagation: HostToContainer\n              readOnly: true\n      hostNetwork: true\n      hostPID: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: proc\n          hostPath:\n            path: /proc\n        - name: sys\n          hostPath:\n            path: /sys\n        - name: root\n          hostPath:\n            path: /\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/node-exporter/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    component: \"node-exporter\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-node-exporter\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"node-exporter\"\n      app: prometheus\n      release: prometheus\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        component: \"node-exporter\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-node-exporter\n      containers:\n        - name: prometheus-node-exporter\n          image: \"quay.io/prometheus/node-exporter:v1.3.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --path.procfs=/host/proc\n            - --path.sysfs=/host/sys\n            - --path.rootfs=/host/root\n            - --web.listen-address=:9100\n          ports:\n            - name: metrics\n              containerPort: 9100\n              hostPort: 9100\n          resources:\n            {}\n          volumeMounts:\n            - name: proc\n              mountPath: /host/proc\n              readOnly:  true\n            - name: sys\n              mountPath: /host/sys\n              readOnly: true\n            - name: root\n              mountPath: /host/root\n              mountPropagation: HostToContainer\n              readOnly: true\n      hostNetwork: true\n      hostPID: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: proc\n          hostPath:\n            path: /proc\n        - name: sys\n          hostPath:\n            path: /sys\n        - name: root\n          hostPath:\n            path: /\n"}}}}]}, {"ruleId": "CKV_K8S_26", "ruleIndex": 34, "level": "error", "attachments": [], "message": {"text": "Do not specify hostPort unless absolutely necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/node-exporter/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    component: \"node-exporter\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-node-exporter\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"node-exporter\"\n      app: prometheus\n      release: prometheus\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        component: \"node-exporter\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-node-exporter\n      containers:\n        - name: prometheus-node-exporter\n          image: \"quay.io/prometheus/node-exporter:v1.3.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --path.procfs=/host/proc\n            - --path.sysfs=/host/sys\n            - --path.rootfs=/host/root\n            - --web.listen-address=:9100\n          ports:\n            - name: metrics\n              containerPort: 9100\n              hostPort: 9100\n          resources:\n            {}\n          volumeMounts:\n            - name: proc\n              mountPath: /host/proc\n              readOnly:  true\n            - name: sys\n              mountPath: /host/sys\n              readOnly: true\n            - name: root\n              mountPath: /host/root\n              mountPropagation: HostToContainer\n              readOnly: true\n      hostNetwork: true\n      hostPID: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: proc\n          hostPath:\n            path: /proc\n        - name: sys\n          hostPath:\n            path: /sys\n        - name: root\n          hostPath:\n            path: /\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/node-exporter/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    component: \"node-exporter\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-node-exporter\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"node-exporter\"\n      app: prometheus\n      release: prometheus\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        component: \"node-exporter\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-node-exporter\n      containers:\n        - name: prometheus-node-exporter\n          image: \"quay.io/prometheus/node-exporter:v1.3.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --path.procfs=/host/proc\n            - --path.sysfs=/host/sys\n            - --path.rootfs=/host/root\n            - --web.listen-address=:9100\n          ports:\n            - name: metrics\n              containerPort: 9100\n              hostPort: 9100\n          resources:\n            {}\n          volumeMounts:\n            - name: proc\n              mountPath: /host/proc\n              readOnly:  true\n            - name: sys\n              mountPath: /host/sys\n              readOnly: true\n            - name: root\n              mountPath: /host/root\n              mountPropagation: HostToContainer\n              readOnly: true\n      hostNetwork: true\n      hostPID: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: proc\n          hostPath:\n            path: /proc\n        - name: sys\n          hostPath:\n            path: /sys\n        - name: root\n          hostPath:\n            path: /\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/node-exporter/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    component: \"node-exporter\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-node-exporter\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"node-exporter\"\n      app: prometheus\n      release: prometheus\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        component: \"node-exporter\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-node-exporter\n      containers:\n        - name: prometheus-node-exporter\n          image: \"quay.io/prometheus/node-exporter:v1.3.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --path.procfs=/host/proc\n            - --path.sysfs=/host/sys\n            - --path.rootfs=/host/root\n            - --web.listen-address=:9100\n          ports:\n            - name: metrics\n              containerPort: 9100\n              hostPort: 9100\n          resources:\n            {}\n          volumeMounts:\n            - name: proc\n              mountPath: /host/proc\n              readOnly:  true\n            - name: sys\n              mountPath: /host/sys\n              readOnly: true\n            - name: root\n              mountPath: /host/root\n              mountPropagation: HostToContainer\n              readOnly: true\n      hostNetwork: true\n      hostPID: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: proc\n          hostPath:\n            path: /proc\n        - name: sys\n          hostPath:\n            path: /sys\n        - name: root\n          hostPath:\n            path: /\n"}}}}]}, {"ruleId": "CKV_K8S_19", "ruleIndex": 31, "level": "error", "attachments": [], "message": {"text": "Containers should not share the host network namespace"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/node-exporter/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    component: \"node-exporter\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-node-exporter\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"node-exporter\"\n      app: prometheus\n      release: prometheus\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        component: \"node-exporter\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-node-exporter\n      containers:\n        - name: prometheus-node-exporter\n          image: \"quay.io/prometheus/node-exporter:v1.3.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --path.procfs=/host/proc\n            - --path.sysfs=/host/sys\n            - --path.rootfs=/host/root\n            - --web.listen-address=:9100\n          ports:\n            - name: metrics\n              containerPort: 9100\n              hostPort: 9100\n          resources:\n            {}\n          volumeMounts:\n            - name: proc\n              mountPath: /host/proc\n              readOnly:  true\n            - name: sys\n              mountPath: /host/sys\n              readOnly: true\n            - name: root\n              mountPath: /host/root\n              mountPropagation: HostToContainer\n              readOnly: true\n      hostNetwork: true\n      hostPID: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: proc\n          hostPath:\n            path: /proc\n        - name: sys\n          hostPath:\n            path: /sys\n        - name: root\n          hostPath:\n            path: /\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/node-exporter/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    component: \"node-exporter\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-node-exporter\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"node-exporter\"\n      app: prometheus\n      release: prometheus\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        component: \"node-exporter\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-node-exporter\n      containers:\n        - name: prometheus-node-exporter\n          image: \"quay.io/prometheus/node-exporter:v1.3.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --path.procfs=/host/proc\n            - --path.sysfs=/host/sys\n            - --path.rootfs=/host/root\n            - --web.listen-address=:9100\n          ports:\n            - name: metrics\n              containerPort: 9100\n              hostPort: 9100\n          resources:\n            {}\n          volumeMounts:\n            - name: proc\n              mountPath: /host/proc\n              readOnly:  true\n            - name: sys\n              mountPath: /host/sys\n              readOnly: true\n            - name: root\n              mountPath: /host/root\n              mountPropagation: HostToContainer\n              readOnly: true\n      hostNetwork: true\n      hostPID: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: proc\n          hostPath:\n            path: /proc\n        - name: sys\n          hostPath:\n            path: /sys\n        - name: root\n          hostPath:\n            path: /\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/node-exporter/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    component: \"node-exporter\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-node-exporter\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"node-exporter\"\n      app: prometheus\n      release: prometheus\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        component: \"node-exporter\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-node-exporter\n      containers:\n        - name: prometheus-node-exporter\n          image: \"quay.io/prometheus/node-exporter:v1.3.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --path.procfs=/host/proc\n            - --path.sysfs=/host/sys\n            - --path.rootfs=/host/root\n            - --web.listen-address=:9100\n          ports:\n            - name: metrics\n              containerPort: 9100\n              hostPort: 9100\n          resources:\n            {}\n          volumeMounts:\n            - name: proc\n              mountPath: /host/proc\n              readOnly:  true\n            - name: sys\n              mountPath: /host/sys\n              readOnly: true\n            - name: root\n              mountPath: /host/root\n              mountPropagation: HostToContainer\n              readOnly: true\n      hostNetwork: true\n      hostPID: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: proc\n          hostPath:\n            path: /proc\n        - name: sys\n          hostPath:\n            path: /sys\n        - name: root\n          hostPath:\n            path: /\n"}}}}]}, {"ruleId": "CKV_K8S_17", "ruleIndex": 35, "level": "error", "attachments": [], "message": {"text": "Containers should not share the host process ID namespace"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/node-exporter/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    component: \"node-exporter\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-node-exporter\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"node-exporter\"\n      app: prometheus\n      release: prometheus\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        component: \"node-exporter\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-node-exporter\n      containers:\n        - name: prometheus-node-exporter\n          image: \"quay.io/prometheus/node-exporter:v1.3.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --path.procfs=/host/proc\n            - --path.sysfs=/host/sys\n            - --path.rootfs=/host/root\n            - --web.listen-address=:9100\n          ports:\n            - name: metrics\n              containerPort: 9100\n              hostPort: 9100\n          resources:\n            {}\n          volumeMounts:\n            - name: proc\n              mountPath: /host/proc\n              readOnly:  true\n            - name: sys\n              mountPath: /host/sys\n              readOnly: true\n            - name: root\n              mountPath: /host/root\n              mountPropagation: HostToContainer\n              readOnly: true\n      hostNetwork: true\n      hostPID: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: proc\n          hostPath:\n            path: /proc\n        - name: sys\n          hostPath:\n            path: /sys\n        - name: root\n          hostPath:\n            path: /\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/node-exporter/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    component: \"node-exporter\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-node-exporter\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"node-exporter\"\n      app: prometheus\n      release: prometheus\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        component: \"node-exporter\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-node-exporter\n      containers:\n        - name: prometheus-node-exporter\n          image: \"quay.io/prometheus/node-exporter:v1.3.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --path.procfs=/host/proc\n            - --path.sysfs=/host/sys\n            - --path.rootfs=/host/root\n            - --web.listen-address=:9100\n          ports:\n            - name: metrics\n              containerPort: 9100\n              hostPort: 9100\n          resources:\n            {}\n          volumeMounts:\n            - name: proc\n              mountPath: /host/proc\n              readOnly:  true\n            - name: sys\n              mountPath: /host/sys\n              readOnly: true\n            - name: root\n              mountPath: /host/root\n              mountPropagation: HostToContainer\n              readOnly: true\n      hostNetwork: true\n      hostPID: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: proc\n          hostPath:\n            path: /proc\n        - name: sys\n          hostPath:\n            path: /sys\n        - name: root\n          hostPath:\n            path: /\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/node-exporter/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    component: \"node-exporter\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-node-exporter\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"node-exporter\"\n      app: prometheus\n      release: prometheus\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        component: \"node-exporter\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-node-exporter\n      containers:\n        - name: prometheus-node-exporter\n          image: \"quay.io/prometheus/node-exporter:v1.3.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --path.procfs=/host/proc\n            - --path.sysfs=/host/sys\n            - --path.rootfs=/host/root\n            - --web.listen-address=:9100\n          ports:\n            - name: metrics\n              containerPort: 9100\n              hostPort: 9100\n          resources:\n            {}\n          volumeMounts:\n            - name: proc\n              mountPath: /host/proc\n              readOnly:  true\n            - name: sys\n              mountPath: /host/sys\n              readOnly: true\n            - name: root\n              mountPath: /host/root\n              mountPropagation: HostToContainer\n              readOnly: true\n      hostNetwork: true\n      hostPID: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: proc\n          hostPath:\n            path: /proc\n        - name: sys\n          hostPath:\n            path: /sys\n        - name: root\n          hostPath:\n            path: /\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/node-exporter/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    component: \"node-exporter\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-node-exporter\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"node-exporter\"\n      app: prometheus\n      release: prometheus\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        component: \"node-exporter\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-node-exporter\n      containers:\n        - name: prometheus-node-exporter\n          image: \"quay.io/prometheus/node-exporter:v1.3.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --path.procfs=/host/proc\n            - --path.sysfs=/host/sys\n            - --path.rootfs=/host/root\n            - --web.listen-address=:9100\n          ports:\n            - name: metrics\n              containerPort: 9100\n              hostPort: 9100\n          resources:\n            {}\n          volumeMounts:\n            - name: proc\n              mountPath: /host/proc\n              readOnly:  true\n            - name: sys\n              mountPath: /host/sys\n              readOnly: true\n            - name: root\n              mountPath: /host/root\n              mountPropagation: HostToContainer\n              readOnly: true\n      hostNetwork: true\n      hostPID: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: proc\n          hostPath:\n            path: /proc\n        - name: sys\n          hostPath:\n            path: /sys\n        - name: root\n          hostPath:\n            path: /\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/node-exporter/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    component: \"node-exporter\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-node-exporter\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"node-exporter\"\n      app: prometheus\n      release: prometheus\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        component: \"node-exporter\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-node-exporter\n      containers:\n        - name: prometheus-node-exporter\n          image: \"quay.io/prometheus/node-exporter:v1.3.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --path.procfs=/host/proc\n            - --path.sysfs=/host/sys\n            - --path.rootfs=/host/root\n            - --web.listen-address=:9100\n          ports:\n            - name: metrics\n              containerPort: 9100\n              hostPort: 9100\n          resources:\n            {}\n          volumeMounts:\n            - name: proc\n              mountPath: /host/proc\n              readOnly:  true\n            - name: sys\n              mountPath: /host/sys\n              readOnly: true\n            - name: root\n              mountPath: /host/root\n              mountPropagation: HostToContainer\n              readOnly: true\n      hostNetwork: true\n      hostPID: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: proc\n          hostPath:\n            path: /proc\n        - name: sys\n          hostPath:\n            path: /sys\n        - name: root\n          hostPath:\n            path: /\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/node-exporter/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    component: \"node-exporter\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-node-exporter\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"node-exporter\"\n      app: prometheus\n      release: prometheus\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        component: \"node-exporter\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-node-exporter\n      containers:\n        - name: prometheus-node-exporter\n          image: \"quay.io/prometheus/node-exporter:v1.3.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --path.procfs=/host/proc\n            - --path.sysfs=/host/sys\n            - --path.rootfs=/host/root\n            - --web.listen-address=:9100\n          ports:\n            - name: metrics\n              containerPort: 9100\n              hostPort: 9100\n          resources:\n            {}\n          volumeMounts:\n            - name: proc\n              mountPath: /host/proc\n              readOnly:  true\n            - name: sys\n              mountPath: /host/sys\n              readOnly: true\n            - name: root\n              mountPath: /host/root\n              mountPropagation: HostToContainer\n              readOnly: true\n      hostNetwork: true\n      hostPID: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: proc\n          hostPath:\n            path: /proc\n        - name: sys\n          hostPath:\n            path: /sys\n        - name: root\n          hostPath:\n            path: /\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/node-exporter/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 74, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    component: \"node-exporter\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-node-exporter\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      component: \"node-exporter\"\n      app: prometheus\n      release: prometheus\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        component: \"node-exporter\"\n        app: prometheus\n        release: prometheus\n        chart: prometheus-15.0.4\n        heritage: Helm\n    spec:\n      serviceAccountName: prometheus-node-exporter\n      containers:\n        - name: prometheus-node-exporter\n          image: \"quay.io/prometheus/node-exporter:v1.3.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - --path.procfs=/host/proc\n            - --path.sysfs=/host/sys\n            - --path.rootfs=/host/root\n            - --web.listen-address=:9100\n          ports:\n            - name: metrics\n              containerPort: 9100\n              hostPort: 9100\n          resources:\n            {}\n          volumeMounts:\n            - name: proc\n              mountPath: /host/proc\n              readOnly:  true\n            - name: sys\n              mountPath: /host/sys\n              readOnly: true\n            - name: root\n              mountPath: /host/root\n              mountPropagation: HostToContainer\n              readOnly: true\n      hostNetwork: true\n      hostPID: true\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n        - name: proc\n          hostPath:\n            path: /proc\n        - name: sys\n          hostPath:\n            path: /sys\n        - name: root\n          hostPath:\n            path: /\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/node-exporter/svc.yaml"}, "region": {"startLine": 3, "endLine": 27, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    prometheus.io/scrape: \"true\"\n  labels:\n    component: \"node-exporter\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-node-exporter\n  namespace: default\nspec:\n  clusterIP: None\n  ports:\n    - name: metrics\n      port: 9100\n      protocol: TCP\n      targetPort: 9100\n  selector:\n    component: \"node-exporter\"\n    app: prometheus\n    release: prometheus\n  type: \"ClusterIP\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/prometheus/templates/node-exporter/serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 15, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    component: \"node-exporter\"\n    app: prometheus\n    release: prometheus\n    chart: prometheus-15.0.4\n    heritage: Helm\n  name: prometheus-node-exporter\n  namespace: default\n  annotations:\n    {}\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-ui/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 77, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-ui\n  labels:\n    helm.sh/chart: usp-ui-0.1.1\n    app.kubernetes.io/name: usp-ui\n    app.kubernetes.io/instance: usp-ui\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-ui\n      app.kubernetes.io/instance: usp-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-ui\n        app.kubernetes.io/instance: usp-ui\n    spec:\n      restartPolicy: Always\n      serviceAccountName: usp-ui\n      volumes:\n        - name: \"usp-ui-node-fs\"\n          emptyDir: {}\n      securityContext:\n        {}\n      containers:\n        - name: usp-ui\n          securityContext:\n            allowPrivilegeEscalation: false\n            privileged: false\n            readOnlyRootFilesystem: true\n          image: \"artifactory.wrs.com/docker-devstar/usp/ui:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 4200\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 8888\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            failureThreshold: 8\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8888\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            failureThreshold: 8\n          resources:\n            limits:\n              cpu: 200m\n              memory: 512Mi\n            requests:\n              cpu: 100m\n              memory: 256Mi\n          volumeMounts:\n            - name: \"usp-ui-node-fs\"\n              mountPath: /usr/share/nginx/html/assets/config\n              subPath: config.json\n            - name: \"usp-ui-node-fs\"\n              mountPath: /run/nginx/\n              subPath: run\n            - name: \"usp-ui-node-fs\"\n              mountPath: /var/log/nginx/\n              subPath: logs\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"http://localhost:3000\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"http://localhost:3000\\\",\\\"portalMsUrl\\\":\\\"http://localhost:3000\\\",\\\"signInFunc\\\":null,\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.api.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-ui/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 77, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-ui\n  labels:\n    helm.sh/chart: usp-ui-0.1.1\n    app.kubernetes.io/name: usp-ui\n    app.kubernetes.io/instance: usp-ui\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-ui\n      app.kubernetes.io/instance: usp-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-ui\n        app.kubernetes.io/instance: usp-ui\n    spec:\n      restartPolicy: Always\n      serviceAccountName: usp-ui\n      volumes:\n        - name: \"usp-ui-node-fs\"\n          emptyDir: {}\n      securityContext:\n        {}\n      containers:\n        - name: usp-ui\n          securityContext:\n            allowPrivilegeEscalation: false\n            privileged: false\n            readOnlyRootFilesystem: true\n          image: \"artifactory.wrs.com/docker-devstar/usp/ui:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 4200\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 8888\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            failureThreshold: 8\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8888\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            failureThreshold: 8\n          resources:\n            limits:\n              cpu: 200m\n              memory: 512Mi\n            requests:\n              cpu: 100m\n              memory: 256Mi\n          volumeMounts:\n            - name: \"usp-ui-node-fs\"\n              mountPath: /usr/share/nginx/html/assets/config\n              subPath: config.json\n            - name: \"usp-ui-node-fs\"\n              mountPath: /run/nginx/\n              subPath: run\n            - name: \"usp-ui-node-fs\"\n              mountPath: /var/log/nginx/\n              subPath: logs\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"http://localhost:3000\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"http://localhost:3000\\\",\\\"portalMsUrl\\\":\\\"http://localhost:3000\\\",\\\"signInFunc\\\":null,\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.api.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-ui/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 77, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-ui\n  labels:\n    helm.sh/chart: usp-ui-0.1.1\n    app.kubernetes.io/name: usp-ui\n    app.kubernetes.io/instance: usp-ui\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-ui\n      app.kubernetes.io/instance: usp-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-ui\n        app.kubernetes.io/instance: usp-ui\n    spec:\n      restartPolicy: Always\n      serviceAccountName: usp-ui\n      volumes:\n        - name: \"usp-ui-node-fs\"\n          emptyDir: {}\n      securityContext:\n        {}\n      containers:\n        - name: usp-ui\n          securityContext:\n            allowPrivilegeEscalation: false\n            privileged: false\n            readOnlyRootFilesystem: true\n          image: \"artifactory.wrs.com/docker-devstar/usp/ui:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 4200\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 8888\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            failureThreshold: 8\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8888\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            failureThreshold: 8\n          resources:\n            limits:\n              cpu: 200m\n              memory: 512Mi\n            requests:\n              cpu: 100m\n              memory: 256Mi\n          volumeMounts:\n            - name: \"usp-ui-node-fs\"\n              mountPath: /usr/share/nginx/html/assets/config\n              subPath: config.json\n            - name: \"usp-ui-node-fs\"\n              mountPath: /run/nginx/\n              subPath: run\n            - name: \"usp-ui-node-fs\"\n              mountPath: /var/log/nginx/\n              subPath: logs\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"http://localhost:3000\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"http://localhost:3000\\\",\\\"portalMsUrl\\\":\\\"http://localhost:3000\\\",\\\"signInFunc\\\":null,\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.api.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-ui/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 77, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-ui\n  labels:\n    helm.sh/chart: usp-ui-0.1.1\n    app.kubernetes.io/name: usp-ui\n    app.kubernetes.io/instance: usp-ui\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-ui\n      app.kubernetes.io/instance: usp-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-ui\n        app.kubernetes.io/instance: usp-ui\n    spec:\n      restartPolicy: Always\n      serviceAccountName: usp-ui\n      volumes:\n        - name: \"usp-ui-node-fs\"\n          emptyDir: {}\n      securityContext:\n        {}\n      containers:\n        - name: usp-ui\n          securityContext:\n            allowPrivilegeEscalation: false\n            privileged: false\n            readOnlyRootFilesystem: true\n          image: \"artifactory.wrs.com/docker-devstar/usp/ui:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 4200\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 8888\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            failureThreshold: 8\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8888\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            failureThreshold: 8\n          resources:\n            limits:\n              cpu: 200m\n              memory: 512Mi\n            requests:\n              cpu: 100m\n              memory: 256Mi\n          volumeMounts:\n            - name: \"usp-ui-node-fs\"\n              mountPath: /usr/share/nginx/html/assets/config\n              subPath: config.json\n            - name: \"usp-ui-node-fs\"\n              mountPath: /run/nginx/\n              subPath: run\n            - name: \"usp-ui-node-fs\"\n              mountPath: /var/log/nginx/\n              subPath: logs\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"http://localhost:3000\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"http://localhost:3000\\\",\\\"portalMsUrl\\\":\\\"http://localhost:3000\\\",\\\"signInFunc\\\":null,\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.api.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-ui/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 77, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-ui\n  labels:\n    helm.sh/chart: usp-ui-0.1.1\n    app.kubernetes.io/name: usp-ui\n    app.kubernetes.io/instance: usp-ui\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-ui\n      app.kubernetes.io/instance: usp-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-ui\n        app.kubernetes.io/instance: usp-ui\n    spec:\n      restartPolicy: Always\n      serviceAccountName: usp-ui\n      volumes:\n        - name: \"usp-ui-node-fs\"\n          emptyDir: {}\n      securityContext:\n        {}\n      containers:\n        - name: usp-ui\n          securityContext:\n            allowPrivilegeEscalation: false\n            privileged: false\n            readOnlyRootFilesystem: true\n          image: \"artifactory.wrs.com/docker-devstar/usp/ui:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 4200\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 8888\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            failureThreshold: 8\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8888\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            failureThreshold: 8\n          resources:\n            limits:\n              cpu: 200m\n              memory: 512Mi\n            requests:\n              cpu: 100m\n              memory: 256Mi\n          volumeMounts:\n            - name: \"usp-ui-node-fs\"\n              mountPath: /usr/share/nginx/html/assets/config\n              subPath: config.json\n            - name: \"usp-ui-node-fs\"\n              mountPath: /run/nginx/\n              subPath: run\n            - name: \"usp-ui-node-fs\"\n              mountPath: /var/log/nginx/\n              subPath: logs\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"http://localhost:3000\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"http://localhost:3000\\\",\\\"portalMsUrl\\\":\\\"http://localhost:3000\\\",\\\"signInFunc\\\":null,\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.api.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-ui/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 77, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-ui\n  labels:\n    helm.sh/chart: usp-ui-0.1.1\n    app.kubernetes.io/name: usp-ui\n    app.kubernetes.io/instance: usp-ui\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-ui\n      app.kubernetes.io/instance: usp-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-ui\n        app.kubernetes.io/instance: usp-ui\n    spec:\n      restartPolicy: Always\n      serviceAccountName: usp-ui\n      volumes:\n        - name: \"usp-ui-node-fs\"\n          emptyDir: {}\n      securityContext:\n        {}\n      containers:\n        - name: usp-ui\n          securityContext:\n            allowPrivilegeEscalation: false\n            privileged: false\n            readOnlyRootFilesystem: true\n          image: \"artifactory.wrs.com/docker-devstar/usp/ui:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 4200\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 8888\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            failureThreshold: 8\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8888\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            failureThreshold: 8\n          resources:\n            limits:\n              cpu: 200m\n              memory: 512Mi\n            requests:\n              cpu: 100m\n              memory: 256Mi\n          volumeMounts:\n            - name: \"usp-ui-node-fs\"\n              mountPath: /usr/share/nginx/html/assets/config\n              subPath: config.json\n            - name: \"usp-ui-node-fs\"\n              mountPath: /run/nginx/\n              subPath: run\n            - name: \"usp-ui-node-fs\"\n              mountPath: /var/log/nginx/\n              subPath: logs\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"http://localhost:3000\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"http://localhost:3000\\\",\\\"portalMsUrl\\\":\\\"http://localhost:3000\\\",\\\"signInFunc\\\":null,\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.api.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-ui/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 77, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-ui\n  labels:\n    helm.sh/chart: usp-ui-0.1.1\n    app.kubernetes.io/name: usp-ui\n    app.kubernetes.io/instance: usp-ui\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-ui\n      app.kubernetes.io/instance: usp-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-ui\n        app.kubernetes.io/instance: usp-ui\n    spec:\n      restartPolicy: Always\n      serviceAccountName: usp-ui\n      volumes:\n        - name: \"usp-ui-node-fs\"\n          emptyDir: {}\n      securityContext:\n        {}\n      containers:\n        - name: usp-ui\n          securityContext:\n            allowPrivilegeEscalation: false\n            privileged: false\n            readOnlyRootFilesystem: true\n          image: \"artifactory.wrs.com/docker-devstar/usp/ui:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 4200\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 8888\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            failureThreshold: 8\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8888\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            failureThreshold: 8\n          resources:\n            limits:\n              cpu: 200m\n              memory: 512Mi\n            requests:\n              cpu: 100m\n              memory: 256Mi\n          volumeMounts:\n            - name: \"usp-ui-node-fs\"\n              mountPath: /usr/share/nginx/html/assets/config\n              subPath: config.json\n            - name: \"usp-ui-node-fs\"\n              mountPath: /run/nginx/\n              subPath: run\n            - name: \"usp-ui-node-fs\"\n              mountPath: /var/log/nginx/\n              subPath: logs\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"http://localhost:3000\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"http://localhost:3000\\\",\\\"portalMsUrl\\\":\\\"http://localhost:3000\\\",\\\"signInFunc\\\":null,\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.api.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-ui/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 77, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-ui\n  labels:\n    helm.sh/chart: usp-ui-0.1.1\n    app.kubernetes.io/name: usp-ui\n    app.kubernetes.io/instance: usp-ui\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-ui\n      app.kubernetes.io/instance: usp-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-ui\n        app.kubernetes.io/instance: usp-ui\n    spec:\n      restartPolicy: Always\n      serviceAccountName: usp-ui\n      volumes:\n        - name: \"usp-ui-node-fs\"\n          emptyDir: {}\n      securityContext:\n        {}\n      containers:\n        - name: usp-ui\n          securityContext:\n            allowPrivilegeEscalation: false\n            privileged: false\n            readOnlyRootFilesystem: true\n          image: \"artifactory.wrs.com/docker-devstar/usp/ui:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 4200\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 8888\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            failureThreshold: 8\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8888\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            failureThreshold: 8\n          resources:\n            limits:\n              cpu: 200m\n              memory: 512Mi\n            requests:\n              cpu: 100m\n              memory: 256Mi\n          volumeMounts:\n            - name: \"usp-ui-node-fs\"\n              mountPath: /usr/share/nginx/html/assets/config\n              subPath: config.json\n            - name: \"usp-ui-node-fs\"\n              mountPath: /run/nginx/\n              subPath: run\n            - name: \"usp-ui-node-fs\"\n              mountPath: /var/log/nginx/\n              subPath: logs\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"http://localhost:3000\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"http://localhost:3000\\\",\\\"portalMsUrl\\\":\\\"http://localhost:3000\\\",\\\"signInFunc\\\":null,\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.api.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-ui/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 77, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: usp-ui\n  labels:\n    helm.sh/chart: usp-ui-0.1.1\n    app.kubernetes.io/name: usp-ui\n    app.kubernetes.io/instance: usp-ui\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: usp-ui\n      app.kubernetes.io/instance: usp-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: usp-ui\n        app.kubernetes.io/instance: usp-ui\n    spec:\n      restartPolicy: Always\n      serviceAccountName: usp-ui\n      volumes:\n        - name: \"usp-ui-node-fs\"\n          emptyDir: {}\n      securityContext:\n        {}\n      containers:\n        - name: usp-ui\n          securityContext:\n            allowPrivilegeEscalation: false\n            privileged: false\n            readOnlyRootFilesystem: true\n          image: \"artifactory.wrs.com/docker-devstar/usp/ui:0.0.1\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 4200\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 8888\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            failureThreshold: 8\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 8888\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            failureThreshold: 8\n          resources:\n            limits:\n              cpu: 200m\n              memory: 512Mi\n            requests:\n              cpu: 100m\n              memory: 256Mi\n          volumeMounts:\n            - name: \"usp-ui-node-fs\"\n              mountPath: /usr/share/nginx/html/assets/config\n              subPath: config.json\n            - name: \"usp-ui-node-fs\"\n              mountPath: /run/nginx/\n              subPath: run\n            - name: \"usp-ui-node-fs\"\n              mountPath: /var/log/nginx/\n              subPath: logs\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"http://localhost:3000\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"http://localhost:3000\\\",\\\"portalMsUrl\\\":\\\"http://localhost:3000\\\",\\\"signInFunc\\\":null,\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"wr-studio-sso\\\",\\\"tozIdHostName\\\":\\\"https://tozny.api.dev.wrstudio1.cloud\\\",\\\"tozIdRealmName\\\":\\\"wrai\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-ui/templates/service.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: usp-ui\n  labels:\n    helm.sh/chart: usp-ui-0.1.1\n    app.kubernetes.io/name: usp-ui\n    app.kubernetes.io/instance: usp-ui\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: NodePort\n  ports:\n    - port: 8888\n      targetPort: 8888\n      protocol: TCP\n      name: http\n  selector:\n    app.kubernetes.io/name: usp-ui\n    app.kubernetes.io/instance: usp-ui\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/usp-ui/templates/serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 12, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: usp-ui\n  labels:\n    helm.sh/chart: usp-ui-0.1.1\n    app.kubernetes.io/name: usp-ui\n    app.kubernetes.io/instance: usp-ui\n    app.kubernetes.io/version: \"1.16.0\"\n    app.kubernetes.io/managed-by: Helm\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/service.yaml"}, "region": {"startLine": 3, "endLine": 19, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: lxbs\n  labels:\n    app.kubernetes.io/name: lxbs\n    app.kubernetes.io/instance: lxbs\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: http\n      protocol: TCP\n      name: http-lxbs\n  selector:\n    app.kubernetes.io/name: lxbs\n    app.kubernetes.io/instance: lxbs\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/ingress/gateway.yaml"}, "region": {"startLine": 3, "endLine": 31, "snippet": {"text": "apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: lxbs-gateway\n  labels:\n    io.kompose.service: tls-terminator\n  annotations:\n    cert-manager.io/cluster-issuer: null\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/proxy-connect-timeout: \"360\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: \"360\"\n    nginx.ingress.kubernetes.io/proxy-send-timeout: \"360\"\n    nginx.ingress.kubernetes.io/service-upstream: \"\"\n    nginx.ingress.kubernetes.io/upstream-vhost: \"\"\nspec:\n  rules:\n    - host: \n      http:\n        paths:\n        - backend:\n            service: \n              name: lxbs-gateway\n              port: \n                number: 80\n          pathType: ImplementationSpecific\n  tls:\n    - hosts:\n      - \n      secretName:\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/ingress/ui.yaml"}, "region": {"startLine": 3, "endLine": 28, "snippet": {"text": "apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  labels:\n    io.kompose.service: tls-terminator\n  annotations:\n    cert-manager.io/cluster-issuer: null\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/service-upstream: \"\"\n    nginx.ingress.kubernetes.io/upstream-vhost: \"\"\n  name: lxbs-ui\nspec:\n  rules:\n    - host: \n      http:\n        paths:\n        - backend:\n            service: \n              name: lxbs-ui\n              port: \n                number: 80\n          pathType: ImplementationSpecific\n  tls:\n    - hosts:\n      - \n      secretName:\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/ingress/layerindex.yaml"}, "region": {"startLine": 3, "endLine": 35, "snippet": {"text": "apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  labels:\n    io.kompose.service: tls-terminator\n  annotations:\n    cert-manager.io/cluster-issuer: null\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/service-upstream: \"\"\n  name: lxbs-layerindex\nspec:\n  rules:\n    - host: \n      http:\n        paths:\n        - path: /\n          pathType: ImplementationSpecific\n          backend:\n            service: \n              name: lxbs-layerindex\n              port: \n                number: 5000\n        - path: /static\n          pathType: ImplementationSpecific\n          backend:\n            service: \n              name: lxbs-layerindex\n              port: \n                number: 80\n  tls:\n    - hosts:\n      - \n      secretName:\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/ingress/wrlsvc.yaml"}, "region": {"startLine": 3, "endLine": 35, "snippet": {"text": "apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  labels:\n    io.kompose.service: tls-terminator\n  annotations:\n    cert-manager.io/cluster-issuer: null\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/service-upstream: \"\"\n  name: lxbs-metadata-svc\nspec:\n  rules:\n    - host: \n      http:\n        paths:\n        - path: /\n          pathType: ImplementationSpecific\n          backend:\n            service: \n              name: lxbs-metadata-svc\n              port: \n                number: 88\n        - path: /apis\n          pathType: ImplementationSpecific\n          backend:\n            service: \n              name: lxbs-metadata-svc\n              port: \n                number: 80\n  tls:\n    - hosts:\n      - \n      secretName:\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-gateway-test-connection\"\n  labels:\n     app: \"lxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-lxbs\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-gateway-test-connection\"\n  labels:\n     app: \"lxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-lxbs\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-gateway-test-connection\"\n  labels:\n     app: \"lxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-lxbs\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-gateway-test-connection\"\n  labels:\n     app: \"lxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-lxbs\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-gateway-test-connection\"\n  labels:\n     app: \"lxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-lxbs\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-gateway-test-connection\"\n  labels:\n     app: \"lxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-lxbs\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-gateway-test-connection\"\n  labels:\n     app: \"lxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-lxbs\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-gateway-test-connection\"\n  labels:\n     app: \"lxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-lxbs\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-gateway-test-connection\"\n  labels:\n     app: \"lxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-lxbs\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-gateway-test-connection\"\n  labels:\n     app: \"lxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-lxbs\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-gateway-test-connection\"\n  labels:\n     app: \"lxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-lxbs\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-gateway-test-connection\"\n  labels:\n     app: \"lxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-lxbs\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-gateway-test-connection\"\n  labels:\n     app: \"lxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-lxbs\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-gateway-test-connection\"\n  labels:\n     app: \"lxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-lxbs\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-gateway-test-connection\"\n  labels:\n     app: \"lxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-lxbs\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-gateway-test-connection\"\n  labels:\n     app: \"lxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-lxbs\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-gateway-test-connection\"\n  labels:\n     app: \"lxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-lxbs\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-gateway-test-connection\"\n  labels:\n     app: \"lxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-lxbs\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/apigateway.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-gateway-test-connection\"\n  labels:\n     app: \"lxbs-gateway-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-lxbs\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-gateway:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-ui-test-connection\"\n  labels:\n     app: \"lxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-ui-test-connection\"\n  labels:\n     app: \"lxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-ui-test-connection\"\n  labels:\n     app: \"lxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-ui-test-connection\"\n  labels:\n     app: \"lxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-ui-test-connection\"\n  labels:\n     app: \"lxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-ui-test-connection\"\n  labels:\n     app: \"lxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-ui-test-connection\"\n  labels:\n     app: \"lxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-ui-test-connection\"\n  labels:\n     app: \"lxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-ui-test-connection\"\n  labels:\n     app: \"lxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-ui-test-connection\"\n  labels:\n     app: \"lxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-ui-test-connection\"\n  labels:\n     app: \"lxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-ui-test-connection\"\n  labels:\n     app: \"lxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-ui-test-connection\"\n  labels:\n     app: \"lxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-ui-test-connection\"\n  labels:\n     app: \"lxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-ui-test-connection\"\n  labels:\n     app: \"lxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-ui-test-connection\"\n  labels:\n     app: \"lxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-ui-test-connection\"\n  labels:\n     app: \"lxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-ui-test-connection\"\n  labels:\n     app: \"lxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/tests/ui.yaml"}, "region": {"startLine": 10, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"lxbs-ui-test-connection\"\n  labels:\n     app: \"lxbs-ui-test-connection\"\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-weight\": \"2\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation, hook-succeeded\nspec:\n  containers:\n    - name: test-for-ui\n      image: system.registry.aws-us-east-2.devstar.cloud/wr-studio-support/internal/usp-helm-test:v1.0\n      command: ['curl']\n      args: ['lxbs-ui:80']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/serviceaccount/wrlsvc.yaml"}, "region": {"startLine": 3, "endLine": 7, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: lxbs-metadata-svc\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/serviceaccount/wrlsvc.yaml"}, "region": {"startLine": 9, "endLine": 20, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: lxbs-metadata-svc\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\", \"logs\", \"namespaces\", \"configmaps\"]\n  verbs: [\"get\", \"list\", \"watch\", \"delete\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/exec\", \"pods/log\"]\n  verbs: [\"get\", \"create\"]\n---\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/serviceaccount/wrlsvc.yaml"}, "region": {"startLine": 22, "endLine": 33, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: lxbs-metadata-svc\nsubjects:\n- kind: ServiceAccount\n  name: lxbs-metadata-svc\n  apiGroup: \"\"\nroleRef:\n  kind: Role\n  name: lxbs-metadata-svc\n  apiGroup: rbac.authorization.k8s.io\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: lxbs-init-db\n  namespace: default\n  labels:\n    app: lxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: lxbs-init-db\n      labels:\n        app: lxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"lxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: lxbs-init-db\n  namespace: default\n  labels:\n    app: lxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: lxbs-init-db\n      labels:\n        app: lxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"lxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: lxbs-init-db\n  namespace: default\n  labels:\n    app: lxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: lxbs-init-db\n      labels:\n        app: lxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"lxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: lxbs-init-db\n  namespace: default\n  labels:\n    app: lxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: lxbs-init-db\n      labels:\n        app: lxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"lxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: lxbs-init-db\n  namespace: default\n  labels:\n    app: lxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: lxbs-init-db\n      labels:\n        app: lxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"lxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: lxbs-init-db\n  namespace: default\n  labels:\n    app: lxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: lxbs-init-db\n      labels:\n        app: lxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"lxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: lxbs-init-db\n  namespace: default\n  labels:\n    app: lxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: lxbs-init-db\n      labels:\n        app: lxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"lxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: lxbs-init-db\n  namespace: default\n  labels:\n    app: lxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: lxbs-init-db\n      labels:\n        app: lxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"lxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: lxbs-init-db\n  namespace: default\n  labels:\n    app: lxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: lxbs-init-db\n      labels:\n        app: lxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"lxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: lxbs-init-db\n  namespace: default\n  labels:\n    app: lxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: lxbs-init-db\n      labels:\n        app: lxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"lxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: lxbs-init-db\n  namespace: default\n  labels:\n    app: lxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: lxbs-init-db\n      labels:\n        app: lxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"lxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: lxbs-init-db\n  namespace: default\n  labels:\n    app: lxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: lxbs-init-db\n      labels:\n        app: lxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"lxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: lxbs-init-db\n  namespace: default\n  labels:\n    app: lxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: lxbs-init-db\n      labels:\n        app: lxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"lxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: lxbs-init-db\n  namespace: default\n  labels:\n    app: lxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: lxbs-init-db\n      labels:\n        app: lxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"lxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: lxbs-init-db\n  namespace: default\n  labels:\n    app: lxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: lxbs-init-db\n      labels:\n        app: lxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"lxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: lxbs-init-db\n  namespace: default\n  labels:\n    app: lxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: lxbs-init-db\n      labels:\n        app: lxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"lxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: lxbs-init-db\n  namespace: default\n  labels:\n    app: lxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: lxbs-init-db\n      labels:\n        app: lxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"lxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/jobs/init-db.yaml"}, "region": {"startLine": 3, "endLine": 40, "snippet": {"text": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: lxbs-init-db\n  namespace: default\n  labels:\n    app: lxbs-init-db\n  annotations:\n    \"helm.sh/hook\": pre-install, pre-upgrade\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    metadata:\n      name: lxbs-init-db\n      labels:\n        app: lxbs-init-db\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccountName: \n      restartPolicy: \"OnFailure\"\n      containers:\n        - name: \"lxbs-init-db\"\n          image: \":1.16.0\"\n          imagePullPolicy: \n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: PG_USER\n              value: \"\"\n            - name: PG_HOST\n              value: \"\"\n            - name: PG_PORT\n              value: \"0\"\n            - name: PG_DBNAME\n              value: \"\"\n            - name: PG_SSL\n              value: \"disable\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/services/gateway.yaml"}, "region": {"startLine": 3, "endLine": 17, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: lxbs-gateway\n  annotations:\n    app.kubernetes.io/name: lxbs-gateway\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: 3000\n      protocol: TCP\n      name: tcp-gateway\n  selector:\n    app: lxbs-gateway\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/services/ui.yaml"}, "region": {"startLine": 3, "endLine": 17, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: lxbs-ui\n  annotations:\n    app.kubernetes.io/name: lxbs-ui \nspec:\n  type: ClusterIP\n  ports:\n    - name: http-ui\n      port: 80\n      protocol: TCP\n      targetPort: 8080\n  selector:\n    app: lxbs-ui\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/services/layerindex.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: lxbs-layerindex\n  labels:\n    app.kubernetes.io/name: lxbs-layerindex\nspec:\n  type: ClusterIP\n  ports:\n    - name: layersapp\n      port: 5000\n      protocol: TCP\n      targetPort: 5000\n    - name: http\n      port: 80\n      protocol: TCP\n      targetPort: 80\n  selector:\n    app: lxbs-layerindex\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/services/wrlsvc.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: lxbs-metadata-svc\n  labels:\n    app.kubernetes.io/name: lxbs-metadata-svc\nspec:\n  type: ClusterIP\n  ports:\n    - name: http-svc\n      port: 88\n      protocol: TCP\n      targetPort: 8088\n    - name: http-apis\n      port: 80\n      protocol: TCP\n      targetPort: 8080\n  selector:\n    app: lxbs-metadata-svc\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-project\n  labels:\n    app: lxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-project\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-project\n  labels:\n    app: lxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-project\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-project\n  labels:\n    app: lxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-project\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-project\n  labels:\n    app: lxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-project\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-project\n  labels:\n    app: lxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-project\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-project\n  labels:\n    app: lxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-project\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-project\n  labels:\n    app: lxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-project\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-project\n  labels:\n    app: lxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-project\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-project\n  labels:\n    app: lxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-project\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-project\n  labels:\n    app: lxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-project\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-project\n  labels:\n    app: lxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-project\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-project\n  labels:\n    app: lxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-project\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-project\n  labels:\n    app: lxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-project\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-project\n  labels:\n    app: lxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-project\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/project.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-project\n  labels:\n    app: lxbs-project\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-project\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-project\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: project\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-project\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-policy\n  labels:\n    app: lxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-policy\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-policy\n  labels:\n    app: lxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-policy\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-policy\n  labels:\n    app: lxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-policy\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-policy\n  labels:\n    app: lxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-policy\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-policy\n  labels:\n    app: lxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-policy\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-policy\n  labels:\n    app: lxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-policy\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-policy\n  labels:\n    app: lxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-policy\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-policy\n  labels:\n    app: lxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-policy\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-policy\n  labels:\n    app: lxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-policy\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-policy\n  labels:\n    app: lxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-policy\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-policy\n  labels:\n    app: lxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-policy\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-policy\n  labels:\n    app: lxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-policy\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-policy\n  labels:\n    app: lxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-policy\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-policy\n  labels:\n    app: lxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-policy\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/policy.yaml"}, "region": {"startLine": 3, "endLine": 47, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-policy\n  labels:\n    app: lxbs-policy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-policy\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-policy\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: policy\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-policy\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-sync\n  labels:\n    app: lxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-sync\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-sync\n  labels:\n    app: lxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-sync\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-sync\n  labels:\n    app: lxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-sync\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-sync\n  labels:\n    app: lxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-sync\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-sync\n  labels:\n    app: lxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-sync\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-sync\n  labels:\n    app: lxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-sync\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-sync\n  labels:\n    app: lxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-sync\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-sync\n  labels:\n    app: lxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-sync\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-sync\n  labels:\n    app: lxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-sync\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-sync\n  labels:\n    app: lxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-sync\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-sync\n  labels:\n    app: lxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-sync\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-sync\n  labels:\n    app: lxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-sync\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-sync\n  labels:\n    app: lxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-sync\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-sync\n  labels:\n    app: lxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-sync\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/sync.yaml"}, "region": {"startLine": 3, "endLine": 53, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-sync\n  labels:\n    app: lxbs-sync\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-sync\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-sync\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: sync\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-sync\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-repository\n  labels:\n    app: lxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-repository\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: repository\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-repository\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-repository\n  labels:\n    app: lxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-repository\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: repository\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-repository\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-repository\n  labels:\n    app: lxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-repository\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: repository\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-repository\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-repository\n  labels:\n    app: lxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-repository\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: repository\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-repository\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-repository\n  labels:\n    app: lxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-repository\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: repository\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-repository\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-repository\n  labels:\n    app: lxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-repository\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: repository\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-repository\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-repository\n  labels:\n    app: lxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-repository\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: repository\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-repository\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-repository\n  labels:\n    app: lxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-repository\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: repository\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-repository\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-repository\n  labels:\n    app: lxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-repository\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: repository\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-repository\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-repository\n  labels:\n    app: lxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-repository\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: repository\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-repository\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-repository\n  labels:\n    app: lxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-repository\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: repository\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-repository\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-repository\n  labels:\n    app: lxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-repository\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: repository\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-repository\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-repository\n  labels:\n    app: lxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-repository\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: repository\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-repository\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-repository\n  labels:\n    app: lxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-repository\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: repository\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-repository\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/repository.yaml"}, "region": {"startLine": 3, "endLine": 51, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-repository\n  labels:\n    app: lxbs-repository\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-repository\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-repository\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: repository\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-repository\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-gateway\n  labels:\n    app: lxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-gateway\n  template:\n    metadata:\n      labels:\n        app: lxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-gateway\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-gateway\n  labels:\n    app: lxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-gateway\n  template:\n    metadata:\n      labels:\n        app: lxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-gateway\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-gateway\n  labels:\n    app: lxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-gateway\n  template:\n    metadata:\n      labels:\n        app: lxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-gateway\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-gateway\n  labels:\n    app: lxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-gateway\n  template:\n    metadata:\n      labels:\n        app: lxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-gateway\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-gateway\n  labels:\n    app: lxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-gateway\n  template:\n    metadata:\n      labels:\n        app: lxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-gateway\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-gateway\n  labels:\n    app: lxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-gateway\n  template:\n    metadata:\n      labels:\n        app: lxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-gateway\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-gateway\n  labels:\n    app: lxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-gateway\n  template:\n    metadata:\n      labels:\n        app: lxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-gateway\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-gateway\n  labels:\n    app: lxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-gateway\n  template:\n    metadata:\n      labels:\n        app: lxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-gateway\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-gateway\n  labels:\n    app: lxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-gateway\n  template:\n    metadata:\n      labels:\n        app: lxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-gateway\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-gateway\n  labels:\n    app: lxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-gateway\n  template:\n    metadata:\n      labels:\n        app: lxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-gateway\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-gateway\n  labels:\n    app: lxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-gateway\n  template:\n    metadata:\n      labels:\n        app: lxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-gateway\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-gateway\n  labels:\n    app: lxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-gateway\n  template:\n    metadata:\n      labels:\n        app: lxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-gateway\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-gateway\n  labels:\n    app: lxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-gateway\n  template:\n    metadata:\n      labels:\n        app: lxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-gateway\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-gateway\n  labels:\n    app: lxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-gateway\n  template:\n    metadata:\n      labels:\n        app: lxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-gateway\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/gateway.yaml"}, "region": {"startLine": 3, "endLine": 46, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-gateway\n  labels:\n    app: lxbs-gateway\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-gateway\n  template:\n    metadata:\n      labels:\n        app: lxbs-gateway\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: gateway\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-gateway\"\n            - name: NODE_CFG_AUTH\n              value: \"{\\\"authApiUrl\\\":\\\"\\\",\\\"authEndpoint\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 37, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-ui\n  labels:\n    app: lxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-ui\n  template:\n    metadata:\n      labels:\n        app: lxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 37, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-ui\n  labels:\n    app: lxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-ui\n  template:\n    metadata:\n      labels:\n        app: lxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 37, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-ui\n  labels:\n    app: lxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-ui\n  template:\n    metadata:\n      labels:\n        app: lxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 37, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-ui\n  labels:\n    app: lxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-ui\n  template:\n    metadata:\n      labels:\n        app: lxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 37, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-ui\n  labels:\n    app: lxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-ui\n  template:\n    metadata:\n      labels:\n        app: lxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 37, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-ui\n  labels:\n    app: lxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-ui\n  template:\n    metadata:\n      labels:\n        app: lxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 37, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-ui\n  labels:\n    app: lxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-ui\n  template:\n    metadata:\n      labels:\n        app: lxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 37, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-ui\n  labels:\n    app: lxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-ui\n  template:\n    metadata:\n      labels:\n        app: lxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 37, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-ui\n  labels:\n    app: lxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-ui\n  template:\n    metadata:\n      labels:\n        app: lxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 37, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-ui\n  labels:\n    app: lxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-ui\n  template:\n    metadata:\n      labels:\n        app: lxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 37, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-ui\n  labels:\n    app: lxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-ui\n  template:\n    metadata:\n      labels:\n        app: lxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 37, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-ui\n  labels:\n    app: lxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-ui\n  template:\n    metadata:\n      labels:\n        app: lxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 37, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-ui\n  labels:\n    app: lxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-ui\n  template:\n    metadata:\n      labels:\n        app: lxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 37, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-ui\n  labels:\n    app: lxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-ui\n  template:\n    metadata:\n      labels:\n        app: lxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/ui.yaml"}, "region": {"startLine": 3, "endLine": 37, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-ui\n  labels:\n    app: lxbs-ui\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-ui\n  template:\n    metadata:\n      labels:\n        app: lxbs-ui\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: ui\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n          env:\n            - name: USP_ENV_ANGULAR_CONFIG\n              value: \"{\\\"apiUrl\\\":\\\"\\\",\\\"authGuardFuncSuccess\\\":null,\\\"authMsUrl\\\":\\\"\\\",\\\"signOutFunc\\\":null,\\\"tozIdClientId\\\":\\\"\\\",\\\"tozIdHostName\\\":\\\"\\\",\\\"tozIdRealmName\\\":\\\"\\\"}\"\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/layerindex.yaml"}, "region": {"startLine": 3, "endLine": 132, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-layerindex\n  labels:\n    app: lxbs-layerindex\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-layerindex\n  template:\n    metadata:\n      labels:\n        app: lxbs-layerindex\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: layersapp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5000\n          securityContext:\n            runAsUser: 500\n            runAsGroup: 500\n          env:\n          - name: LAYERINDEX_INIT\n            value: \"yes\"\n          - name: LAYERINDEX_ADMIN\n            value: \"admin\"\n          - name: LAYERINDEX_ADMIN_EMAIL\n            value: \"admin@localhost\"\n          - name: LAYERINDEX_ADMIN_PASS\n            value: \"admin\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n          command: [\"/bin/sh\", \"-c\"]\n          args:\n            - set +x;\n              cd /opt/layerindex;\n              while true;\n              do if ! /usr/local/bin/python3 manage.py check;\n                  then /usr/local/bin/python3 manage.py migrate;\n                else\n                  break;\n                fi;\n                sleep 60;\n              done;\n              /entrypoint.sh\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /opt/layers\n          #  subPath: layerindex/layers_cache\n        - name: celery-worker\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          env:\n          - name: RUN_CELERY\n            value: \"true\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: mariadb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          args:\n            - --character-set-server=utf8mb4\n            - --collation-server=utf8mb4_unicode_ci\n          ports:\n          - containerPort: 3306\n          env:\n          - name: MYSQL_ROOT_PASSWORD\n            value: root\n          - name: MYSQL_DATABASE\n            value: layerindex\n          - name: MYSQL_USER\n            value: oelayer\n          - name: MYSQL_PASSWORD\n            value: oelayer\n          - name: LANG\n            value: en_US.UTF-8\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /var/lib/mysql\n          #  subPath: mariadb\n        - name: rabbitmq\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5672\n          - containerPort: 15672\n          env:\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: layersweb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 80\n          resources:\n            {}\n          volumeMounts:\n          - name: metadata-volume\n            mountPath: /data\n          #- name: metadata-volume\n          #  mountPath: /usr/local/apache2/htdocs/static\n          #  subPath: layerindex/static\n          #  readOnly: true\n      volumes:\n        - name: metadata-volume\n          persistentVolumeClaim:\n            claimName: storage-wrlinux-metadata\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/layerindex.yaml"}, "region": {"startLine": 3, "endLine": 132, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-layerindex\n  labels:\n    app: lxbs-layerindex\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-layerindex\n  template:\n    metadata:\n      labels:\n        app: lxbs-layerindex\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: layersapp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5000\n          securityContext:\n            runAsUser: 500\n            runAsGroup: 500\n          env:\n          - name: LAYERINDEX_INIT\n            value: \"yes\"\n          - name: LAYERINDEX_ADMIN\n            value: \"admin\"\n          - name: LAYERINDEX_ADMIN_EMAIL\n            value: \"admin@localhost\"\n          - name: LAYERINDEX_ADMIN_PASS\n            value: \"admin\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n          command: [\"/bin/sh\", \"-c\"]\n          args:\n            - set +x;\n              cd /opt/layerindex;\n              while true;\n              do if ! /usr/local/bin/python3 manage.py check;\n                  then /usr/local/bin/python3 manage.py migrate;\n                else\n                  break;\n                fi;\n                sleep 60;\n              done;\n              /entrypoint.sh\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /opt/layers\n          #  subPath: layerindex/layers_cache\n        - name: celery-worker\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          env:\n          - name: RUN_CELERY\n            value: \"true\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: mariadb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          args:\n            - --character-set-server=utf8mb4\n            - --collation-server=utf8mb4_unicode_ci\n          ports:\n          - containerPort: 3306\n          env:\n          - name: MYSQL_ROOT_PASSWORD\n            value: root\n          - name: MYSQL_DATABASE\n            value: layerindex\n          - name: MYSQL_USER\n            value: oelayer\n          - name: MYSQL_PASSWORD\n            value: oelayer\n          - name: LANG\n            value: en_US.UTF-8\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /var/lib/mysql\n          #  subPath: mariadb\n        - name: rabbitmq\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5672\n          - containerPort: 15672\n          env:\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: layersweb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 80\n          resources:\n            {}\n          volumeMounts:\n          - name: metadata-volume\n            mountPath: /data\n          #- name: metadata-volume\n          #  mountPath: /usr/local/apache2/htdocs/static\n          #  subPath: layerindex/static\n          #  readOnly: true\n      volumes:\n        - name: metadata-volume\n          persistentVolumeClaim:\n            claimName: storage-wrlinux-metadata\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/layerindex.yaml"}, "region": {"startLine": 3, "endLine": 132, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-layerindex\n  labels:\n    app: lxbs-layerindex\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-layerindex\n  template:\n    metadata:\n      labels:\n        app: lxbs-layerindex\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: layersapp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5000\n          securityContext:\n            runAsUser: 500\n            runAsGroup: 500\n          env:\n          - name: LAYERINDEX_INIT\n            value: \"yes\"\n          - name: LAYERINDEX_ADMIN\n            value: \"admin\"\n          - name: LAYERINDEX_ADMIN_EMAIL\n            value: \"admin@localhost\"\n          - name: LAYERINDEX_ADMIN_PASS\n            value: \"admin\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n          command: [\"/bin/sh\", \"-c\"]\n          args:\n            - set +x;\n              cd /opt/layerindex;\n              while true;\n              do if ! /usr/local/bin/python3 manage.py check;\n                  then /usr/local/bin/python3 manage.py migrate;\n                else\n                  break;\n                fi;\n                sleep 60;\n              done;\n              /entrypoint.sh\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /opt/layers\n          #  subPath: layerindex/layers_cache\n        - name: celery-worker\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          env:\n          - name: RUN_CELERY\n            value: \"true\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: mariadb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          args:\n            - --character-set-server=utf8mb4\n            - --collation-server=utf8mb4_unicode_ci\n          ports:\n          - containerPort: 3306\n          env:\n          - name: MYSQL_ROOT_PASSWORD\n            value: root\n          - name: MYSQL_DATABASE\n            value: layerindex\n          - name: MYSQL_USER\n            value: oelayer\n          - name: MYSQL_PASSWORD\n            value: oelayer\n          - name: LANG\n            value: en_US.UTF-8\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /var/lib/mysql\n          #  subPath: mariadb\n        - name: rabbitmq\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5672\n          - containerPort: 15672\n          env:\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: layersweb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 80\n          resources:\n            {}\n          volumeMounts:\n          - name: metadata-volume\n            mountPath: /data\n          #- name: metadata-volume\n          #  mountPath: /usr/local/apache2/htdocs/static\n          #  subPath: layerindex/static\n          #  readOnly: true\n      volumes:\n        - name: metadata-volume\n          persistentVolumeClaim:\n            claimName: storage-wrlinux-metadata\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/layerindex.yaml"}, "region": {"startLine": 3, "endLine": 132, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-layerindex\n  labels:\n    app: lxbs-layerindex\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-layerindex\n  template:\n    metadata:\n      labels:\n        app: lxbs-layerindex\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: layersapp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5000\n          securityContext:\n            runAsUser: 500\n            runAsGroup: 500\n          env:\n          - name: LAYERINDEX_INIT\n            value: \"yes\"\n          - name: LAYERINDEX_ADMIN\n            value: \"admin\"\n          - name: LAYERINDEX_ADMIN_EMAIL\n            value: \"admin@localhost\"\n          - name: LAYERINDEX_ADMIN_PASS\n            value: \"admin\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n          command: [\"/bin/sh\", \"-c\"]\n          args:\n            - set +x;\n              cd /opt/layerindex;\n              while true;\n              do if ! /usr/local/bin/python3 manage.py check;\n                  then /usr/local/bin/python3 manage.py migrate;\n                else\n                  break;\n                fi;\n                sleep 60;\n              done;\n              /entrypoint.sh\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /opt/layers\n          #  subPath: layerindex/layers_cache\n        - name: celery-worker\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          env:\n          - name: RUN_CELERY\n            value: \"true\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: mariadb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          args:\n            - --character-set-server=utf8mb4\n            - --collation-server=utf8mb4_unicode_ci\n          ports:\n          - containerPort: 3306\n          env:\n          - name: MYSQL_ROOT_PASSWORD\n            value: root\n          - name: MYSQL_DATABASE\n            value: layerindex\n          - name: MYSQL_USER\n            value: oelayer\n          - name: MYSQL_PASSWORD\n            value: oelayer\n          - name: LANG\n            value: en_US.UTF-8\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /var/lib/mysql\n          #  subPath: mariadb\n        - name: rabbitmq\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5672\n          - containerPort: 15672\n          env:\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: layersweb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 80\n          resources:\n            {}\n          volumeMounts:\n          - name: metadata-volume\n            mountPath: /data\n          #- name: metadata-volume\n          #  mountPath: /usr/local/apache2/htdocs/static\n          #  subPath: layerindex/static\n          #  readOnly: true\n      volumes:\n        - name: metadata-volume\n          persistentVolumeClaim:\n            claimName: storage-wrlinux-metadata\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/layerindex.yaml"}, "region": {"startLine": 3, "endLine": 132, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-layerindex\n  labels:\n    app: lxbs-layerindex\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-layerindex\n  template:\n    metadata:\n      labels:\n        app: lxbs-layerindex\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: layersapp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5000\n          securityContext:\n            runAsUser: 500\n            runAsGroup: 500\n          env:\n          - name: LAYERINDEX_INIT\n            value: \"yes\"\n          - name: LAYERINDEX_ADMIN\n            value: \"admin\"\n          - name: LAYERINDEX_ADMIN_EMAIL\n            value: \"admin@localhost\"\n          - name: LAYERINDEX_ADMIN_PASS\n            value: \"admin\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n          command: [\"/bin/sh\", \"-c\"]\n          args:\n            - set +x;\n              cd /opt/layerindex;\n              while true;\n              do if ! /usr/local/bin/python3 manage.py check;\n                  then /usr/local/bin/python3 manage.py migrate;\n                else\n                  break;\n                fi;\n                sleep 60;\n              done;\n              /entrypoint.sh\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /opt/layers\n          #  subPath: layerindex/layers_cache\n        - name: celery-worker\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          env:\n          - name: RUN_CELERY\n            value: \"true\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: mariadb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          args:\n            - --character-set-server=utf8mb4\n            - --collation-server=utf8mb4_unicode_ci\n          ports:\n          - containerPort: 3306\n          env:\n          - name: MYSQL_ROOT_PASSWORD\n            value: root\n          - name: MYSQL_DATABASE\n            value: layerindex\n          - name: MYSQL_USER\n            value: oelayer\n          - name: MYSQL_PASSWORD\n            value: oelayer\n          - name: LANG\n            value: en_US.UTF-8\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /var/lib/mysql\n          #  subPath: mariadb\n        - name: rabbitmq\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5672\n          - containerPort: 15672\n          env:\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: layersweb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 80\n          resources:\n            {}\n          volumeMounts:\n          - name: metadata-volume\n            mountPath: /data\n          #- name: metadata-volume\n          #  mountPath: /usr/local/apache2/htdocs/static\n          #  subPath: layerindex/static\n          #  readOnly: true\n      volumes:\n        - name: metadata-volume\n          persistentVolumeClaim:\n            claimName: storage-wrlinux-metadata\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/layerindex.yaml"}, "region": {"startLine": 3, "endLine": 132, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-layerindex\n  labels:\n    app: lxbs-layerindex\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-layerindex\n  template:\n    metadata:\n      labels:\n        app: lxbs-layerindex\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: layersapp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5000\n          securityContext:\n            runAsUser: 500\n            runAsGroup: 500\n          env:\n          - name: LAYERINDEX_INIT\n            value: \"yes\"\n          - name: LAYERINDEX_ADMIN\n            value: \"admin\"\n          - name: LAYERINDEX_ADMIN_EMAIL\n            value: \"admin@localhost\"\n          - name: LAYERINDEX_ADMIN_PASS\n            value: \"admin\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n          command: [\"/bin/sh\", \"-c\"]\n          args:\n            - set +x;\n              cd /opt/layerindex;\n              while true;\n              do if ! /usr/local/bin/python3 manage.py check;\n                  then /usr/local/bin/python3 manage.py migrate;\n                else\n                  break;\n                fi;\n                sleep 60;\n              done;\n              /entrypoint.sh\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /opt/layers\n          #  subPath: layerindex/layers_cache\n        - name: celery-worker\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          env:\n          - name: RUN_CELERY\n            value: \"true\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: mariadb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          args:\n            - --character-set-server=utf8mb4\n            - --collation-server=utf8mb4_unicode_ci\n          ports:\n          - containerPort: 3306\n          env:\n          - name: MYSQL_ROOT_PASSWORD\n            value: root\n          - name: MYSQL_DATABASE\n            value: layerindex\n          - name: MYSQL_USER\n            value: oelayer\n          - name: MYSQL_PASSWORD\n            value: oelayer\n          - name: LANG\n            value: en_US.UTF-8\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /var/lib/mysql\n          #  subPath: mariadb\n        - name: rabbitmq\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5672\n          - containerPort: 15672\n          env:\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: layersweb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 80\n          resources:\n            {}\n          volumeMounts:\n          - name: metadata-volume\n            mountPath: /data\n          #- name: metadata-volume\n          #  mountPath: /usr/local/apache2/htdocs/static\n          #  subPath: layerindex/static\n          #  readOnly: true\n      volumes:\n        - name: metadata-volume\n          persistentVolumeClaim:\n            claimName: storage-wrlinux-metadata\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/layerindex.yaml"}, "region": {"startLine": 3, "endLine": 132, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-layerindex\n  labels:\n    app: lxbs-layerindex\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-layerindex\n  template:\n    metadata:\n      labels:\n        app: lxbs-layerindex\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: layersapp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5000\n          securityContext:\n            runAsUser: 500\n            runAsGroup: 500\n          env:\n          - name: LAYERINDEX_INIT\n            value: \"yes\"\n          - name: LAYERINDEX_ADMIN\n            value: \"admin\"\n          - name: LAYERINDEX_ADMIN_EMAIL\n            value: \"admin@localhost\"\n          - name: LAYERINDEX_ADMIN_PASS\n            value: \"admin\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n          command: [\"/bin/sh\", \"-c\"]\n          args:\n            - set +x;\n              cd /opt/layerindex;\n              while true;\n              do if ! /usr/local/bin/python3 manage.py check;\n                  then /usr/local/bin/python3 manage.py migrate;\n                else\n                  break;\n                fi;\n                sleep 60;\n              done;\n              /entrypoint.sh\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /opt/layers\n          #  subPath: layerindex/layers_cache\n        - name: celery-worker\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          env:\n          - name: RUN_CELERY\n            value: \"true\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: mariadb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          args:\n            - --character-set-server=utf8mb4\n            - --collation-server=utf8mb4_unicode_ci\n          ports:\n          - containerPort: 3306\n          env:\n          - name: MYSQL_ROOT_PASSWORD\n            value: root\n          - name: MYSQL_DATABASE\n            value: layerindex\n          - name: MYSQL_USER\n            value: oelayer\n          - name: MYSQL_PASSWORD\n            value: oelayer\n          - name: LANG\n            value: en_US.UTF-8\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /var/lib/mysql\n          #  subPath: mariadb\n        - name: rabbitmq\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5672\n          - containerPort: 15672\n          env:\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: layersweb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 80\n          resources:\n            {}\n          volumeMounts:\n          - name: metadata-volume\n            mountPath: /data\n          #- name: metadata-volume\n          #  mountPath: /usr/local/apache2/htdocs/static\n          #  subPath: layerindex/static\n          #  readOnly: true\n      volumes:\n        - name: metadata-volume\n          persistentVolumeClaim:\n            claimName: storage-wrlinux-metadata\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/layerindex.yaml"}, "region": {"startLine": 3, "endLine": 132, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-layerindex\n  labels:\n    app: lxbs-layerindex\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-layerindex\n  template:\n    metadata:\n      labels:\n        app: lxbs-layerindex\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: layersapp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5000\n          securityContext:\n            runAsUser: 500\n            runAsGroup: 500\n          env:\n          - name: LAYERINDEX_INIT\n            value: \"yes\"\n          - name: LAYERINDEX_ADMIN\n            value: \"admin\"\n          - name: LAYERINDEX_ADMIN_EMAIL\n            value: \"admin@localhost\"\n          - name: LAYERINDEX_ADMIN_PASS\n            value: \"admin\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n          command: [\"/bin/sh\", \"-c\"]\n          args:\n            - set +x;\n              cd /opt/layerindex;\n              while true;\n              do if ! /usr/local/bin/python3 manage.py check;\n                  then /usr/local/bin/python3 manage.py migrate;\n                else\n                  break;\n                fi;\n                sleep 60;\n              done;\n              /entrypoint.sh\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /opt/layers\n          #  subPath: layerindex/layers_cache\n        - name: celery-worker\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          env:\n          - name: RUN_CELERY\n            value: \"true\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: mariadb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          args:\n            - --character-set-server=utf8mb4\n            - --collation-server=utf8mb4_unicode_ci\n          ports:\n          - containerPort: 3306\n          env:\n          - name: MYSQL_ROOT_PASSWORD\n            value: root\n          - name: MYSQL_DATABASE\n            value: layerindex\n          - name: MYSQL_USER\n            value: oelayer\n          - name: MYSQL_PASSWORD\n            value: oelayer\n          - name: LANG\n            value: en_US.UTF-8\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /var/lib/mysql\n          #  subPath: mariadb\n        - name: rabbitmq\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5672\n          - containerPort: 15672\n          env:\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: layersweb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 80\n          resources:\n            {}\n          volumeMounts:\n          - name: metadata-volume\n            mountPath: /data\n          #- name: metadata-volume\n          #  mountPath: /usr/local/apache2/htdocs/static\n          #  subPath: layerindex/static\n          #  readOnly: true\n      volumes:\n        - name: metadata-volume\n          persistentVolumeClaim:\n            claimName: storage-wrlinux-metadata\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/layerindex.yaml"}, "region": {"startLine": 3, "endLine": 132, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-layerindex\n  labels:\n    app: lxbs-layerindex\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-layerindex\n  template:\n    metadata:\n      labels:\n        app: lxbs-layerindex\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: layersapp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5000\n          securityContext:\n            runAsUser: 500\n            runAsGroup: 500\n          env:\n          - name: LAYERINDEX_INIT\n            value: \"yes\"\n          - name: LAYERINDEX_ADMIN\n            value: \"admin\"\n          - name: LAYERINDEX_ADMIN_EMAIL\n            value: \"admin@localhost\"\n          - name: LAYERINDEX_ADMIN_PASS\n            value: \"admin\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n          command: [\"/bin/sh\", \"-c\"]\n          args:\n            - set +x;\n              cd /opt/layerindex;\n              while true;\n              do if ! /usr/local/bin/python3 manage.py check;\n                  then /usr/local/bin/python3 manage.py migrate;\n                else\n                  break;\n                fi;\n                sleep 60;\n              done;\n              /entrypoint.sh\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /opt/layers\n          #  subPath: layerindex/layers_cache\n        - name: celery-worker\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          env:\n          - name: RUN_CELERY\n            value: \"true\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: mariadb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          args:\n            - --character-set-server=utf8mb4\n            - --collation-server=utf8mb4_unicode_ci\n          ports:\n          - containerPort: 3306\n          env:\n          - name: MYSQL_ROOT_PASSWORD\n            value: root\n          - name: MYSQL_DATABASE\n            value: layerindex\n          - name: MYSQL_USER\n            value: oelayer\n          - name: MYSQL_PASSWORD\n            value: oelayer\n          - name: LANG\n            value: en_US.UTF-8\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /var/lib/mysql\n          #  subPath: mariadb\n        - name: rabbitmq\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5672\n          - containerPort: 15672\n          env:\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: layersweb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 80\n          resources:\n            {}\n          volumeMounts:\n          - name: metadata-volume\n            mountPath: /data\n          #- name: metadata-volume\n          #  mountPath: /usr/local/apache2/htdocs/static\n          #  subPath: layerindex/static\n          #  readOnly: true\n      volumes:\n        - name: metadata-volume\n          persistentVolumeClaim:\n            claimName: storage-wrlinux-metadata\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/layerindex.yaml"}, "region": {"startLine": 3, "endLine": 132, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-layerindex\n  labels:\n    app: lxbs-layerindex\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-layerindex\n  template:\n    metadata:\n      labels:\n        app: lxbs-layerindex\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: layersapp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5000\n          securityContext:\n            runAsUser: 500\n            runAsGroup: 500\n          env:\n          - name: LAYERINDEX_INIT\n            value: \"yes\"\n          - name: LAYERINDEX_ADMIN\n            value: \"admin\"\n          - name: LAYERINDEX_ADMIN_EMAIL\n            value: \"admin@localhost\"\n          - name: LAYERINDEX_ADMIN_PASS\n            value: \"admin\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n          command: [\"/bin/sh\", \"-c\"]\n          args:\n            - set +x;\n              cd /opt/layerindex;\n              while true;\n              do if ! /usr/local/bin/python3 manage.py check;\n                  then /usr/local/bin/python3 manage.py migrate;\n                else\n                  break;\n                fi;\n                sleep 60;\n              done;\n              /entrypoint.sh\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /opt/layers\n          #  subPath: layerindex/layers_cache\n        - name: celery-worker\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          env:\n          - name: RUN_CELERY\n            value: \"true\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: mariadb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          args:\n            - --character-set-server=utf8mb4\n            - --collation-server=utf8mb4_unicode_ci\n          ports:\n          - containerPort: 3306\n          env:\n          - name: MYSQL_ROOT_PASSWORD\n            value: root\n          - name: MYSQL_DATABASE\n            value: layerindex\n          - name: MYSQL_USER\n            value: oelayer\n          - name: MYSQL_PASSWORD\n            value: oelayer\n          - name: LANG\n            value: en_US.UTF-8\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /var/lib/mysql\n          #  subPath: mariadb\n        - name: rabbitmq\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5672\n          - containerPort: 15672\n          env:\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: layersweb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 80\n          resources:\n            {}\n          volumeMounts:\n          - name: metadata-volume\n            mountPath: /data\n          #- name: metadata-volume\n          #  mountPath: /usr/local/apache2/htdocs/static\n          #  subPath: layerindex/static\n          #  readOnly: true\n      volumes:\n        - name: metadata-volume\n          persistentVolumeClaim:\n            claimName: storage-wrlinux-metadata\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/layerindex.yaml"}, "region": {"startLine": 3, "endLine": 132, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-layerindex\n  labels:\n    app: lxbs-layerindex\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-layerindex\n  template:\n    metadata:\n      labels:\n        app: lxbs-layerindex\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: layersapp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5000\n          securityContext:\n            runAsUser: 500\n            runAsGroup: 500\n          env:\n          - name: LAYERINDEX_INIT\n            value: \"yes\"\n          - name: LAYERINDEX_ADMIN\n            value: \"admin\"\n          - name: LAYERINDEX_ADMIN_EMAIL\n            value: \"admin@localhost\"\n          - name: LAYERINDEX_ADMIN_PASS\n            value: \"admin\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n          command: [\"/bin/sh\", \"-c\"]\n          args:\n            - set +x;\n              cd /opt/layerindex;\n              while true;\n              do if ! /usr/local/bin/python3 manage.py check;\n                  then /usr/local/bin/python3 manage.py migrate;\n                else\n                  break;\n                fi;\n                sleep 60;\n              done;\n              /entrypoint.sh\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /opt/layers\n          #  subPath: layerindex/layers_cache\n        - name: celery-worker\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          env:\n          - name: RUN_CELERY\n            value: \"true\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: mariadb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          args:\n            - --character-set-server=utf8mb4\n            - --collation-server=utf8mb4_unicode_ci\n          ports:\n          - containerPort: 3306\n          env:\n          - name: MYSQL_ROOT_PASSWORD\n            value: root\n          - name: MYSQL_DATABASE\n            value: layerindex\n          - name: MYSQL_USER\n            value: oelayer\n          - name: MYSQL_PASSWORD\n            value: oelayer\n          - name: LANG\n            value: en_US.UTF-8\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /var/lib/mysql\n          #  subPath: mariadb\n        - name: rabbitmq\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5672\n          - containerPort: 15672\n          env:\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: layersweb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 80\n          resources:\n            {}\n          volumeMounts:\n          - name: metadata-volume\n            mountPath: /data\n          #- name: metadata-volume\n          #  mountPath: /usr/local/apache2/htdocs/static\n          #  subPath: layerindex/static\n          #  readOnly: true\n      volumes:\n        - name: metadata-volume\n          persistentVolumeClaim:\n            claimName: storage-wrlinux-metadata\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/layerindex.yaml"}, "region": {"startLine": 3, "endLine": 132, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-layerindex\n  labels:\n    app: lxbs-layerindex\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-layerindex\n  template:\n    metadata:\n      labels:\n        app: lxbs-layerindex\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: layersapp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5000\n          securityContext:\n            runAsUser: 500\n            runAsGroup: 500\n          env:\n          - name: LAYERINDEX_INIT\n            value: \"yes\"\n          - name: LAYERINDEX_ADMIN\n            value: \"admin\"\n          - name: LAYERINDEX_ADMIN_EMAIL\n            value: \"admin@localhost\"\n          - name: LAYERINDEX_ADMIN_PASS\n            value: \"admin\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n          command: [\"/bin/sh\", \"-c\"]\n          args:\n            - set +x;\n              cd /opt/layerindex;\n              while true;\n              do if ! /usr/local/bin/python3 manage.py check;\n                  then /usr/local/bin/python3 manage.py migrate;\n                else\n                  break;\n                fi;\n                sleep 60;\n              done;\n              /entrypoint.sh\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /opt/layers\n          #  subPath: layerindex/layers_cache\n        - name: celery-worker\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          env:\n          - name: RUN_CELERY\n            value: \"true\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: mariadb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          args:\n            - --character-set-server=utf8mb4\n            - --collation-server=utf8mb4_unicode_ci\n          ports:\n          - containerPort: 3306\n          env:\n          - name: MYSQL_ROOT_PASSWORD\n            value: root\n          - name: MYSQL_DATABASE\n            value: layerindex\n          - name: MYSQL_USER\n            value: oelayer\n          - name: MYSQL_PASSWORD\n            value: oelayer\n          - name: LANG\n            value: en_US.UTF-8\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /var/lib/mysql\n          #  subPath: mariadb\n        - name: rabbitmq\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5672\n          - containerPort: 15672\n          env:\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: layersweb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 80\n          resources:\n            {}\n          volumeMounts:\n          - name: metadata-volume\n            mountPath: /data\n          #- name: metadata-volume\n          #  mountPath: /usr/local/apache2/htdocs/static\n          #  subPath: layerindex/static\n          #  readOnly: true\n      volumes:\n        - name: metadata-volume\n          persistentVolumeClaim:\n            claimName: storage-wrlinux-metadata\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/layerindex.yaml"}, "region": {"startLine": 3, "endLine": 132, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-layerindex\n  labels:\n    app: lxbs-layerindex\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-layerindex\n  template:\n    metadata:\n      labels:\n        app: lxbs-layerindex\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: layersapp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5000\n          securityContext:\n            runAsUser: 500\n            runAsGroup: 500\n          env:\n          - name: LAYERINDEX_INIT\n            value: \"yes\"\n          - name: LAYERINDEX_ADMIN\n            value: \"admin\"\n          - name: LAYERINDEX_ADMIN_EMAIL\n            value: \"admin@localhost\"\n          - name: LAYERINDEX_ADMIN_PASS\n            value: \"admin\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n          command: [\"/bin/sh\", \"-c\"]\n          args:\n            - set +x;\n              cd /opt/layerindex;\n              while true;\n              do if ! /usr/local/bin/python3 manage.py check;\n                  then /usr/local/bin/python3 manage.py migrate;\n                else\n                  break;\n                fi;\n                sleep 60;\n              done;\n              /entrypoint.sh\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /opt/layers\n          #  subPath: layerindex/layers_cache\n        - name: celery-worker\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          env:\n          - name: RUN_CELERY\n            value: \"true\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: mariadb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          args:\n            - --character-set-server=utf8mb4\n            - --collation-server=utf8mb4_unicode_ci\n          ports:\n          - containerPort: 3306\n          env:\n          - name: MYSQL_ROOT_PASSWORD\n            value: root\n          - name: MYSQL_DATABASE\n            value: layerindex\n          - name: MYSQL_USER\n            value: oelayer\n          - name: MYSQL_PASSWORD\n            value: oelayer\n          - name: LANG\n            value: en_US.UTF-8\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /var/lib/mysql\n          #  subPath: mariadb\n        - name: rabbitmq\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5672\n          - containerPort: 15672\n          env:\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: layersweb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 80\n          resources:\n            {}\n          volumeMounts:\n          - name: metadata-volume\n            mountPath: /data\n          #- name: metadata-volume\n          #  mountPath: /usr/local/apache2/htdocs/static\n          #  subPath: layerindex/static\n          #  readOnly: true\n      volumes:\n        - name: metadata-volume\n          persistentVolumeClaim:\n            claimName: storage-wrlinux-metadata\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/layerindex.yaml"}, "region": {"startLine": 3, "endLine": 132, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-layerindex\n  labels:\n    app: lxbs-layerindex\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-layerindex\n  template:\n    metadata:\n      labels:\n        app: lxbs-layerindex\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: layersapp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5000\n          securityContext:\n            runAsUser: 500\n            runAsGroup: 500\n          env:\n          - name: LAYERINDEX_INIT\n            value: \"yes\"\n          - name: LAYERINDEX_ADMIN\n            value: \"admin\"\n          - name: LAYERINDEX_ADMIN_EMAIL\n            value: \"admin@localhost\"\n          - name: LAYERINDEX_ADMIN_PASS\n            value: \"admin\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n          command: [\"/bin/sh\", \"-c\"]\n          args:\n            - set +x;\n              cd /opt/layerindex;\n              while true;\n              do if ! /usr/local/bin/python3 manage.py check;\n                  then /usr/local/bin/python3 manage.py migrate;\n                else\n                  break;\n                fi;\n                sleep 60;\n              done;\n              /entrypoint.sh\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /opt/layers\n          #  subPath: layerindex/layers_cache\n        - name: celery-worker\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          env:\n          - name: RUN_CELERY\n            value: \"true\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: mariadb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          args:\n            - --character-set-server=utf8mb4\n            - --collation-server=utf8mb4_unicode_ci\n          ports:\n          - containerPort: 3306\n          env:\n          - name: MYSQL_ROOT_PASSWORD\n            value: root\n          - name: MYSQL_DATABASE\n            value: layerindex\n          - name: MYSQL_USER\n            value: oelayer\n          - name: MYSQL_PASSWORD\n            value: oelayer\n          - name: LANG\n            value: en_US.UTF-8\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /var/lib/mysql\n          #  subPath: mariadb\n        - name: rabbitmq\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5672\n          - containerPort: 15672\n          env:\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: layersweb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 80\n          resources:\n            {}\n          volumeMounts:\n          - name: metadata-volume\n            mountPath: /data\n          #- name: metadata-volume\n          #  mountPath: /usr/local/apache2/htdocs/static\n          #  subPath: layerindex/static\n          #  readOnly: true\n      volumes:\n        - name: metadata-volume\n          persistentVolumeClaim:\n            claimName: storage-wrlinux-metadata\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/layerindex.yaml"}, "region": {"startLine": 3, "endLine": 132, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-layerindex\n  labels:\n    app: lxbs-layerindex\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-layerindex\n  template:\n    metadata:\n      labels:\n        app: lxbs-layerindex\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: layersapp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5000\n          securityContext:\n            runAsUser: 500\n            runAsGroup: 500\n          env:\n          - name: LAYERINDEX_INIT\n            value: \"yes\"\n          - name: LAYERINDEX_ADMIN\n            value: \"admin\"\n          - name: LAYERINDEX_ADMIN_EMAIL\n            value: \"admin@localhost\"\n          - name: LAYERINDEX_ADMIN_PASS\n            value: \"admin\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n          command: [\"/bin/sh\", \"-c\"]\n          args:\n            - set +x;\n              cd /opt/layerindex;\n              while true;\n              do if ! /usr/local/bin/python3 manage.py check;\n                  then /usr/local/bin/python3 manage.py migrate;\n                else\n                  break;\n                fi;\n                sleep 60;\n              done;\n              /entrypoint.sh\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /opt/layers\n          #  subPath: layerindex/layers_cache\n        - name: celery-worker\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          env:\n          - name: RUN_CELERY\n            value: \"true\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: mariadb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          args:\n            - --character-set-server=utf8mb4\n            - --collation-server=utf8mb4_unicode_ci\n          ports:\n          - containerPort: 3306\n          env:\n          - name: MYSQL_ROOT_PASSWORD\n            value: root\n          - name: MYSQL_DATABASE\n            value: layerindex\n          - name: MYSQL_USER\n            value: oelayer\n          - name: MYSQL_PASSWORD\n            value: oelayer\n          - name: LANG\n            value: en_US.UTF-8\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /var/lib/mysql\n          #  subPath: mariadb\n        - name: rabbitmq\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5672\n          - containerPort: 15672\n          env:\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: layersweb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 80\n          resources:\n            {}\n          volumeMounts:\n          - name: metadata-volume\n            mountPath: /data\n          #- name: metadata-volume\n          #  mountPath: /usr/local/apache2/htdocs/static\n          #  subPath: layerindex/static\n          #  readOnly: true\n      volumes:\n        - name: metadata-volume\n          persistentVolumeClaim:\n            claimName: storage-wrlinux-metadata\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/layerindex.yaml"}, "region": {"startLine": 3, "endLine": 132, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-layerindex\n  labels:\n    app: lxbs-layerindex\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-layerindex\n  template:\n    metadata:\n      labels:\n        app: lxbs-layerindex\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: layersapp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5000\n          securityContext:\n            runAsUser: 500\n            runAsGroup: 500\n          env:\n          - name: LAYERINDEX_INIT\n            value: \"yes\"\n          - name: LAYERINDEX_ADMIN\n            value: \"admin\"\n          - name: LAYERINDEX_ADMIN_EMAIL\n            value: \"admin@localhost\"\n          - name: LAYERINDEX_ADMIN_PASS\n            value: \"admin\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n          command: [\"/bin/sh\", \"-c\"]\n          args:\n            - set +x;\n              cd /opt/layerindex;\n              while true;\n              do if ! /usr/local/bin/python3 manage.py check;\n                  then /usr/local/bin/python3 manage.py migrate;\n                else\n                  break;\n                fi;\n                sleep 60;\n              done;\n              /entrypoint.sh\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /opt/layers\n          #  subPath: layerindex/layers_cache\n        - name: celery-worker\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          env:\n          - name: RUN_CELERY\n            value: \"true\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: mariadb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          args:\n            - --character-set-server=utf8mb4\n            - --collation-server=utf8mb4_unicode_ci\n          ports:\n          - containerPort: 3306\n          env:\n          - name: MYSQL_ROOT_PASSWORD\n            value: root\n          - name: MYSQL_DATABASE\n            value: layerindex\n          - name: MYSQL_USER\n            value: oelayer\n          - name: MYSQL_PASSWORD\n            value: oelayer\n          - name: LANG\n            value: en_US.UTF-8\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /var/lib/mysql\n          #  subPath: mariadb\n        - name: rabbitmq\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5672\n          - containerPort: 15672\n          env:\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: layersweb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 80\n          resources:\n            {}\n          volumeMounts:\n          - name: metadata-volume\n            mountPath: /data\n          #- name: metadata-volume\n          #  mountPath: /usr/local/apache2/htdocs/static\n          #  subPath: layerindex/static\n          #  readOnly: true\n      volumes:\n        - name: metadata-volume\n          persistentVolumeClaim:\n            claimName: storage-wrlinux-metadata\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/layerindex.yaml"}, "region": {"startLine": 3, "endLine": 132, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-layerindex\n  labels:\n    app: lxbs-layerindex\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-layerindex\n  template:\n    metadata:\n      labels:\n        app: lxbs-layerindex\n    spec:\n      # serviceAccountName: default\n      securityContext:\n        {}\n      containers:\n        - name: layersapp\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5000\n          securityContext:\n            runAsUser: 500\n            runAsGroup: 500\n          env:\n          - name: LAYERINDEX_INIT\n            value: \"yes\"\n          - name: LAYERINDEX_ADMIN\n            value: \"admin\"\n          - name: LAYERINDEX_ADMIN_EMAIL\n            value: \"admin@localhost\"\n          - name: LAYERINDEX_ADMIN_PASS\n            value: \"admin\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n          command: [\"/bin/sh\", \"-c\"]\n          args:\n            - set +x;\n              cd /opt/layerindex;\n              while true;\n              do if ! /usr/local/bin/python3 manage.py check;\n                  then /usr/local/bin/python3 manage.py migrate;\n                else\n                  break;\n                fi;\n                sleep 60;\n              done;\n              /entrypoint.sh\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /opt/layers\n          #  subPath: layerindex/layers_cache\n        - name: celery-worker\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          env:\n          - name: RUN_CELERY\n            value: \"true\"\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: mariadb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          args:\n            - --character-set-server=utf8mb4\n            - --collation-server=utf8mb4_unicode_ci\n          ports:\n          - containerPort: 3306\n          env:\n          - name: MYSQL_ROOT_PASSWORD\n            value: root\n          - name: MYSQL_DATABASE\n            value: layerindex\n          - name: MYSQL_USER\n            value: oelayer\n          - name: MYSQL_PASSWORD\n            value: oelayer\n          - name: LANG\n            value: en_US.UTF-8\n          #volumeMounts:\n          #- name: metadata-volume\n          #  mountPath: /var/lib/mysql\n          #  subPath: mariadb\n        - name: rabbitmq\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 5672\n          - containerPort: 15672\n          env:\n          - name: RABBITMQ_DEFAULT_USER\n            value: guest\n          - name: RABBITMQ_DEFAULT_PASS\n            value: guest\n        - name: layersweb\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          ports:\n          - containerPort: 80\n          resources:\n            {}\n          volumeMounts:\n          - name: metadata-volume\n            mountPath: /data\n          #- name: metadata-volume\n          #  mountPath: /usr/local/apache2/htdocs/static\n          #  subPath: layerindex/static\n          #  readOnly: true\n      volumes:\n        - name: metadata-volume\n          persistentVolumeClaim:\n            claimName: storage-wrlinux-metadata\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 59, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-build\n  labels:\n    app: lxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-build\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"artifactLocation\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 59, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-build\n  labels:\n    app: lxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-build\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"artifactLocation\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 59, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-build\n  labels:\n    app: lxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-build\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"artifactLocation\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 59, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-build\n  labels:\n    app: lxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-build\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"artifactLocation\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 59, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-build\n  labels:\n    app: lxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-build\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"artifactLocation\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 59, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-build\n  labels:\n    app: lxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-build\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"artifactLocation\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 59, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-build\n  labels:\n    app: lxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-build\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"artifactLocation\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 59, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-build\n  labels:\n    app: lxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-build\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"artifactLocation\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 59, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-build\n  labels:\n    app: lxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-build\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"artifactLocation\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 59, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-build\n  labels:\n    app: lxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-build\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"artifactLocation\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 59, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-build\n  labels:\n    app: lxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-build\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"artifactLocation\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 59, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-build\n  labels:\n    app: lxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-build\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"artifactLocation\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 59, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-build\n  labels:\n    app: lxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-build\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"artifactLocation\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 59, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-build\n  labels:\n    app: lxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-build\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"artifactLocation\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/lxbs/templates/deployments/build.yaml"}, "region": {"startLine": 3, "endLine": 59, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lxbs-build\n  labels:\n    app: lxbs-build\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lxbs-build\n  template:\n    metadata:\n      annotations:\n      labels:\n        app: lxbs-build\n    spec:\n      serviceAccountName: \n      securityContext:\n        {}\n      containers:\n        - name: build\n          securityContext:\n            {}\n          image: \":\"\n          imagePullPolicy: \n          command: [\"npm\", \"run\",start:%!s(<nil>)]\n          env:\n            - name: VAULT_ENABLE\n              value: \n            - name: MS_NAME\n              value: \"lxbs-build\"\n            - name: NODE_CFG_JENKINS\n              value: \"{\\\"baseUrl\\\":\\\"\\\",\\\"jobName\\\":\\\"\\\"}\"\n            - name: NODE_CFG_JENKINS_SECRETS\n              value: \"{\\\"password\\\":\\\"\\\",\\\"username\\\":\\\"\\\"}\"\n            - name: NODE_CFG_GITLAB\n              value: \"{\\\"apiBaseUrl\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PROJECT_INFO\n              value: \"{\\\"customLayerAPI\\\":\\\"\\\",\\\"layersRepoBaseUrl\\\":\\\"\\\",\\\"url\\\":\\\"\\\",\\\"wrLinuxMetadata\\\":\\\"\\\"}\"\n            - name: NODE_CFG_PG\n              value: \"{\\\"database\\\":\\\"\\\",\\\"host\\\":\\\"\\\",\\\"port\\\":0,\\\"ssl\\\":\\\"disable\\\",\\\"user\\\":\\\"\\\"}\"\n            - name: NODE_CFG_KAFKA\n              value: \"{\\\"clientId\\\":\\\"\\\",\\\"cluster\\\":[\\\"\\\"]}\"\n            - name: NODE_CFG_ARTIFACTS\n              value: \"{\\\"artifactLocation\\\":\\\"\\\",\\\"baseUrl\\\":\\\"\\\",\\\"bucket\\\":\\\"\\\",\\\"folderPath\\\":\\\"\\\"}\"\n            - name: NODE_CFG_LOG\n              value: \"{\\\"logLevel\\\":\\\"\\\",\\\"writeLog\\\":null}\"\n            - name: NODE_CFG_SYNC\n              value: \"{\\\"checkTimeoutTimer\\\":\\\"\\\",\\\"maxExpiryTimeInHours\\\":null,\\\"policyTimer\\\":\\\"\\\",\\\"syncBuildTimer\\\":\\\"\\\"}\"\n          # livenessProbe:\n          #   exec:\n          #     command:\n          #       - cat\n          #       - /tmp/healthy\n          resources:\n            {}\n"}}}}]}, {"ruleId": "CKV_K8S_156", "ruleIndex": 36, "level": "error", "attachments": [], "message": {"text": "Minimize ClusterRoles that grant permissions to approve CertificateSigningRequests"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/base/templates/clusterrole.yaml"}, "region": {"startLine": 3, "endLine": 101, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: istiod-istio-system\n  labels:\n    app: istiod\n    release: base\nrules:\n  # sidecar injection controller\n  - apiGroups: [\"admissionregistration.k8s.io\"]\n    resources: [\"mutatingwebhookconfigurations\"]\n    verbs: [\"get\", \"list\", \"watch\", \"update\", \"patch\"]\n\n  # configuration validation webhook controller\n  - apiGroups: [\"admissionregistration.k8s.io\"]\n    resources: [\"validatingwebhookconfigurations\"]\n    verbs: [\"get\", \"list\", \"watch\", \"update\"]\n\n  # istio configuration\n  # removing CRD permissions can break older versions of Istio running alongside this control plane (https://github.com/istio/istio/issues/29382)\n  # please proceed with caution\n  - apiGroups: [\"config.istio.io\", \"security.istio.io\", \"networking.istio.io\", \"authentication.istio.io\", \"rbac.istio.io\", \"telemetry.istio.io\"]\n    verbs: [\"get\", \"watch\", \"list\"]\n    resources: [\"*\"]\n  - apiGroups: [\"networking.istio.io\"]\n    verbs: [ \"get\", \"watch\", \"list\", \"update\", \"patch\", \"create\", \"delete\" ]\n    resources: [ \"workloadentries\" ]\n  - apiGroups: [\"networking.istio.io\"]\n    verbs: [ \"get\", \"watch\", \"list\", \"update\", \"patch\", \"create\", \"delete\" ]\n    resources: [ \"workloadentries/status\" ]\n\n  # auto-detect installed CRD definitions\n  - apiGroups: [\"apiextensions.k8s.io\"]\n    resources: [\"customresourcedefinitions\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n\n  # discovery and routing\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"nodes\", \"services\", \"namespaces\", \"endpoints\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"discovery.k8s.io\"]\n    resources: [\"endpointslices\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n\n  # ingress controller\n  - apiGroups: [\"networking.k8s.io\"]\n    resources: [\"ingresses\", \"ingressclasses\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"networking.k8s.io\"]\n    resources: [\"ingresses/status\"]\n    verbs: [\"*\"]\n\n  # required for CA's namespace controller\n  - apiGroups: [\"\"]\n    resources: [\"configmaps\"]\n    verbs: [\"create\", \"get\", \"list\", \"watch\", \"update\"]\n\n  # Istiod and bootstrap.\n  - apiGroups: [\"certificates.k8s.io\"]\n    resources:\n      - \"certificatesigningrequests\"\n      - \"certificatesigningrequests/approval\"\n      - \"certificatesigningrequests/status\"\n    verbs: [\"update\", \"create\", \"get\", \"delete\", \"watch\"]\n  - apiGroups: [\"certificates.k8s.io\"]\n    resources:\n      - \"signers\"\n    resourceNames:\n    - \"kubernetes.io/legacy-unknown\"\n    verbs: [\"approve\"]\n\n  # Used by Istiod to verify the JWT tokens\n  - apiGroups: [\"authentication.k8s.io\"]\n    resources: [\"tokenreviews\"]\n    verbs: [\"create\"]\n\n  # Used by Istiod to verify gateway SDS\n  - apiGroups: [\"authorization.k8s.io\"]\n    resources: [\"subjectaccessreviews\"]\n    verbs: [\"create\"]\n\n  # Use for Kubernetes Service APIs\n  - apiGroups: [\"networking.x-k8s.io\"]\n    resources: [\"*\"]\n    verbs: [\"get\", \"watch\", \"list\"]\n  - apiGroups: [\"networking.x-k8s.io\"]\n    resources: [\"*\"] # TODO: should be on just */status but wildcard is not supported\n    verbs: [\"update\"]\n\n  # Needed for multicluster secret reading, possibly ingress certs in the future\n  - apiGroups: [\"\"]\n    resources: [\"secrets\"]\n    verbs: [\"get\", \"watch\", \"list\"]\n\n  # Used for MCS serviceexport management\n  - apiGroups: [\"multicluster.x-k8s.io\"]\n    resources: [\"serviceexports\"]\n    verbs: [\"get\", \"watch\", \"list\", \"create\", \"delete\"]\n---\n"}}}}]}, {"ruleId": "CKV_K8S_155", "ruleIndex": 25, "level": "error", "attachments": [], "message": {"text": "Minimize ClusterRoles that grant control over validating or mutating admission webhook configurations"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/base/templates/clusterrole.yaml"}, "region": {"startLine": 3, "endLine": 101, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: istiod-istio-system\n  labels:\n    app: istiod\n    release: base\nrules:\n  # sidecar injection controller\n  - apiGroups: [\"admissionregistration.k8s.io\"]\n    resources: [\"mutatingwebhookconfigurations\"]\n    verbs: [\"get\", \"list\", \"watch\", \"update\", \"patch\"]\n\n  # configuration validation webhook controller\n  - apiGroups: [\"admissionregistration.k8s.io\"]\n    resources: [\"validatingwebhookconfigurations\"]\n    verbs: [\"get\", \"list\", \"watch\", \"update\"]\n\n  # istio configuration\n  # removing CRD permissions can break older versions of Istio running alongside this control plane (https://github.com/istio/istio/issues/29382)\n  # please proceed with caution\n  - apiGroups: [\"config.istio.io\", \"security.istio.io\", \"networking.istio.io\", \"authentication.istio.io\", \"rbac.istio.io\", \"telemetry.istio.io\"]\n    verbs: [\"get\", \"watch\", \"list\"]\n    resources: [\"*\"]\n  - apiGroups: [\"networking.istio.io\"]\n    verbs: [ \"get\", \"watch\", \"list\", \"update\", \"patch\", \"create\", \"delete\" ]\n    resources: [ \"workloadentries\" ]\n  - apiGroups: [\"networking.istio.io\"]\n    verbs: [ \"get\", \"watch\", \"list\", \"update\", \"patch\", \"create\", \"delete\" ]\n    resources: [ \"workloadentries/status\" ]\n\n  # auto-detect installed CRD definitions\n  - apiGroups: [\"apiextensions.k8s.io\"]\n    resources: [\"customresourcedefinitions\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n\n  # discovery and routing\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"nodes\", \"services\", \"namespaces\", \"endpoints\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"discovery.k8s.io\"]\n    resources: [\"endpointslices\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n\n  # ingress controller\n  - apiGroups: [\"networking.k8s.io\"]\n    resources: [\"ingresses\", \"ingressclasses\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"networking.k8s.io\"]\n    resources: [\"ingresses/status\"]\n    verbs: [\"*\"]\n\n  # required for CA's namespace controller\n  - apiGroups: [\"\"]\n    resources: [\"configmaps\"]\n    verbs: [\"create\", \"get\", \"list\", \"watch\", \"update\"]\n\n  # Istiod and bootstrap.\n  - apiGroups: [\"certificates.k8s.io\"]\n    resources:\n      - \"certificatesigningrequests\"\n      - \"certificatesigningrequests/approval\"\n      - \"certificatesigningrequests/status\"\n    verbs: [\"update\", \"create\", \"get\", \"delete\", \"watch\"]\n  - apiGroups: [\"certificates.k8s.io\"]\n    resources:\n      - \"signers\"\n    resourceNames:\n    - \"kubernetes.io/legacy-unknown\"\n    verbs: [\"approve\"]\n\n  # Used by Istiod to verify the JWT tokens\n  - apiGroups: [\"authentication.k8s.io\"]\n    resources: [\"tokenreviews\"]\n    verbs: [\"create\"]\n\n  # Used by Istiod to verify gateway SDS\n  - apiGroups: [\"authorization.k8s.io\"]\n    resources: [\"subjectaccessreviews\"]\n    verbs: [\"create\"]\n\n  # Use for Kubernetes Service APIs\n  - apiGroups: [\"networking.x-k8s.io\"]\n    resources: [\"*\"]\n    verbs: [\"get\", \"watch\", \"list\"]\n  - apiGroups: [\"networking.x-k8s.io\"]\n    resources: [\"*\"] # TODO: should be on just */status but wildcard is not supported\n    verbs: [\"update\"]\n\n  # Needed for multicluster secret reading, possibly ingress certs in the future\n  - apiGroups: [\"\"]\n    resources: [\"secrets\"]\n    verbs: [\"get\", \"watch\", \"list\"]\n\n  # Used for MCS serviceexport management\n  - apiGroups: [\"multicluster.x-k8s.io\"]\n    resources: [\"serviceexports\"]\n    verbs: [\"get\", \"watch\", \"list\", \"create\", \"delete\"]\n---\n"}}}}]}, {"ruleId": "CKV_K8S_49", "ruleIndex": 24, "level": "error", "attachments": [], "message": {"text": "Minimize wildcard use in Roles and ClusterRoles"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/base/templates/clusterrole.yaml"}, "region": {"startLine": 103, "endLine": 139, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: istio-reader-istio-system\n  labels:\n    app: istio-reader\n    release: base\nrules:\n  - apiGroups:\n      - \"config.istio.io\"\n      - \"security.istio.io\"\n      - \"networking.istio.io\"\n      - \"authentication.istio.io\"\n      - \"rbac.istio.io\"\n    resources: [\"*\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"\"]\n    resources: [\"endpoints\", \"pods\", \"services\", \"nodes\", \"replicationcontrollers\", \"namespaces\", \"secrets\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"networking.istio.io\"]\n    verbs: [ \"get\", \"watch\", \"list\" ]\n    resources: [ \"workloadentries\" ]\n  - apiGroups: [\"apiextensions.k8s.io\"]\n    resources: [\"customresourcedefinitions\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"discovery.k8s.io\"]\n    resources: [\"endpointslices\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"apps\"]\n    resources: [\"replicasets\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"authentication.k8s.io\"]\n    resources: [\"tokenreviews\"]\n    verbs: [\"create\"]\n  - apiGroups: [\"authorization.k8s.io\"]\n    resources: [\"subjectaccessreviews\"]\n    verbs: [\"create\"]\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/controller-serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 16, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: ingress-nginx\n  namespace: default\nautomountServiceAccountToken: true\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/controller-deployment.yaml"}, "region": {"startLine": 3, "endLine": 116, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: ingress-nginx\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: controller\n          image: \"registry.k8s.io/ingress-nginx/controller:v1.2.1\"\n          imagePullPolicy: IfNotPresent\n          lifecycle: \n            preStop:\n              exec:\n                command:\n                - /wait-shutdown\n          args:\n            - /nginx-ingress-controller\n            - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --election-id=ingress-controller-leader\n            - --controller-class=k8s.io/ingress-nginx\n            - --ingress-class=nginx\n            - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --validating-webhook=:8443\n            - --validating-webhook-certificate=/usr/local/certificates/cert\n            - --validating-webhook-key=/usr/local/certificates/key\n          securityContext: \n            capabilities:\n              drop:\n              - ALL\n              add:\n              - NET_BIND_SERVICE\n            runAsUser: 101\n            allowPrivilegeEscalation: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: LD_PRELOAD\n              value: /usr/local/lib/libmimalloc.so\n          livenessProbe: \n            failureThreshold: 5\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          readinessProbe: \n            failureThreshold: 3\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n            - name: https\n              containerPort: 443\n              protocol: TCP\n            - name: webhook\n              containerPort: 8443\n              protocol: TCP\n          volumeMounts:\n            - name: webhook-cert\n              mountPath: /usr/local/certificates/\n              readOnly: true\n          resources: \n            requests:\n              cpu: 100m\n              memory: 90Mi\n      nodeSelector: \n        kubernetes.io/os: linux\n      serviceAccountName: ingress-nginx\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: webhook-cert\n          secret:\n            secretName: ingress-nginx-admission\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/controller-deployment.yaml"}, "region": {"startLine": 3, "endLine": 116, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: ingress-nginx\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: controller\n          image: \"registry.k8s.io/ingress-nginx/controller:v1.2.1\"\n          imagePullPolicy: IfNotPresent\n          lifecycle: \n            preStop:\n              exec:\n                command:\n                - /wait-shutdown\n          args:\n            - /nginx-ingress-controller\n            - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --election-id=ingress-controller-leader\n            - --controller-class=k8s.io/ingress-nginx\n            - --ingress-class=nginx\n            - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --validating-webhook=:8443\n            - --validating-webhook-certificate=/usr/local/certificates/cert\n            - --validating-webhook-key=/usr/local/certificates/key\n          securityContext: \n            capabilities:\n              drop:\n              - ALL\n              add:\n              - NET_BIND_SERVICE\n            runAsUser: 101\n            allowPrivilegeEscalation: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: LD_PRELOAD\n              value: /usr/local/lib/libmimalloc.so\n          livenessProbe: \n            failureThreshold: 5\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          readinessProbe: \n            failureThreshold: 3\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n            - name: https\n              containerPort: 443\n              protocol: TCP\n            - name: webhook\n              containerPort: 8443\n              protocol: TCP\n          volumeMounts:\n            - name: webhook-cert\n              mountPath: /usr/local/certificates/\n              readOnly: true\n          resources: \n            requests:\n              cpu: 100m\n              memory: 90Mi\n      nodeSelector: \n        kubernetes.io/os: linux\n      serviceAccountName: ingress-nginx\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: webhook-cert\n          secret:\n            secretName: ingress-nginx-admission\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/controller-deployment.yaml"}, "region": {"startLine": 3, "endLine": 116, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: ingress-nginx\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: controller\n          image: \"registry.k8s.io/ingress-nginx/controller:v1.2.1\"\n          imagePullPolicy: IfNotPresent\n          lifecycle: \n            preStop:\n              exec:\n                command:\n                - /wait-shutdown\n          args:\n            - /nginx-ingress-controller\n            - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --election-id=ingress-controller-leader\n            - --controller-class=k8s.io/ingress-nginx\n            - --ingress-class=nginx\n            - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --validating-webhook=:8443\n            - --validating-webhook-certificate=/usr/local/certificates/cert\n            - --validating-webhook-key=/usr/local/certificates/key\n          securityContext: \n            capabilities:\n              drop:\n              - ALL\n              add:\n              - NET_BIND_SERVICE\n            runAsUser: 101\n            allowPrivilegeEscalation: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: LD_PRELOAD\n              value: /usr/local/lib/libmimalloc.so\n          livenessProbe: \n            failureThreshold: 5\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          readinessProbe: \n            failureThreshold: 3\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n            - name: https\n              containerPort: 443\n              protocol: TCP\n            - name: webhook\n              containerPort: 8443\n              protocol: TCP\n          volumeMounts:\n            - name: webhook-cert\n              mountPath: /usr/local/certificates/\n              readOnly: true\n          resources: \n            requests:\n              cpu: 100m\n              memory: 90Mi\n      nodeSelector: \n        kubernetes.io/os: linux\n      serviceAccountName: ingress-nginx\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: webhook-cert\n          secret:\n            secretName: ingress-nginx-admission\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/controller-deployment.yaml"}, "region": {"startLine": 3, "endLine": 116, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: ingress-nginx\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: controller\n          image: \"registry.k8s.io/ingress-nginx/controller:v1.2.1\"\n          imagePullPolicy: IfNotPresent\n          lifecycle: \n            preStop:\n              exec:\n                command:\n                - /wait-shutdown\n          args:\n            - /nginx-ingress-controller\n            - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --election-id=ingress-controller-leader\n            - --controller-class=k8s.io/ingress-nginx\n            - --ingress-class=nginx\n            - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --validating-webhook=:8443\n            - --validating-webhook-certificate=/usr/local/certificates/cert\n            - --validating-webhook-key=/usr/local/certificates/key\n          securityContext: \n            capabilities:\n              drop:\n              - ALL\n              add:\n              - NET_BIND_SERVICE\n            runAsUser: 101\n            allowPrivilegeEscalation: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: LD_PRELOAD\n              value: /usr/local/lib/libmimalloc.so\n          livenessProbe: \n            failureThreshold: 5\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          readinessProbe: \n            failureThreshold: 3\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n            - name: https\n              containerPort: 443\n              protocol: TCP\n            - name: webhook\n              containerPort: 8443\n              protocol: TCP\n          volumeMounts:\n            - name: webhook-cert\n              mountPath: /usr/local/certificates/\n              readOnly: true\n          resources: \n            requests:\n              cpu: 100m\n              memory: 90Mi\n      nodeSelector: \n        kubernetes.io/os: linux\n      serviceAccountName: ingress-nginx\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: webhook-cert\n          secret:\n            secretName: ingress-nginx-admission\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/controller-deployment.yaml"}, "region": {"startLine": 3, "endLine": 116, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: ingress-nginx\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: controller\n          image: \"registry.k8s.io/ingress-nginx/controller:v1.2.1\"\n          imagePullPolicy: IfNotPresent\n          lifecycle: \n            preStop:\n              exec:\n                command:\n                - /wait-shutdown\n          args:\n            - /nginx-ingress-controller\n            - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --election-id=ingress-controller-leader\n            - --controller-class=k8s.io/ingress-nginx\n            - --ingress-class=nginx\n            - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --validating-webhook=:8443\n            - --validating-webhook-certificate=/usr/local/certificates/cert\n            - --validating-webhook-key=/usr/local/certificates/key\n          securityContext: \n            capabilities:\n              drop:\n              - ALL\n              add:\n              - NET_BIND_SERVICE\n            runAsUser: 101\n            allowPrivilegeEscalation: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: LD_PRELOAD\n              value: /usr/local/lib/libmimalloc.so\n          livenessProbe: \n            failureThreshold: 5\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          readinessProbe: \n            failureThreshold: 3\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n            - name: https\n              containerPort: 443\n              protocol: TCP\n            - name: webhook\n              containerPort: 8443\n              protocol: TCP\n          volumeMounts:\n            - name: webhook-cert\n              mountPath: /usr/local/certificates/\n              readOnly: true\n          resources: \n            requests:\n              cpu: 100m\n              memory: 90Mi\n      nodeSelector: \n        kubernetes.io/os: linux\n      serviceAccountName: ingress-nginx\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: webhook-cert\n          secret:\n            secretName: ingress-nginx-admission\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/controller-deployment.yaml"}, "region": {"startLine": 3, "endLine": 116, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: ingress-nginx\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: controller\n          image: \"registry.k8s.io/ingress-nginx/controller:v1.2.1\"\n          imagePullPolicy: IfNotPresent\n          lifecycle: \n            preStop:\n              exec:\n                command:\n                - /wait-shutdown\n          args:\n            - /nginx-ingress-controller\n            - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --election-id=ingress-controller-leader\n            - --controller-class=k8s.io/ingress-nginx\n            - --ingress-class=nginx\n            - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --validating-webhook=:8443\n            - --validating-webhook-certificate=/usr/local/certificates/cert\n            - --validating-webhook-key=/usr/local/certificates/key\n          securityContext: \n            capabilities:\n              drop:\n              - ALL\n              add:\n              - NET_BIND_SERVICE\n            runAsUser: 101\n            allowPrivilegeEscalation: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: LD_PRELOAD\n              value: /usr/local/lib/libmimalloc.so\n          livenessProbe: \n            failureThreshold: 5\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          readinessProbe: \n            failureThreshold: 3\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n            - name: https\n              containerPort: 443\n              protocol: TCP\n            - name: webhook\n              containerPort: 8443\n              protocol: TCP\n          volumeMounts:\n            - name: webhook-cert\n              mountPath: /usr/local/certificates/\n              readOnly: true\n          resources: \n            requests:\n              cpu: 100m\n              memory: 90Mi\n      nodeSelector: \n        kubernetes.io/os: linux\n      serviceAccountName: ingress-nginx\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: webhook-cert\n          secret:\n            secretName: ingress-nginx-admission\n"}}}}]}, {"ruleId": "CKV_K8S_25", "ruleIndex": 37, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with added capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/controller-deployment.yaml"}, "region": {"startLine": 3, "endLine": 116, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: ingress-nginx\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: controller\n          image: \"registry.k8s.io/ingress-nginx/controller:v1.2.1\"\n          imagePullPolicy: IfNotPresent\n          lifecycle: \n            preStop:\n              exec:\n                command:\n                - /wait-shutdown\n          args:\n            - /nginx-ingress-controller\n            - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --election-id=ingress-controller-leader\n            - --controller-class=k8s.io/ingress-nginx\n            - --ingress-class=nginx\n            - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --validating-webhook=:8443\n            - --validating-webhook-certificate=/usr/local/certificates/cert\n            - --validating-webhook-key=/usr/local/certificates/key\n          securityContext: \n            capabilities:\n              drop:\n              - ALL\n              add:\n              - NET_BIND_SERVICE\n            runAsUser: 101\n            allowPrivilegeEscalation: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: LD_PRELOAD\n              value: /usr/local/lib/libmimalloc.so\n          livenessProbe: \n            failureThreshold: 5\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          readinessProbe: \n            failureThreshold: 3\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n            - name: https\n              containerPort: 443\n              protocol: TCP\n            - name: webhook\n              containerPort: 8443\n              protocol: TCP\n          volumeMounts:\n            - name: webhook-cert\n              mountPath: /usr/local/certificates/\n              readOnly: true\n          resources: \n            requests:\n              cpu: 100m\n              memory: 90Mi\n      nodeSelector: \n        kubernetes.io/os: linux\n      serviceAccountName: ingress-nginx\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: webhook-cert\n          secret:\n            secretName: ingress-nginx-admission\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/controller-deployment.yaml"}, "region": {"startLine": 3, "endLine": 116, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: ingress-nginx\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: controller\n          image: \"registry.k8s.io/ingress-nginx/controller:v1.2.1\"\n          imagePullPolicy: IfNotPresent\n          lifecycle: \n            preStop:\n              exec:\n                command:\n                - /wait-shutdown\n          args:\n            - /nginx-ingress-controller\n            - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --election-id=ingress-controller-leader\n            - --controller-class=k8s.io/ingress-nginx\n            - --ingress-class=nginx\n            - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --validating-webhook=:8443\n            - --validating-webhook-certificate=/usr/local/certificates/cert\n            - --validating-webhook-key=/usr/local/certificates/key\n          securityContext: \n            capabilities:\n              drop:\n              - ALL\n              add:\n              - NET_BIND_SERVICE\n            runAsUser: 101\n            allowPrivilegeEscalation: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: LD_PRELOAD\n              value: /usr/local/lib/libmimalloc.so\n          livenessProbe: \n            failureThreshold: 5\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          readinessProbe: \n            failureThreshold: 3\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n            - name: https\n              containerPort: 443\n              protocol: TCP\n            - name: webhook\n              containerPort: 8443\n              protocol: TCP\n          volumeMounts:\n            - name: webhook-cert\n              mountPath: /usr/local/certificates/\n              readOnly: true\n          resources: \n            requests:\n              cpu: 100m\n              memory: 90Mi\n      nodeSelector: \n        kubernetes.io/os: linux\n      serviceAccountName: ingress-nginx\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: webhook-cert\n          secret:\n            secretName: ingress-nginx-admission\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/controller-deployment.yaml"}, "region": {"startLine": 3, "endLine": 116, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: ingress-nginx\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: controller\n          image: \"registry.k8s.io/ingress-nginx/controller:v1.2.1\"\n          imagePullPolicy: IfNotPresent\n          lifecycle: \n            preStop:\n              exec:\n                command:\n                - /wait-shutdown\n          args:\n            - /nginx-ingress-controller\n            - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --election-id=ingress-controller-leader\n            - --controller-class=k8s.io/ingress-nginx\n            - --ingress-class=nginx\n            - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --validating-webhook=:8443\n            - --validating-webhook-certificate=/usr/local/certificates/cert\n            - --validating-webhook-key=/usr/local/certificates/key\n          securityContext: \n            capabilities:\n              drop:\n              - ALL\n              add:\n              - NET_BIND_SERVICE\n            runAsUser: 101\n            allowPrivilegeEscalation: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: LD_PRELOAD\n              value: /usr/local/lib/libmimalloc.so\n          livenessProbe: \n            failureThreshold: 5\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          readinessProbe: \n            failureThreshold: 3\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n            - name: https\n              containerPort: 443\n              protocol: TCP\n            - name: webhook\n              containerPort: 8443\n              protocol: TCP\n          volumeMounts:\n            - name: webhook-cert\n              mountPath: /usr/local/certificates/\n              readOnly: true\n          resources: \n            requests:\n              cpu: 100m\n              memory: 90Mi\n      nodeSelector: \n        kubernetes.io/os: linux\n      serviceAccountName: ingress-nginx\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: webhook-cert\n          secret:\n            secretName: ingress-nginx-admission\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/controller-deployment.yaml"}, "region": {"startLine": 3, "endLine": 116, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: ingress-nginx\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: controller\n          image: \"registry.k8s.io/ingress-nginx/controller:v1.2.1\"\n          imagePullPolicy: IfNotPresent\n          lifecycle: \n            preStop:\n              exec:\n                command:\n                - /wait-shutdown\n          args:\n            - /nginx-ingress-controller\n            - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --election-id=ingress-controller-leader\n            - --controller-class=k8s.io/ingress-nginx\n            - --ingress-class=nginx\n            - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --validating-webhook=:8443\n            - --validating-webhook-certificate=/usr/local/certificates/cert\n            - --validating-webhook-key=/usr/local/certificates/key\n          securityContext: \n            capabilities:\n              drop:\n              - ALL\n              add:\n              - NET_BIND_SERVICE\n            runAsUser: 101\n            allowPrivilegeEscalation: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: LD_PRELOAD\n              value: /usr/local/lib/libmimalloc.so\n          livenessProbe: \n            failureThreshold: 5\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          readinessProbe: \n            failureThreshold: 3\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n            - name: https\n              containerPort: 443\n              protocol: TCP\n            - name: webhook\n              containerPort: 8443\n              protocol: TCP\n          volumeMounts:\n            - name: webhook-cert\n              mountPath: /usr/local/certificates/\n              readOnly: true\n          resources: \n            requests:\n              cpu: 100m\n              memory: 90Mi\n      nodeSelector: \n        kubernetes.io/os: linux\n      serviceAccountName: ingress-nginx\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: webhook-cert\n          secret:\n            secretName: ingress-nginx-admission\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/controller-deployment.yaml"}, "region": {"startLine": 3, "endLine": 116, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: ingress-nginx\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: controller\n          image: \"registry.k8s.io/ingress-nginx/controller:v1.2.1\"\n          imagePullPolicy: IfNotPresent\n          lifecycle: \n            preStop:\n              exec:\n                command:\n                - /wait-shutdown\n          args:\n            - /nginx-ingress-controller\n            - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --election-id=ingress-controller-leader\n            - --controller-class=k8s.io/ingress-nginx\n            - --ingress-class=nginx\n            - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --validating-webhook=:8443\n            - --validating-webhook-certificate=/usr/local/certificates/cert\n            - --validating-webhook-key=/usr/local/certificates/key\n          securityContext: \n            capabilities:\n              drop:\n              - ALL\n              add:\n              - NET_BIND_SERVICE\n            runAsUser: 101\n            allowPrivilegeEscalation: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: LD_PRELOAD\n              value: /usr/local/lib/libmimalloc.so\n          livenessProbe: \n            failureThreshold: 5\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          readinessProbe: \n            failureThreshold: 3\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n            - name: https\n              containerPort: 443\n              protocol: TCP\n            - name: webhook\n              containerPort: 8443\n              protocol: TCP\n          volumeMounts:\n            - name: webhook-cert\n              mountPath: /usr/local/certificates/\n              readOnly: true\n          resources: \n            requests:\n              cpu: 100m\n              memory: 90Mi\n      nodeSelector: \n        kubernetes.io/os: linux\n      serviceAccountName: ingress-nginx\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: webhook-cert\n          secret:\n            secretName: ingress-nginx-admission\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/controller-deployment.yaml"}, "region": {"startLine": 3, "endLine": 116, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: ingress-nginx\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n        - name: controller\n          image: \"registry.k8s.io/ingress-nginx/controller:v1.2.1\"\n          imagePullPolicy: IfNotPresent\n          lifecycle: \n            preStop:\n              exec:\n                command:\n                - /wait-shutdown\n          args:\n            - /nginx-ingress-controller\n            - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --election-id=ingress-controller-leader\n            - --controller-class=k8s.io/ingress-nginx\n            - --ingress-class=nginx\n            - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller\n            - --validating-webhook=:8443\n            - --validating-webhook-certificate=/usr/local/certificates/cert\n            - --validating-webhook-key=/usr/local/certificates/key\n          securityContext: \n            capabilities:\n              drop:\n              - ALL\n              add:\n              - NET_BIND_SERVICE\n            runAsUser: 101\n            allowPrivilegeEscalation: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: LD_PRELOAD\n              value: /usr/local/lib/libmimalloc.so\n          livenessProbe: \n            failureThreshold: 5\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          readinessProbe: \n            failureThreshold: 3\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n            - name: https\n              containerPort: 443\n              protocol: TCP\n            - name: webhook\n              containerPort: 8443\n              protocol: TCP\n          volumeMounts:\n            - name: webhook-cert\n              mountPath: /usr/local/certificates/\n              readOnly: true\n          resources: \n            requests:\n              cpu: 100m\n              memory: 90Mi\n      nodeSelector: \n        kubernetes.io/os: linux\n      serviceAccountName: ingress-nginx\n      terminationGracePeriodSeconds: 300\n      volumes:\n        - name: webhook-cert\n          secret:\n            secretName: ingress-nginx-admission\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/controller-service-webhook.yaml"}, "region": {"startLine": 3, "endLine": 26, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: ingress-nginx-controller-admission\n  namespace: default\nspec:\n  type: ClusterIP\n  ports:\n    - name: https-webhook\n      port: 443\n      targetPort: webhook\n      appProtocol: https\n  selector:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/component: controller\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/controller-role.yaml"}, "region": {"startLine": 3, "endLine": 85, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: ingress-nginx\n  namespace: default\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - namespaces\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n      - pods\n      - secrets\n      - endpoints\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - services\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - networking.k8s.io\n    resources:\n      - ingresses\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - networking.k8s.io\n    resources:\n      - ingresses/status\n    verbs:\n      - update\n  - apiGroups:\n      - networking.k8s.io\n    resources:\n      - ingressclasses\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n    resourceNames:\n      - ingress-controller-leader\n    verbs:\n      - get\n      - update\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n    verbs:\n      - create\n  - apiGroups:\n      - \"\"\n    resources:\n      - events\n    verbs:\n      - create\n      - patch\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/controller-service.yaml"}, "region": {"startLine": 3, "endLine": 36, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: ingress-nginx-controller\n  namespace: default\nspec:\n  type: LoadBalancer\n  ipFamilyPolicy: SingleStack\n  ipFamilies: \n    - IPv4\n  ports:\n    - name: http\n      port: 80\n      protocol: TCP\n      targetPort: http\n      appProtocol: http\n    - name: https\n      port: 443\n      protocol: TCP\n      targetPort: https\n      appProtocol: https\n  selector:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/component: controller\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/controller-configmap.yaml"}, "region": {"startLine": 3, "endLine": 17, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: ingress-nginx-controller\n  namespace: default\ndata:\n  allow-snippet-annotations: \"true\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/controller-rolebinding.yaml"}, "region": {"startLine": 3, "endLine": 23, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: ingress-nginx\n  namespace: default\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: ingress-nginx\nsubjects:\n  - kind: ServiceAccount\n    name: ingress-nginx\n    namespace: \"default\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/role.yaml"}, "region": {"startLine": 3, "endLine": 26, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name:  ingress-nginx-admission\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade,post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - secrets\n    verbs:\n      - get\n      - create\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-createSecret.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: create\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - create\n            - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n            - --namespace=$(POD_NAMESPACE)\n            - --secret-name=ingress-nginx-admission\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-createSecret.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: create\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - create\n            - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n            - --namespace=$(POD_NAMESPACE)\n            - --secret-name=ingress-nginx-admission\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-createSecret.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: create\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - create\n            - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n            - --namespace=$(POD_NAMESPACE)\n            - --secret-name=ingress-nginx-admission\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-createSecret.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: create\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - create\n            - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n            - --namespace=$(POD_NAMESPACE)\n            - --secret-name=ingress-nginx-admission\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-createSecret.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: create\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - create\n            - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n            - --namespace=$(POD_NAMESPACE)\n            - --secret-name=ingress-nginx-admission\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-createSecret.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: create\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - create\n            - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n            - --namespace=$(POD_NAMESPACE)\n            - --secret-name=ingress-nginx-admission\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-createSecret.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: create\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - create\n            - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n            - --namespace=$(POD_NAMESPACE)\n            - --secret-name=ingress-nginx-admission\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-createSecret.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: create\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - create\n            - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n            - --namespace=$(POD_NAMESPACE)\n            - --secret-name=ingress-nginx-admission\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-createSecret.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: create\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - create\n            - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n            - --namespace=$(POD_NAMESPACE)\n            - --secret-name=ingress-nginx-admission\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-createSecret.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: create\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - create\n            - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n            - --namespace=$(POD_NAMESPACE)\n            - --secret-name=ingress-nginx-admission\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-createSecret.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: create\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - create\n            - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n            - --namespace=$(POD_NAMESPACE)\n            - --secret-name=ingress-nginx-admission\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-createSecret.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: create\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - create\n            - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n            - --namespace=$(POD_NAMESPACE)\n            - --secret-name=ingress-nginx-admission\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-createSecret.yaml"}, "region": {"startLine": 3, "endLine": 55, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: create\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - create\n            - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n            - --namespace=$(POD_NAMESPACE)\n            - --secret-name=ingress-nginx-admission\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-patchWebhook.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: patch\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - patch\n            - --webhook-name=ingress-nginx-admission\n            - --namespace=$(POD_NAMESPACE)\n            - --patch-mutating=false\n            - --secret-name=ingress-nginx-admission\n            - --patch-failure-policy=Fail\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-patchWebhook.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: patch\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - patch\n            - --webhook-name=ingress-nginx-admission\n            - --namespace=$(POD_NAMESPACE)\n            - --patch-mutating=false\n            - --secret-name=ingress-nginx-admission\n            - --patch-failure-policy=Fail\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-patchWebhook.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: patch\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - patch\n            - --webhook-name=ingress-nginx-admission\n            - --namespace=$(POD_NAMESPACE)\n            - --patch-mutating=false\n            - --secret-name=ingress-nginx-admission\n            - --patch-failure-policy=Fail\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-patchWebhook.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: patch\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - patch\n            - --webhook-name=ingress-nginx-admission\n            - --namespace=$(POD_NAMESPACE)\n            - --patch-mutating=false\n            - --secret-name=ingress-nginx-admission\n            - --patch-failure-policy=Fail\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-patchWebhook.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: patch\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - patch\n            - --webhook-name=ingress-nginx-admission\n            - --namespace=$(POD_NAMESPACE)\n            - --patch-mutating=false\n            - --secret-name=ingress-nginx-admission\n            - --patch-failure-policy=Fail\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-patchWebhook.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: patch\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - patch\n            - --webhook-name=ingress-nginx-admission\n            - --namespace=$(POD_NAMESPACE)\n            - --patch-mutating=false\n            - --secret-name=ingress-nginx-admission\n            - --patch-failure-policy=Fail\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-patchWebhook.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: patch\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - patch\n            - --webhook-name=ingress-nginx-admission\n            - --namespace=$(POD_NAMESPACE)\n            - --patch-mutating=false\n            - --secret-name=ingress-nginx-admission\n            - --patch-failure-policy=Fail\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-patchWebhook.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: patch\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - patch\n            - --webhook-name=ingress-nginx-admission\n            - --namespace=$(POD_NAMESPACE)\n            - --patch-mutating=false\n            - --secret-name=ingress-nginx-admission\n            - --patch-failure-policy=Fail\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-patchWebhook.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: patch\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - patch\n            - --webhook-name=ingress-nginx-admission\n            - --namespace=$(POD_NAMESPACE)\n            - --patch-mutating=false\n            - --secret-name=ingress-nginx-admission\n            - --patch-failure-policy=Fail\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-patchWebhook.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: patch\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - patch\n            - --webhook-name=ingress-nginx-admission\n            - --namespace=$(POD_NAMESPACE)\n            - --patch-mutating=false\n            - --secret-name=ingress-nginx-admission\n            - --patch-failure-policy=Fail\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-patchWebhook.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: patch\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - patch\n            - --webhook-name=ingress-nginx-admission\n            - --namespace=$(POD_NAMESPACE)\n            - --patch-mutating=false\n            - --secret-name=ingress-nginx-admission\n            - --patch-failure-policy=Fail\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-patchWebhook.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: patch\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - patch\n            - --webhook-name=ingress-nginx-admission\n            - --namespace=$(POD_NAMESPACE)\n            - --patch-mutating=false\n            - --secret-name=ingress-nginx-admission\n            - --patch-failure-policy=Fail\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/job-patchWebhook.yaml"}, "region": {"startLine": 3, "endLine": 57, "snippet": {"text": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  template:\n    metadata:\n      name: ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.1.4\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/version: \"1.2.1\"\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n        - name: patch\n          image: \"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.1.1\"\n          imagePullPolicy: IfNotPresent\n          args:\n            - patch\n            - --webhook-name=ingress-nginx-admission\n            - --namespace=$(POD_NAMESPACE)\n            - --patch-mutating=false\n            - --secret-name=ingress-nginx-admission\n            - --patch-failure-policy=Fail\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          securityContext:\n            allowPrivilegeEscalation: false\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n      nodeSelector: \n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 2000\n        fsGroup: 2000\n"}}}}]}, {"ruleId": "CKV_K8S_155", "ruleIndex": 25, "level": "error", "attachments": [], "message": {"text": "Minimize ClusterRoles that grant control over validating or mutating admission webhook configurations"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/clusterrole.yaml"}, "region": {"startLine": 3, "endLine": 25, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: ingress-nginx-admission\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade,post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nrules:\n  - apiGroups:\n      - admissionregistration.k8s.io\n    resources:\n      - validatingwebhookconfigurations\n    verbs:\n      - get\n      - update\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 18, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: ingress-nginx-admission\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade,post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/ingress-nginx/templates/admission-webhooks/job-patch/rolebinding.yaml"}, "region": {"startLine": 3, "endLine": 26, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: ingress-nginx-admission\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade,post-install,post-upgrade\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.1.4\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: \"1.2.1\"\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: ingress-nginx-admission\nsubjects:\n  - kind: ServiceAccount\n    name: ingress-nginx-admission\n    namespace: \"default\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/configmap-luascripts.yaml"}, "region": {"startLine": 3, "endLine": 15, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: fluent-bit-luascripts\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\ndata:\n  \n  log_processing.lua: \"function log_processing(tag, timestamp, record)\\n    -- Record modified, so 'code' return value (first parameter) is 1\\n    extra_data = {}\\n    extra_data[\\\"namespace_name\\\"]=record[\\\"kubernetes\\\"][\\\"namespace_name\\\"]\\n    extra_data[\\\"container_name\\\"]=record[\\\"kubernetes\\\"][\\\"container_name\\\"]\\n    extra_data[\\\"container_image\\\"]=record[\\\"kubernetes\\\"][\\\"container_image\\\"]\\n    extra_data[\\\"container_hash\\\"]=record[\\\"kubernetes\\\"][\\\"container_hash\\\"]\\n    new_record = {}\\n    new_record[\\\"log\\\"] = record[\\\"log\\\"]\\n    new_record[\\\"hostname\\\"] = record[\\\"kubernetes\\\"][\\\"host\\\"]\\n    new_record[\\\"appname\\\"] = record[\\\"kubernetes\\\"][\\\"pod_name\\\"]\\n    new_record[\\\"procid\\\"] = record[\\\"kubernetes\\\"][\\\"pod_id\\\"]\\n    new_record[\\\"sd\\\"] = extra_data\\n\\n    new_record[\\\"facility\\\"] = 16\\n\\n    new_record[\\\"severity\\\"] = 6 -- default value is \\\"informational\\\" => 6\\n\\n    severity = record[\\\"severity\\\"]\\n    if severity == \\\"EMERG\\\" or severity == \\\"emerg\\\" then new_record[\\\"severity\\\"] = 0 end\\n    if severity == \\\"ALERT\\\" or severity == \\\"alert\\\" then new_record[\\\"severity\\\"] = 1 end\\n    if severity == \\\"CRITICAL\\\" or severity == \\\"critical\\\" then new_record[\\\"severity\\\"] = 2 end\\n    if severity == \\\"ERROR\\\" or severity == \\\"error\\\" then new_record[\\\"severity\\\"] = 3 end\\n    if severity == \\\"WARNING\\\" or severity == \\\"warning\\\" then new_record[\\\"severity\\\"] = 4 end\\n    if severity == \\\"NOTICE\\\" or severity == \\\"notice\\\" then new_record[\\\"severity\\\"] = 5 end\\n    if severity == \\\"DEBUG\\\" or severity == \\\"debug\\\" then new_record[\\\"severity\\\"] = 7 end\\n\\n    return 1, timestamp, new_record\\nend\\n\"\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 87, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluent-bit\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: fluent-bit\n  template:\n    metadata:\n      annotations:\n        checksum/config: 454c9266b8d110d5c16f142168ebdd95d9de02a4551b3f8aea27fa93c2b5df07\n        checksum/luascripts: 19447830f535d57b8f89b1fa7514cba4533e1e579c671d388c5fb7107d92372c\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: fluent-bit\n    spec:\n      \n      serviceAccountName: fluent-bit\n      securityContext:\n        {}\n      containers:\n        - name: fluent-bit\n          securityContext:\n            {}\n          image: \":1.7.9\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 2020\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /api/v1/health\n              port: http\n          resources:\n            {}\n          volumeMounts:\n            - mountPath: /fluent-bit/etc/fluent-bit.conf\n              name: config\n              subPath: fluent-bit.conf\n            - mountPath: /fluent-bit/etc/custom_parsers.conf\n              name: config\n              subPath: custom_parsers.conf\n            - name: luascripts\n              mountPath: /fluent-bit/etc/log_processing.lua\n              subPath: log_processing.lua\n            - mountPath: /var/log\n              name: varlog\n            - mountPath: /data/var/lib/docker/containers\n              name: varlibdockercontainers\n              readOnly: true\n            - mountPath: /etc/machine-id\n              name: etcmachineid\n              readOnly: true\n      volumes:\n        - name: config\n          configMap:\n            name: fluent-bit\n        - name: luascripts\n          configMap:\n            name: fluent-bit-luascripts\n        - hostPath:\n            path: /var/log\n            type: \"\"\n          name: varlog\n        - hostPath:\n            path: /data/var/lib/docker/containers\n            type: \"\"\n          name: varlibdockercontainers\n        - hostPath:\n            path: /etc/machine-id\n            type: File\n          name: etcmachineid\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 87, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluent-bit\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: fluent-bit\n  template:\n    metadata:\n      annotations:\n        checksum/config: 454c9266b8d110d5c16f142168ebdd95d9de02a4551b3f8aea27fa93c2b5df07\n        checksum/luascripts: 19447830f535d57b8f89b1fa7514cba4533e1e579c671d388c5fb7107d92372c\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: fluent-bit\n    spec:\n      \n      serviceAccountName: fluent-bit\n      securityContext:\n        {}\n      containers:\n        - name: fluent-bit\n          securityContext:\n            {}\n          image: \":1.7.9\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 2020\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /api/v1/health\n              port: http\n          resources:\n            {}\n          volumeMounts:\n            - mountPath: /fluent-bit/etc/fluent-bit.conf\n              name: config\n              subPath: fluent-bit.conf\n            - mountPath: /fluent-bit/etc/custom_parsers.conf\n              name: config\n              subPath: custom_parsers.conf\n            - name: luascripts\n              mountPath: /fluent-bit/etc/log_processing.lua\n              subPath: log_processing.lua\n            - mountPath: /var/log\n              name: varlog\n            - mountPath: /data/var/lib/docker/containers\n              name: varlibdockercontainers\n              readOnly: true\n            - mountPath: /etc/machine-id\n              name: etcmachineid\n              readOnly: true\n      volumes:\n        - name: config\n          configMap:\n            name: fluent-bit\n        - name: luascripts\n          configMap:\n            name: fluent-bit-luascripts\n        - hostPath:\n            path: /var/log\n            type: \"\"\n          name: varlog\n        - hostPath:\n            path: /data/var/lib/docker/containers\n            type: \"\"\n          name: varlibdockercontainers\n        - hostPath:\n            path: /etc/machine-id\n            type: File\n          name: etcmachineid\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 87, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluent-bit\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: fluent-bit\n  template:\n    metadata:\n      annotations:\n        checksum/config: 454c9266b8d110d5c16f142168ebdd95d9de02a4551b3f8aea27fa93c2b5df07\n        checksum/luascripts: 19447830f535d57b8f89b1fa7514cba4533e1e579c671d388c5fb7107d92372c\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: fluent-bit\n    spec:\n      \n      serviceAccountName: fluent-bit\n      securityContext:\n        {}\n      containers:\n        - name: fluent-bit\n          securityContext:\n            {}\n          image: \":1.7.9\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 2020\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /api/v1/health\n              port: http\n          resources:\n            {}\n          volumeMounts:\n            - mountPath: /fluent-bit/etc/fluent-bit.conf\n              name: config\n              subPath: fluent-bit.conf\n            - mountPath: /fluent-bit/etc/custom_parsers.conf\n              name: config\n              subPath: custom_parsers.conf\n            - name: luascripts\n              mountPath: /fluent-bit/etc/log_processing.lua\n              subPath: log_processing.lua\n            - mountPath: /var/log\n              name: varlog\n            - mountPath: /data/var/lib/docker/containers\n              name: varlibdockercontainers\n              readOnly: true\n            - mountPath: /etc/machine-id\n              name: etcmachineid\n              readOnly: true\n      volumes:\n        - name: config\n          configMap:\n            name: fluent-bit\n        - name: luascripts\n          configMap:\n            name: fluent-bit-luascripts\n        - hostPath:\n            path: /var/log\n            type: \"\"\n          name: varlog\n        - hostPath:\n            path: /data/var/lib/docker/containers\n            type: \"\"\n          name: varlibdockercontainers\n        - hostPath:\n            path: /etc/machine-id\n            type: File\n          name: etcmachineid\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 87, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluent-bit\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: fluent-bit\n  template:\n    metadata:\n      annotations:\n        checksum/config: 454c9266b8d110d5c16f142168ebdd95d9de02a4551b3f8aea27fa93c2b5df07\n        checksum/luascripts: 19447830f535d57b8f89b1fa7514cba4533e1e579c671d388c5fb7107d92372c\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: fluent-bit\n    spec:\n      \n      serviceAccountName: fluent-bit\n      securityContext:\n        {}\n      containers:\n        - name: fluent-bit\n          securityContext:\n            {}\n          image: \":1.7.9\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 2020\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /api/v1/health\n              port: http\n          resources:\n            {}\n          volumeMounts:\n            - mountPath: /fluent-bit/etc/fluent-bit.conf\n              name: config\n              subPath: fluent-bit.conf\n            - mountPath: /fluent-bit/etc/custom_parsers.conf\n              name: config\n              subPath: custom_parsers.conf\n            - name: luascripts\n              mountPath: /fluent-bit/etc/log_processing.lua\n              subPath: log_processing.lua\n            - mountPath: /var/log\n              name: varlog\n            - mountPath: /data/var/lib/docker/containers\n              name: varlibdockercontainers\n              readOnly: true\n            - mountPath: /etc/machine-id\n              name: etcmachineid\n              readOnly: true\n      volumes:\n        - name: config\n          configMap:\n            name: fluent-bit\n        - name: luascripts\n          configMap:\n            name: fluent-bit-luascripts\n        - hostPath:\n            path: /var/log\n            type: \"\"\n          name: varlog\n        - hostPath:\n            path: /data/var/lib/docker/containers\n            type: \"\"\n          name: varlibdockercontainers\n        - hostPath:\n            path: /etc/machine-id\n            type: File\n          name: etcmachineid\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 87, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluent-bit\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: fluent-bit\n  template:\n    metadata:\n      annotations:\n        checksum/config: 454c9266b8d110d5c16f142168ebdd95d9de02a4551b3f8aea27fa93c2b5df07\n        checksum/luascripts: 19447830f535d57b8f89b1fa7514cba4533e1e579c671d388c5fb7107d92372c\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: fluent-bit\n    spec:\n      \n      serviceAccountName: fluent-bit\n      securityContext:\n        {}\n      containers:\n        - name: fluent-bit\n          securityContext:\n            {}\n          image: \":1.7.9\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 2020\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /api/v1/health\n              port: http\n          resources:\n            {}\n          volumeMounts:\n            - mountPath: /fluent-bit/etc/fluent-bit.conf\n              name: config\n              subPath: fluent-bit.conf\n            - mountPath: /fluent-bit/etc/custom_parsers.conf\n              name: config\n              subPath: custom_parsers.conf\n            - name: luascripts\n              mountPath: /fluent-bit/etc/log_processing.lua\n              subPath: log_processing.lua\n            - mountPath: /var/log\n              name: varlog\n            - mountPath: /data/var/lib/docker/containers\n              name: varlibdockercontainers\n              readOnly: true\n            - mountPath: /etc/machine-id\n              name: etcmachineid\n              readOnly: true\n      volumes:\n        - name: config\n          configMap:\n            name: fluent-bit\n        - name: luascripts\n          configMap:\n            name: fluent-bit-luascripts\n        - hostPath:\n            path: /var/log\n            type: \"\"\n          name: varlog\n        - hostPath:\n            path: /data/var/lib/docker/containers\n            type: \"\"\n          name: varlibdockercontainers\n        - hostPath:\n            path: /etc/machine-id\n            type: File\n          name: etcmachineid\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 87, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluent-bit\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: fluent-bit\n  template:\n    metadata:\n      annotations:\n        checksum/config: 454c9266b8d110d5c16f142168ebdd95d9de02a4551b3f8aea27fa93c2b5df07\n        checksum/luascripts: 19447830f535d57b8f89b1fa7514cba4533e1e579c671d388c5fb7107d92372c\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: fluent-bit\n    spec:\n      \n      serviceAccountName: fluent-bit\n      securityContext:\n        {}\n      containers:\n        - name: fluent-bit\n          securityContext:\n            {}\n          image: \":1.7.9\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 2020\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /api/v1/health\n              port: http\n          resources:\n            {}\n          volumeMounts:\n            - mountPath: /fluent-bit/etc/fluent-bit.conf\n              name: config\n              subPath: fluent-bit.conf\n            - mountPath: /fluent-bit/etc/custom_parsers.conf\n              name: config\n              subPath: custom_parsers.conf\n            - name: luascripts\n              mountPath: /fluent-bit/etc/log_processing.lua\n              subPath: log_processing.lua\n            - mountPath: /var/log\n              name: varlog\n            - mountPath: /data/var/lib/docker/containers\n              name: varlibdockercontainers\n              readOnly: true\n            - mountPath: /etc/machine-id\n              name: etcmachineid\n              readOnly: true\n      volumes:\n        - name: config\n          configMap:\n            name: fluent-bit\n        - name: luascripts\n          configMap:\n            name: fluent-bit-luascripts\n        - hostPath:\n            path: /var/log\n            type: \"\"\n          name: varlog\n        - hostPath:\n            path: /data/var/lib/docker/containers\n            type: \"\"\n          name: varlibdockercontainers\n        - hostPath:\n            path: /etc/machine-id\n            type: File\n          name: etcmachineid\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 87, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluent-bit\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: fluent-bit\n  template:\n    metadata:\n      annotations:\n        checksum/config: 454c9266b8d110d5c16f142168ebdd95d9de02a4551b3f8aea27fa93c2b5df07\n        checksum/luascripts: 19447830f535d57b8f89b1fa7514cba4533e1e579c671d388c5fb7107d92372c\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: fluent-bit\n    spec:\n      \n      serviceAccountName: fluent-bit\n      securityContext:\n        {}\n      containers:\n        - name: fluent-bit\n          securityContext:\n            {}\n          image: \":1.7.9\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 2020\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /api/v1/health\n              port: http\n          resources:\n            {}\n          volumeMounts:\n            - mountPath: /fluent-bit/etc/fluent-bit.conf\n              name: config\n              subPath: fluent-bit.conf\n            - mountPath: /fluent-bit/etc/custom_parsers.conf\n              name: config\n              subPath: custom_parsers.conf\n            - name: luascripts\n              mountPath: /fluent-bit/etc/log_processing.lua\n              subPath: log_processing.lua\n            - mountPath: /var/log\n              name: varlog\n            - mountPath: /data/var/lib/docker/containers\n              name: varlibdockercontainers\n              readOnly: true\n            - mountPath: /etc/machine-id\n              name: etcmachineid\n              readOnly: true\n      volumes:\n        - name: config\n          configMap:\n            name: fluent-bit\n        - name: luascripts\n          configMap:\n            name: fluent-bit-luascripts\n        - hostPath:\n            path: /var/log\n            type: \"\"\n          name: varlog\n        - hostPath:\n            path: /data/var/lib/docker/containers\n            type: \"\"\n          name: varlibdockercontainers\n        - hostPath:\n            path: /etc/machine-id\n            type: File\n          name: etcmachineid\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 87, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluent-bit\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: fluent-bit\n  template:\n    metadata:\n      annotations:\n        checksum/config: 454c9266b8d110d5c16f142168ebdd95d9de02a4551b3f8aea27fa93c2b5df07\n        checksum/luascripts: 19447830f535d57b8f89b1fa7514cba4533e1e579c671d388c5fb7107d92372c\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: fluent-bit\n    spec:\n      \n      serviceAccountName: fluent-bit\n      securityContext:\n        {}\n      containers:\n        - name: fluent-bit\n          securityContext:\n            {}\n          image: \":1.7.9\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 2020\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /api/v1/health\n              port: http\n          resources:\n            {}\n          volumeMounts:\n            - mountPath: /fluent-bit/etc/fluent-bit.conf\n              name: config\n              subPath: fluent-bit.conf\n            - mountPath: /fluent-bit/etc/custom_parsers.conf\n              name: config\n              subPath: custom_parsers.conf\n            - name: luascripts\n              mountPath: /fluent-bit/etc/log_processing.lua\n              subPath: log_processing.lua\n            - mountPath: /var/log\n              name: varlog\n            - mountPath: /data/var/lib/docker/containers\n              name: varlibdockercontainers\n              readOnly: true\n            - mountPath: /etc/machine-id\n              name: etcmachineid\n              readOnly: true\n      volumes:\n        - name: config\n          configMap:\n            name: fluent-bit\n        - name: luascripts\n          configMap:\n            name: fluent-bit-luascripts\n        - hostPath:\n            path: /var/log\n            type: \"\"\n          name: varlog\n        - hostPath:\n            path: /data/var/lib/docker/containers\n            type: \"\"\n          name: varlibdockercontainers\n        - hostPath:\n            path: /etc/machine-id\n            type: File\n          name: etcmachineid\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 87, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluent-bit\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: fluent-bit\n  template:\n    metadata:\n      annotations:\n        checksum/config: 454c9266b8d110d5c16f142168ebdd95d9de02a4551b3f8aea27fa93c2b5df07\n        checksum/luascripts: 19447830f535d57b8f89b1fa7514cba4533e1e579c671d388c5fb7107d92372c\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: fluent-bit\n    spec:\n      \n      serviceAccountName: fluent-bit\n      securityContext:\n        {}\n      containers:\n        - name: fluent-bit\n          securityContext:\n            {}\n          image: \":1.7.9\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 2020\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /api/v1/health\n              port: http\n          resources:\n            {}\n          volumeMounts:\n            - mountPath: /fluent-bit/etc/fluent-bit.conf\n              name: config\n              subPath: fluent-bit.conf\n            - mountPath: /fluent-bit/etc/custom_parsers.conf\n              name: config\n              subPath: custom_parsers.conf\n            - name: luascripts\n              mountPath: /fluent-bit/etc/log_processing.lua\n              subPath: log_processing.lua\n            - mountPath: /var/log\n              name: varlog\n            - mountPath: /data/var/lib/docker/containers\n              name: varlibdockercontainers\n              readOnly: true\n            - mountPath: /etc/machine-id\n              name: etcmachineid\n              readOnly: true\n      volumes:\n        - name: config\n          configMap:\n            name: fluent-bit\n        - name: luascripts\n          configMap:\n            name: fluent-bit-luascripts\n        - hostPath:\n            path: /var/log\n            type: \"\"\n          name: varlog\n        - hostPath:\n            path: /data/var/lib/docker/containers\n            type: \"\"\n          name: varlibdockercontainers\n        - hostPath:\n            path: /etc/machine-id\n            type: File\n          name: etcmachineid\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 87, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluent-bit\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: fluent-bit\n  template:\n    metadata:\n      annotations:\n        checksum/config: 454c9266b8d110d5c16f142168ebdd95d9de02a4551b3f8aea27fa93c2b5df07\n        checksum/luascripts: 19447830f535d57b8f89b1fa7514cba4533e1e579c671d388c5fb7107d92372c\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: fluent-bit\n    spec:\n      \n      serviceAccountName: fluent-bit\n      securityContext:\n        {}\n      containers:\n        - name: fluent-bit\n          securityContext:\n            {}\n          image: \":1.7.9\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 2020\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /api/v1/health\n              port: http\n          resources:\n            {}\n          volumeMounts:\n            - mountPath: /fluent-bit/etc/fluent-bit.conf\n              name: config\n              subPath: fluent-bit.conf\n            - mountPath: /fluent-bit/etc/custom_parsers.conf\n              name: config\n              subPath: custom_parsers.conf\n            - name: luascripts\n              mountPath: /fluent-bit/etc/log_processing.lua\n              subPath: log_processing.lua\n            - mountPath: /var/log\n              name: varlog\n            - mountPath: /data/var/lib/docker/containers\n              name: varlibdockercontainers\n              readOnly: true\n            - mountPath: /etc/machine-id\n              name: etcmachineid\n              readOnly: true\n      volumes:\n        - name: config\n          configMap:\n            name: fluent-bit\n        - name: luascripts\n          configMap:\n            name: fluent-bit-luascripts\n        - hostPath:\n            path: /var/log\n            type: \"\"\n          name: varlog\n        - hostPath:\n            path: /data/var/lib/docker/containers\n            type: \"\"\n          name: varlibdockercontainers\n        - hostPath:\n            path: /etc/machine-id\n            type: File\n          name: etcmachineid\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 87, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluent-bit\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: fluent-bit\n  template:\n    metadata:\n      annotations:\n        checksum/config: 454c9266b8d110d5c16f142168ebdd95d9de02a4551b3f8aea27fa93c2b5df07\n        checksum/luascripts: 19447830f535d57b8f89b1fa7514cba4533e1e579c671d388c5fb7107d92372c\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: fluent-bit\n    spec:\n      \n      serviceAccountName: fluent-bit\n      securityContext:\n        {}\n      containers:\n        - name: fluent-bit\n          securityContext:\n            {}\n          image: \":1.7.9\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 2020\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /api/v1/health\n              port: http\n          resources:\n            {}\n          volumeMounts:\n            - mountPath: /fluent-bit/etc/fluent-bit.conf\n              name: config\n              subPath: fluent-bit.conf\n            - mountPath: /fluent-bit/etc/custom_parsers.conf\n              name: config\n              subPath: custom_parsers.conf\n            - name: luascripts\n              mountPath: /fluent-bit/etc/log_processing.lua\n              subPath: log_processing.lua\n            - mountPath: /var/log\n              name: varlog\n            - mountPath: /data/var/lib/docker/containers\n              name: varlibdockercontainers\n              readOnly: true\n            - mountPath: /etc/machine-id\n              name: etcmachineid\n              readOnly: true\n      volumes:\n        - name: config\n          configMap:\n            name: fluent-bit\n        - name: luascripts\n          configMap:\n            name: fluent-bit-luascripts\n        - hostPath:\n            path: /var/log\n            type: \"\"\n          name: varlog\n        - hostPath:\n            path: /data/var/lib/docker/containers\n            type: \"\"\n          name: varlibdockercontainers\n        - hostPath:\n            path: /etc/machine-id\n            type: File\n          name: etcmachineid\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 87, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluent-bit\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: fluent-bit\n  template:\n    metadata:\n      annotations:\n        checksum/config: 454c9266b8d110d5c16f142168ebdd95d9de02a4551b3f8aea27fa93c2b5df07\n        checksum/luascripts: 19447830f535d57b8f89b1fa7514cba4533e1e579c671d388c5fb7107d92372c\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: fluent-bit\n    spec:\n      \n      serviceAccountName: fluent-bit\n      securityContext:\n        {}\n      containers:\n        - name: fluent-bit\n          securityContext:\n            {}\n          image: \":1.7.9\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 2020\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /api/v1/health\n              port: http\n          resources:\n            {}\n          volumeMounts:\n            - mountPath: /fluent-bit/etc/fluent-bit.conf\n              name: config\n              subPath: fluent-bit.conf\n            - mountPath: /fluent-bit/etc/custom_parsers.conf\n              name: config\n              subPath: custom_parsers.conf\n            - name: luascripts\n              mountPath: /fluent-bit/etc/log_processing.lua\n              subPath: log_processing.lua\n            - mountPath: /var/log\n              name: varlog\n            - mountPath: /data/var/lib/docker/containers\n              name: varlibdockercontainers\n              readOnly: true\n            - mountPath: /etc/machine-id\n              name: etcmachineid\n              readOnly: true\n      volumes:\n        - name: config\n          configMap:\n            name: fluent-bit\n        - name: luascripts\n          configMap:\n            name: fluent-bit-luascripts\n        - hostPath:\n            path: /var/log\n            type: \"\"\n          name: varlog\n        - hostPath:\n            path: /data/var/lib/docker/containers\n            type: \"\"\n          name: varlibdockercontainers\n        - hostPath:\n            path: /etc/machine-id\n            type: File\n          name: etcmachineid\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 87, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluent-bit\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: fluent-bit\n  template:\n    metadata:\n      annotations:\n        checksum/config: 454c9266b8d110d5c16f142168ebdd95d9de02a4551b3f8aea27fa93c2b5df07\n        checksum/luascripts: 19447830f535d57b8f89b1fa7514cba4533e1e579c671d388c5fb7107d92372c\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: fluent-bit\n    spec:\n      \n      serviceAccountName: fluent-bit\n      securityContext:\n        {}\n      containers:\n        - name: fluent-bit\n          securityContext:\n            {}\n          image: \":1.7.9\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 2020\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /api/v1/health\n              port: http\n          resources:\n            {}\n          volumeMounts:\n            - mountPath: /fluent-bit/etc/fluent-bit.conf\n              name: config\n              subPath: fluent-bit.conf\n            - mountPath: /fluent-bit/etc/custom_parsers.conf\n              name: config\n              subPath: custom_parsers.conf\n            - name: luascripts\n              mountPath: /fluent-bit/etc/log_processing.lua\n              subPath: log_processing.lua\n            - mountPath: /var/log\n              name: varlog\n            - mountPath: /data/var/lib/docker/containers\n              name: varlibdockercontainers\n              readOnly: true\n            - mountPath: /etc/machine-id\n              name: etcmachineid\n              readOnly: true\n      volumes:\n        - name: config\n          configMap:\n            name: fluent-bit\n        - name: luascripts\n          configMap:\n            name: fluent-bit-luascripts\n        - hostPath:\n            path: /var/log\n            type: \"\"\n          name: varlog\n        - hostPath:\n            path: /data/var/lib/docker/containers\n            type: \"\"\n          name: varlibdockercontainers\n        - hostPath:\n            path: /etc/machine-id\n            type: File\n          name: etcmachineid\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/daemonset.yaml"}, "region": {"startLine": 3, "endLine": 87, "snippet": {"text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluent-bit\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: fluent-bit\n  template:\n    metadata:\n      annotations:\n        checksum/config: 454c9266b8d110d5c16f142168ebdd95d9de02a4551b3f8aea27fa93c2b5df07\n        checksum/luascripts: 19447830f535d57b8f89b1fa7514cba4533e1e579c671d388c5fb7107d92372c\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: fluent-bit\n    spec:\n      \n      serviceAccountName: fluent-bit\n      securityContext:\n        {}\n      containers:\n        - name: fluent-bit\n          securityContext:\n            {}\n          image: \":1.7.9\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 2020\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /api/v1/health\n              port: http\n          resources:\n            {}\n          volumeMounts:\n            - mountPath: /fluent-bit/etc/fluent-bit.conf\n              name: config\n              subPath: fluent-bit.conf\n            - mountPath: /fluent-bit/etc/custom_parsers.conf\n              name: config\n              subPath: custom_parsers.conf\n            - name: luascripts\n              mountPath: /fluent-bit/etc/log_processing.lua\n              subPath: log_processing.lua\n            - mountPath: /var/log\n              name: varlog\n            - mountPath: /data/var/lib/docker/containers\n              name: varlibdockercontainers\n              readOnly: true\n            - mountPath: /etc/machine-id\n              name: etcmachineid\n              readOnly: true\n      volumes:\n        - name: config\n          configMap:\n            name: fluent-bit\n        - name: luascripts\n          configMap:\n            name: fluent-bit-luascripts\n        - hostPath:\n            path: /var/log\n            type: \"\"\n          name: varlog\n        - hostPath:\n            path: /data/var/lib/docker/containers\n            type: \"\"\n          name: varlibdockercontainers\n        - hostPath:\n            path: /etc/machine-id\n            type: File\n          name: etcmachineid\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/service.yaml"}, "region": {"startLine": 3, "endLine": 22, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: fluent-bit\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ClusterIP\n  ports:\n    - port: 2020\n      targetPort: http\n      protocol: TCP\n      name: http\n  selector:\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 12, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: fluent-bit\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/configmap.yaml"}, "region": {"startLine": 3, "endLine": 76, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: fluent-bit\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\ndata:\n  custom_parsers.conf: |\n    [PARSER]\n        Name severity-log\n        Format regex\n        Regex (?<severity>(EMERG|ALERT|CRITICAL|ERROR|WARNING|NOTICE|INFO|DEBUG|emerg|alert|critical|error|warning|notice|info|debug))+\n        Time_Keep On\n        Time_Key time\n        Time_Format %Y-%m-%dT%H:%M:%S.%L\n    \n  fluent-bit.conf: |\n    [SERVICE]\n        Flush 1\n        Daemon Off\n        Log_Level debug\n        Parsers_File parsers.conf\n        Parsers_File custom_parsers.conf\n        HTTP_Server On\n        HTTP_Listen 0.0.0.0\n        HTTP_Port 2020\n        storage.path              /var/log/flb-storage/\n        storage.sync              normal\n        storage.checksum          On\n        storage.backlog.mem_limit 512MB\n    \n    [INPUT]\n        Name tail\n        Path /var/log/containers/tozny-*, /var/log/containers/harbor-*, /var/log/containers/jenkins-*, /var/log/containers/gitlab-*, /var/log/containers/networking-flow-logs-*\n        Tag k8s.*\n        Mem_Buf_Limit 512MB\n        Skip_Long_Lines On\n        storage.type  filesystem\n    \n    [FILTER]\n        Name kubernetes\n        Match *\n        Kube_Tag_Prefix k8s.var.log.containers\n        Merge_Log On\n        Keep_Log Off\n        K8S-Logging.Parser On\n        K8S-Logging.Exclude On\n    [FILTER]\n        Name parser\n        Match *\n        Key_Name log\n        Parser severity-log\n        Preserve_Key True\n        Reserve_Data True\n    [FILTER]\n        Name    lua\n        Match   *\n        script  log_processing.lua\n        call    log_processing\n    \n    [OUTPUT]\n        Name  http\n        Match *\n        Host  logs-signing-proxy.prod-wrai-infra-logs-signing-proxy\n        Port  80\n        Format json\n    \n    [OUTPUT]\n        name stdout\n        match *\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/fluent-bit/templates/tests/test-connection.yaml"}, "region": {"startLine": 25, "endLine": 44, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"fluent-bit-test-connection\"\n  labels:\n    helm.sh/chart: fluent-bit-0.15.15\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/version: \"1.7.9\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    \"helm.sh/hook\": test-success\nspec:\n  containers:\n    - name: wget\n      image: \":\"\n      imagePullPolicy: IfNotPresent\n      command: ['wget']\n      args: ['fluent-bit:2020']\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-connector/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 45, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gitlab-connector\n  labels:\n  annotations:\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: gitlab-connector\n  template:\n    metadata:\n      labels:\n        app: gitlab-connector\n      annotations:\n    spec:\n      containers:\n      - name: gitlab-connector\n        command: [\"node\", \"./server.js\", --secret=/example]\n        image: \":latest\"\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 4000\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 4000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 50m\n            memory: 150Mi \n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-connector/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 45, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gitlab-connector\n  labels:\n  annotations:\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: gitlab-connector\n  template:\n    metadata:\n      labels:\n        app: gitlab-connector\n      annotations:\n    spec:\n      containers:\n      - name: gitlab-connector\n        command: [\"node\", \"./server.js\", --secret=/example]\n        image: \":latest\"\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 4000\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 4000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 50m\n            memory: 150Mi \n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-connector/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 45, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gitlab-connector\n  labels:\n  annotations:\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: gitlab-connector\n  template:\n    metadata:\n      labels:\n        app: gitlab-connector\n      annotations:\n    spec:\n      containers:\n      - name: gitlab-connector\n        command: [\"node\", \"./server.js\", --secret=/example]\n        image: \":latest\"\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 4000\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 4000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 50m\n            memory: 150Mi \n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-connector/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 45, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gitlab-connector\n  labels:\n  annotations:\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: gitlab-connector\n  template:\n    metadata:\n      labels:\n        app: gitlab-connector\n      annotations:\n    spec:\n      containers:\n      - name: gitlab-connector\n        command: [\"node\", \"./server.js\", --secret=/example]\n        image: \":latest\"\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 4000\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 4000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 50m\n            memory: 150Mi \n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-connector/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 45, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gitlab-connector\n  labels:\n  annotations:\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: gitlab-connector\n  template:\n    metadata:\n      labels:\n        app: gitlab-connector\n      annotations:\n    spec:\n      containers:\n      - name: gitlab-connector\n        command: [\"node\", \"./server.js\", --secret=/example]\n        image: \":latest\"\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 4000\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 4000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 50m\n            memory: 150Mi \n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-connector/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 45, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gitlab-connector\n  labels:\n  annotations:\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: gitlab-connector\n  template:\n    metadata:\n      labels:\n        app: gitlab-connector\n      annotations:\n    spec:\n      containers:\n      - name: gitlab-connector\n        command: [\"node\", \"./server.js\", --secret=/example]\n        image: \":latest\"\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 4000\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 4000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 50m\n            memory: 150Mi \n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-connector/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 45, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gitlab-connector\n  labels:\n  annotations:\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: gitlab-connector\n  template:\n    metadata:\n      labels:\n        app: gitlab-connector\n      annotations:\n    spec:\n      containers:\n      - name: gitlab-connector\n        command: [\"node\", \"./server.js\", --secret=/example]\n        image: \":latest\"\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 4000\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 4000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 50m\n            memory: 150Mi \n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n"}}}}]}, {"ruleId": "CKV_K8S_14", "ruleIndex": 8, "level": "error", "attachments": [], "message": {"text": "Image Tag should be fixed - not latest or blank"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-connector/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 45, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gitlab-connector\n  labels:\n  annotations:\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: gitlab-connector\n  template:\n    metadata:\n      labels:\n        app: gitlab-connector\n      annotations:\n    spec:\n      containers:\n      - name: gitlab-connector\n        command: [\"node\", \"./server.js\", --secret=/example]\n        image: \":latest\"\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 4000\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 4000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 50m\n            memory: 150Mi \n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-connector/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 45, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gitlab-connector\n  labels:\n  annotations:\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: gitlab-connector\n  template:\n    metadata:\n      labels:\n        app: gitlab-connector\n      annotations:\n    spec:\n      containers:\n      - name: gitlab-connector\n        command: [\"node\", \"./server.js\", --secret=/example]\n        image: \":latest\"\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 4000\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 4000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 50m\n            memory: 150Mi \n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-connector/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 45, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gitlab-connector\n  labels:\n  annotations:\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: gitlab-connector\n  template:\n    metadata:\n      labels:\n        app: gitlab-connector\n      annotations:\n    spec:\n      containers:\n      - name: gitlab-connector\n        command: [\"node\", \"./server.js\", --secret=/example]\n        image: \":latest\"\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 4000\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 4000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 50m\n            memory: 150Mi \n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-connector/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 45, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gitlab-connector\n  labels:\n  annotations:\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: gitlab-connector\n  template:\n    metadata:\n      labels:\n        app: gitlab-connector\n      annotations:\n    spec:\n      containers:\n      - name: gitlab-connector\n        command: [\"node\", \"./server.js\", --secret=/example]\n        image: \":latest\"\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 4000\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 4000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 50m\n            memory: 150Mi \n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-connector/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 45, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gitlab-connector\n  labels:\n  annotations:\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: gitlab-connector\n  template:\n    metadata:\n      labels:\n        app: gitlab-connector\n      annotations:\n    spec:\n      containers:\n      - name: gitlab-connector\n        command: [\"node\", \"./server.js\", --secret=/example]\n        image: \":latest\"\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 4000\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 4000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 50m\n            memory: 150Mi \n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-connector/templates/service.yaml"}, "region": {"startLine": 3, "endLine": 15, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: gitlab-connector\n  labels:\n    app: gitlab-connector\nspec:\n  type: ClusterIP\n  ports:\n  - port: 4000\n    targetPort: 4000\n  selector:\n    app: gitlab-connector\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/server-config-configmap.yaml"}, "region": {"startLine": 3, "endLine": 25, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vault-config\n  namespace: default\n  labels:\n    helm.sh/chart: vault-0.20.1\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\ndata:\n  extraconfig-from-values.hcl: |-\n    disable_mlock = true\n    ui = true\n    \n    listener \"tcp\" {\n      tls_disable = 1\n      address = \"[::]:8200\"\n      cluster_address = \"[::]:8201\"\n    }\n    storage \"file\" {\n      path = \"/vault/data\"\n    }\n"}}}}]}, {"ruleId": "CKV_K8S_155", "ruleIndex": 25, "level": "error", "attachments": [], "message": {"text": "Minimize ClusterRoles that grant control over validating or mutating admission webhook configurations"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/injector-clusterrole.yaml"}, "region": {"startLine": 3, "endLine": 18, "snippet": {"text": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: vault-agent-injector-clusterrole\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\nrules:\n- apiGroups: [\"admissionregistration.k8s.io\"]\n  resources: [\"mutatingwebhookconfigurations\"]\n  verbs:\n    - \"get\"\n    - \"list\"\n    - \"watch\"\n    - \"patch\"\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/server-service.yaml"}, "region": {"startLine": 4, "endLine": 30, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: vault\n  namespace: default\n  labels:\n    helm.sh/chart: vault-0.20.1\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n\nspec:\n  # We want the servers to become available even if they're not ready\n  # since this DNS is also used for join operations.\n  publishNotReadyAddresses: true\n  ports:\n    - name: http\n      port: 8200\n      targetPort: 8200\n    - name: https-internal\n      port: 8201\n      targetPort: 8201\n  selector:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: vault\n    component: server\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/server-statefulset.yaml"}, "region": {"startLine": 4, "endLine": 178, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: vault\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.20.1\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: vault\n        component: server\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault\n                  app.kubernetes.io/instance: \"vault\"\n                  component: server\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      terminationGracePeriodSeconds: 10\n      serviceAccountName: vault\n      \n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      volumes:\n        \n        - name: config\n          configMap:\n            name: vault-config\n  \n        - name: home\n          emptyDir: {}\n      containers:\n        - name: vault\n          \n          image: hashicorp/vault:1.10.3\n          imagePullPolicy: IfNotPresent\n          command:\n          - \"/bin/sh\"\n          - \"-ec\"\n          args: \n          - |\n            cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\n            [ -n \"${HOST_IP}\" ] && sed -Ei \"s|HOST_IP|${HOST_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${POD_IP}\" ] && sed -Ei \"s|POD_IP|${POD_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${HOSTNAME}\" ] && sed -Ei \"s|HOSTNAME|${HOSTNAME?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${API_ADDR}\" ] && sed -Ei \"s|API_ADDR|${API_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${TRANSIT_ADDR}\" ] && sed -Ei \"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${RAFT_ADDR}\" ] && sed -Ei \"s|RAFT_ADDR|${RAFT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            /usr/local/bin/docker-entrypoint.sh vault server -config=/tmp/storageconfig.hcl \n   \n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: HOST_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.hostIP\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n            - name: VAULT_K8S_POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_K8S_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: VAULT_ADDR\n              value: \"http://127.0.0.1:8200\"\n            - name: VAULT_API_ADDR\n              value: \"http://$(POD_IP):8200\"\n            - name: SKIP_CHOWN\n              value: \"true\"\n            - name: SKIP_SETCAP\n              value: \"true\"\n            - name: HOSTNAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_CLUSTER_ADDR\n              value: \"https://$(HOSTNAME).vault-internal:8201\"\n            - name: HOME\n              value: \"/home/vault\"\n            \n            \n            \n          volumeMounts:\n          \n  \n    \n            - name: data\n              mountPath: /vault/data\n    \n  \n  \n            - name: config\n              mountPath: /vault/config\n  \n            - name: home\n              mountPath: /home/vault\n          ports:\n            - containerPort: 8200\n              name: http\n            - containerPort: 8201\n              name: https-internal\n            - containerPort: 8202\n              name: http-rep\n          readinessProbe:\n            # Check status; unsealed vault servers return 0\n            # The exit code reflects the seal status:\n            #   0 - unsealed\n            #   1 - error\n            #   2 - sealed\n            exec:\n              command: [\"/bin/sh\", \"-ec\", \"vault status -tls-skip-verify\"]\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 3\n          lifecycle:\n            # Vault container doesn't receive SIGTERM from Kubernetes\n            # and after the grace period ends, Kube sends SIGKILL.  This\n            # causes issues with graceful shutdowns such as deregistering itself\n            # from Consul (zombie services).\n            preStop:\n              exec:\n                command: [\n                  \"/bin/sh\", \"-c\",\n                  # Adding a sleep here to give the pod eviction a\n                  # chance to propagate, so requests will not be made\n                  # to this pod while it's terminating\n                  \"sleep 5 && kill -SIGTERM $(pidof vault)\",\n                ]\n      \n  \n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      \n      spec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 10Gi\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/server-statefulset.yaml"}, "region": {"startLine": 4, "endLine": 178, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: vault\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.20.1\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: vault\n        component: server\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault\n                  app.kubernetes.io/instance: \"vault\"\n                  component: server\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      terminationGracePeriodSeconds: 10\n      serviceAccountName: vault\n      \n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      volumes:\n        \n        - name: config\n          configMap:\n            name: vault-config\n  \n        - name: home\n          emptyDir: {}\n      containers:\n        - name: vault\n          \n          image: hashicorp/vault:1.10.3\n          imagePullPolicy: IfNotPresent\n          command:\n          - \"/bin/sh\"\n          - \"-ec\"\n          args: \n          - |\n            cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\n            [ -n \"${HOST_IP}\" ] && sed -Ei \"s|HOST_IP|${HOST_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${POD_IP}\" ] && sed -Ei \"s|POD_IP|${POD_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${HOSTNAME}\" ] && sed -Ei \"s|HOSTNAME|${HOSTNAME?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${API_ADDR}\" ] && sed -Ei \"s|API_ADDR|${API_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${TRANSIT_ADDR}\" ] && sed -Ei \"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${RAFT_ADDR}\" ] && sed -Ei \"s|RAFT_ADDR|${RAFT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            /usr/local/bin/docker-entrypoint.sh vault server -config=/tmp/storageconfig.hcl \n   \n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: HOST_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.hostIP\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n            - name: VAULT_K8S_POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_K8S_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: VAULT_ADDR\n              value: \"http://127.0.0.1:8200\"\n            - name: VAULT_API_ADDR\n              value: \"http://$(POD_IP):8200\"\n            - name: SKIP_CHOWN\n              value: \"true\"\n            - name: SKIP_SETCAP\n              value: \"true\"\n            - name: HOSTNAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_CLUSTER_ADDR\n              value: \"https://$(HOSTNAME).vault-internal:8201\"\n            - name: HOME\n              value: \"/home/vault\"\n            \n            \n            \n          volumeMounts:\n          \n  \n    \n            - name: data\n              mountPath: /vault/data\n    \n  \n  \n            - name: config\n              mountPath: /vault/config\n  \n            - name: home\n              mountPath: /home/vault\n          ports:\n            - containerPort: 8200\n              name: http\n            - containerPort: 8201\n              name: https-internal\n            - containerPort: 8202\n              name: http-rep\n          readinessProbe:\n            # Check status; unsealed vault servers return 0\n            # The exit code reflects the seal status:\n            #   0 - unsealed\n            #   1 - error\n            #   2 - sealed\n            exec:\n              command: [\"/bin/sh\", \"-ec\", \"vault status -tls-skip-verify\"]\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 3\n          lifecycle:\n            # Vault container doesn't receive SIGTERM from Kubernetes\n            # and after the grace period ends, Kube sends SIGKILL.  This\n            # causes issues with graceful shutdowns such as deregistering itself\n            # from Consul (zombie services).\n            preStop:\n              exec:\n                command: [\n                  \"/bin/sh\", \"-c\",\n                  # Adding a sleep here to give the pod eviction a\n                  # chance to propagate, so requests will not be made\n                  # to this pod while it's terminating\n                  \"sleep 5 && kill -SIGTERM $(pidof vault)\",\n                ]\n      \n  \n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      \n      spec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 10Gi\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/server-statefulset.yaml"}, "region": {"startLine": 4, "endLine": 178, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: vault\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.20.1\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: vault\n        component: server\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault\n                  app.kubernetes.io/instance: \"vault\"\n                  component: server\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      terminationGracePeriodSeconds: 10\n      serviceAccountName: vault\n      \n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      volumes:\n        \n        - name: config\n          configMap:\n            name: vault-config\n  \n        - name: home\n          emptyDir: {}\n      containers:\n        - name: vault\n          \n          image: hashicorp/vault:1.10.3\n          imagePullPolicy: IfNotPresent\n          command:\n          - \"/bin/sh\"\n          - \"-ec\"\n          args: \n          - |\n            cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\n            [ -n \"${HOST_IP}\" ] && sed -Ei \"s|HOST_IP|${HOST_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${POD_IP}\" ] && sed -Ei \"s|POD_IP|${POD_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${HOSTNAME}\" ] && sed -Ei \"s|HOSTNAME|${HOSTNAME?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${API_ADDR}\" ] && sed -Ei \"s|API_ADDR|${API_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${TRANSIT_ADDR}\" ] && sed -Ei \"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${RAFT_ADDR}\" ] && sed -Ei \"s|RAFT_ADDR|${RAFT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            /usr/local/bin/docker-entrypoint.sh vault server -config=/tmp/storageconfig.hcl \n   \n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: HOST_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.hostIP\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n            - name: VAULT_K8S_POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_K8S_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: VAULT_ADDR\n              value: \"http://127.0.0.1:8200\"\n            - name: VAULT_API_ADDR\n              value: \"http://$(POD_IP):8200\"\n            - name: SKIP_CHOWN\n              value: \"true\"\n            - name: SKIP_SETCAP\n              value: \"true\"\n            - name: HOSTNAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_CLUSTER_ADDR\n              value: \"https://$(HOSTNAME).vault-internal:8201\"\n            - name: HOME\n              value: \"/home/vault\"\n            \n            \n            \n          volumeMounts:\n          \n  \n    \n            - name: data\n              mountPath: /vault/data\n    \n  \n  \n            - name: config\n              mountPath: /vault/config\n  \n            - name: home\n              mountPath: /home/vault\n          ports:\n            - containerPort: 8200\n              name: http\n            - containerPort: 8201\n              name: https-internal\n            - containerPort: 8202\n              name: http-rep\n          readinessProbe:\n            # Check status; unsealed vault servers return 0\n            # The exit code reflects the seal status:\n            #   0 - unsealed\n            #   1 - error\n            #   2 - sealed\n            exec:\n              command: [\"/bin/sh\", \"-ec\", \"vault status -tls-skip-verify\"]\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 3\n          lifecycle:\n            # Vault container doesn't receive SIGTERM from Kubernetes\n            # and after the grace period ends, Kube sends SIGKILL.  This\n            # causes issues with graceful shutdowns such as deregistering itself\n            # from Consul (zombie services).\n            preStop:\n              exec:\n                command: [\n                  \"/bin/sh\", \"-c\",\n                  # Adding a sleep here to give the pod eviction a\n                  # chance to propagate, so requests will not be made\n                  # to this pod while it's terminating\n                  \"sleep 5 && kill -SIGTERM $(pidof vault)\",\n                ]\n      \n  \n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      \n      spec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 10Gi\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/server-statefulset.yaml"}, "region": {"startLine": 4, "endLine": 178, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: vault\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.20.1\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: vault\n        component: server\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault\n                  app.kubernetes.io/instance: \"vault\"\n                  component: server\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      terminationGracePeriodSeconds: 10\n      serviceAccountName: vault\n      \n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      volumes:\n        \n        - name: config\n          configMap:\n            name: vault-config\n  \n        - name: home\n          emptyDir: {}\n      containers:\n        - name: vault\n          \n          image: hashicorp/vault:1.10.3\n          imagePullPolicy: IfNotPresent\n          command:\n          - \"/bin/sh\"\n          - \"-ec\"\n          args: \n          - |\n            cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\n            [ -n \"${HOST_IP}\" ] && sed -Ei \"s|HOST_IP|${HOST_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${POD_IP}\" ] && sed -Ei \"s|POD_IP|${POD_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${HOSTNAME}\" ] && sed -Ei \"s|HOSTNAME|${HOSTNAME?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${API_ADDR}\" ] && sed -Ei \"s|API_ADDR|${API_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${TRANSIT_ADDR}\" ] && sed -Ei \"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${RAFT_ADDR}\" ] && sed -Ei \"s|RAFT_ADDR|${RAFT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            /usr/local/bin/docker-entrypoint.sh vault server -config=/tmp/storageconfig.hcl \n   \n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: HOST_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.hostIP\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n            - name: VAULT_K8S_POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_K8S_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: VAULT_ADDR\n              value: \"http://127.0.0.1:8200\"\n            - name: VAULT_API_ADDR\n              value: \"http://$(POD_IP):8200\"\n            - name: SKIP_CHOWN\n              value: \"true\"\n            - name: SKIP_SETCAP\n              value: \"true\"\n            - name: HOSTNAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_CLUSTER_ADDR\n              value: \"https://$(HOSTNAME).vault-internal:8201\"\n            - name: HOME\n              value: \"/home/vault\"\n            \n            \n            \n          volumeMounts:\n          \n  \n    \n            - name: data\n              mountPath: /vault/data\n    \n  \n  \n            - name: config\n              mountPath: /vault/config\n  \n            - name: home\n              mountPath: /home/vault\n          ports:\n            - containerPort: 8200\n              name: http\n            - containerPort: 8201\n              name: https-internal\n            - containerPort: 8202\n              name: http-rep\n          readinessProbe:\n            # Check status; unsealed vault servers return 0\n            # The exit code reflects the seal status:\n            #   0 - unsealed\n            #   1 - error\n            #   2 - sealed\n            exec:\n              command: [\"/bin/sh\", \"-ec\", \"vault status -tls-skip-verify\"]\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 3\n          lifecycle:\n            # Vault container doesn't receive SIGTERM from Kubernetes\n            # and after the grace period ends, Kube sends SIGKILL.  This\n            # causes issues with graceful shutdowns such as deregistering itself\n            # from Consul (zombie services).\n            preStop:\n              exec:\n                command: [\n                  \"/bin/sh\", \"-c\",\n                  # Adding a sleep here to give the pod eviction a\n                  # chance to propagate, so requests will not be made\n                  # to this pod while it's terminating\n                  \"sleep 5 && kill -SIGTERM $(pidof vault)\",\n                ]\n      \n  \n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      \n      spec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 10Gi\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/server-statefulset.yaml"}, "region": {"startLine": 4, "endLine": 178, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: vault\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.20.1\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: vault\n        component: server\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault\n                  app.kubernetes.io/instance: \"vault\"\n                  component: server\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      terminationGracePeriodSeconds: 10\n      serviceAccountName: vault\n      \n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      volumes:\n        \n        - name: config\n          configMap:\n            name: vault-config\n  \n        - name: home\n          emptyDir: {}\n      containers:\n        - name: vault\n          \n          image: hashicorp/vault:1.10.3\n          imagePullPolicy: IfNotPresent\n          command:\n          - \"/bin/sh\"\n          - \"-ec\"\n          args: \n          - |\n            cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\n            [ -n \"${HOST_IP}\" ] && sed -Ei \"s|HOST_IP|${HOST_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${POD_IP}\" ] && sed -Ei \"s|POD_IP|${POD_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${HOSTNAME}\" ] && sed -Ei \"s|HOSTNAME|${HOSTNAME?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${API_ADDR}\" ] && sed -Ei \"s|API_ADDR|${API_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${TRANSIT_ADDR}\" ] && sed -Ei \"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${RAFT_ADDR}\" ] && sed -Ei \"s|RAFT_ADDR|${RAFT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            /usr/local/bin/docker-entrypoint.sh vault server -config=/tmp/storageconfig.hcl \n   \n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: HOST_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.hostIP\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n            - name: VAULT_K8S_POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_K8S_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: VAULT_ADDR\n              value: \"http://127.0.0.1:8200\"\n            - name: VAULT_API_ADDR\n              value: \"http://$(POD_IP):8200\"\n            - name: SKIP_CHOWN\n              value: \"true\"\n            - name: SKIP_SETCAP\n              value: \"true\"\n            - name: HOSTNAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_CLUSTER_ADDR\n              value: \"https://$(HOSTNAME).vault-internal:8201\"\n            - name: HOME\n              value: \"/home/vault\"\n            \n            \n            \n          volumeMounts:\n          \n  \n    \n            - name: data\n              mountPath: /vault/data\n    \n  \n  \n            - name: config\n              mountPath: /vault/config\n  \n            - name: home\n              mountPath: /home/vault\n          ports:\n            - containerPort: 8200\n              name: http\n            - containerPort: 8201\n              name: https-internal\n            - containerPort: 8202\n              name: http-rep\n          readinessProbe:\n            # Check status; unsealed vault servers return 0\n            # The exit code reflects the seal status:\n            #   0 - unsealed\n            #   1 - error\n            #   2 - sealed\n            exec:\n              command: [\"/bin/sh\", \"-ec\", \"vault status -tls-skip-verify\"]\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 3\n          lifecycle:\n            # Vault container doesn't receive SIGTERM from Kubernetes\n            # and after the grace period ends, Kube sends SIGKILL.  This\n            # causes issues with graceful shutdowns such as deregistering itself\n            # from Consul (zombie services).\n            preStop:\n              exec:\n                command: [\n                  \"/bin/sh\", \"-c\",\n                  # Adding a sleep here to give the pod eviction a\n                  # chance to propagate, so requests will not be made\n                  # to this pod while it's terminating\n                  \"sleep 5 && kill -SIGTERM $(pidof vault)\",\n                ]\n      \n  \n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      \n      spec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 10Gi\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/server-statefulset.yaml"}, "region": {"startLine": 4, "endLine": 178, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: vault\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.20.1\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: vault\n        component: server\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault\n                  app.kubernetes.io/instance: \"vault\"\n                  component: server\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      terminationGracePeriodSeconds: 10\n      serviceAccountName: vault\n      \n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      volumes:\n        \n        - name: config\n          configMap:\n            name: vault-config\n  \n        - name: home\n          emptyDir: {}\n      containers:\n        - name: vault\n          \n          image: hashicorp/vault:1.10.3\n          imagePullPolicy: IfNotPresent\n          command:\n          - \"/bin/sh\"\n          - \"-ec\"\n          args: \n          - |\n            cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\n            [ -n \"${HOST_IP}\" ] && sed -Ei \"s|HOST_IP|${HOST_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${POD_IP}\" ] && sed -Ei \"s|POD_IP|${POD_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${HOSTNAME}\" ] && sed -Ei \"s|HOSTNAME|${HOSTNAME?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${API_ADDR}\" ] && sed -Ei \"s|API_ADDR|${API_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${TRANSIT_ADDR}\" ] && sed -Ei \"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${RAFT_ADDR}\" ] && sed -Ei \"s|RAFT_ADDR|${RAFT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            /usr/local/bin/docker-entrypoint.sh vault server -config=/tmp/storageconfig.hcl \n   \n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: HOST_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.hostIP\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n            - name: VAULT_K8S_POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_K8S_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: VAULT_ADDR\n              value: \"http://127.0.0.1:8200\"\n            - name: VAULT_API_ADDR\n              value: \"http://$(POD_IP):8200\"\n            - name: SKIP_CHOWN\n              value: \"true\"\n            - name: SKIP_SETCAP\n              value: \"true\"\n            - name: HOSTNAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_CLUSTER_ADDR\n              value: \"https://$(HOSTNAME).vault-internal:8201\"\n            - name: HOME\n              value: \"/home/vault\"\n            \n            \n            \n          volumeMounts:\n          \n  \n    \n            - name: data\n              mountPath: /vault/data\n    \n  \n  \n            - name: config\n              mountPath: /vault/config\n  \n            - name: home\n              mountPath: /home/vault\n          ports:\n            - containerPort: 8200\n              name: http\n            - containerPort: 8201\n              name: https-internal\n            - containerPort: 8202\n              name: http-rep\n          readinessProbe:\n            # Check status; unsealed vault servers return 0\n            # The exit code reflects the seal status:\n            #   0 - unsealed\n            #   1 - error\n            #   2 - sealed\n            exec:\n              command: [\"/bin/sh\", \"-ec\", \"vault status -tls-skip-verify\"]\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 3\n          lifecycle:\n            # Vault container doesn't receive SIGTERM from Kubernetes\n            # and after the grace period ends, Kube sends SIGKILL.  This\n            # causes issues with graceful shutdowns such as deregistering itself\n            # from Consul (zombie services).\n            preStop:\n              exec:\n                command: [\n                  \"/bin/sh\", \"-c\",\n                  # Adding a sleep here to give the pod eviction a\n                  # chance to propagate, so requests will not be made\n                  # to this pod while it's terminating\n                  \"sleep 5 && kill -SIGTERM $(pidof vault)\",\n                ]\n      \n  \n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      \n      spec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 10Gi\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/server-statefulset.yaml"}, "region": {"startLine": 4, "endLine": 178, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: vault\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.20.1\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: vault\n        component: server\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault\n                  app.kubernetes.io/instance: \"vault\"\n                  component: server\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      terminationGracePeriodSeconds: 10\n      serviceAccountName: vault\n      \n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      volumes:\n        \n        - name: config\n          configMap:\n            name: vault-config\n  \n        - name: home\n          emptyDir: {}\n      containers:\n        - name: vault\n          \n          image: hashicorp/vault:1.10.3\n          imagePullPolicy: IfNotPresent\n          command:\n          - \"/bin/sh\"\n          - \"-ec\"\n          args: \n          - |\n            cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\n            [ -n \"${HOST_IP}\" ] && sed -Ei \"s|HOST_IP|${HOST_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${POD_IP}\" ] && sed -Ei \"s|POD_IP|${POD_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${HOSTNAME}\" ] && sed -Ei \"s|HOSTNAME|${HOSTNAME?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${API_ADDR}\" ] && sed -Ei \"s|API_ADDR|${API_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${TRANSIT_ADDR}\" ] && sed -Ei \"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${RAFT_ADDR}\" ] && sed -Ei \"s|RAFT_ADDR|${RAFT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            /usr/local/bin/docker-entrypoint.sh vault server -config=/tmp/storageconfig.hcl \n   \n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: HOST_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.hostIP\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n            - name: VAULT_K8S_POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_K8S_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: VAULT_ADDR\n              value: \"http://127.0.0.1:8200\"\n            - name: VAULT_API_ADDR\n              value: \"http://$(POD_IP):8200\"\n            - name: SKIP_CHOWN\n              value: \"true\"\n            - name: SKIP_SETCAP\n              value: \"true\"\n            - name: HOSTNAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_CLUSTER_ADDR\n              value: \"https://$(HOSTNAME).vault-internal:8201\"\n            - name: HOME\n              value: \"/home/vault\"\n            \n            \n            \n          volumeMounts:\n          \n  \n    \n            - name: data\n              mountPath: /vault/data\n    \n  \n  \n            - name: config\n              mountPath: /vault/config\n  \n            - name: home\n              mountPath: /home/vault\n          ports:\n            - containerPort: 8200\n              name: http\n            - containerPort: 8201\n              name: https-internal\n            - containerPort: 8202\n              name: http-rep\n          readinessProbe:\n            # Check status; unsealed vault servers return 0\n            # The exit code reflects the seal status:\n            #   0 - unsealed\n            #   1 - error\n            #   2 - sealed\n            exec:\n              command: [\"/bin/sh\", \"-ec\", \"vault status -tls-skip-verify\"]\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 3\n          lifecycle:\n            # Vault container doesn't receive SIGTERM from Kubernetes\n            # and after the grace period ends, Kube sends SIGKILL.  This\n            # causes issues with graceful shutdowns such as deregistering itself\n            # from Consul (zombie services).\n            preStop:\n              exec:\n                command: [\n                  \"/bin/sh\", \"-c\",\n                  # Adding a sleep here to give the pod eviction a\n                  # chance to propagate, so requests will not be made\n                  # to this pod while it's terminating\n                  \"sleep 5 && kill -SIGTERM $(pidof vault)\",\n                ]\n      \n  \n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      \n      spec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 10Gi\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/server-statefulset.yaml"}, "region": {"startLine": 4, "endLine": 178, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: vault\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.20.1\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: vault\n        component: server\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault\n                  app.kubernetes.io/instance: \"vault\"\n                  component: server\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      terminationGracePeriodSeconds: 10\n      serviceAccountName: vault\n      \n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      volumes:\n        \n        - name: config\n          configMap:\n            name: vault-config\n  \n        - name: home\n          emptyDir: {}\n      containers:\n        - name: vault\n          \n          image: hashicorp/vault:1.10.3\n          imagePullPolicy: IfNotPresent\n          command:\n          - \"/bin/sh\"\n          - \"-ec\"\n          args: \n          - |\n            cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\n            [ -n \"${HOST_IP}\" ] && sed -Ei \"s|HOST_IP|${HOST_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${POD_IP}\" ] && sed -Ei \"s|POD_IP|${POD_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${HOSTNAME}\" ] && sed -Ei \"s|HOSTNAME|${HOSTNAME?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${API_ADDR}\" ] && sed -Ei \"s|API_ADDR|${API_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${TRANSIT_ADDR}\" ] && sed -Ei \"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${RAFT_ADDR}\" ] && sed -Ei \"s|RAFT_ADDR|${RAFT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            /usr/local/bin/docker-entrypoint.sh vault server -config=/tmp/storageconfig.hcl \n   \n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: HOST_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.hostIP\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n            - name: VAULT_K8S_POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_K8S_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: VAULT_ADDR\n              value: \"http://127.0.0.1:8200\"\n            - name: VAULT_API_ADDR\n              value: \"http://$(POD_IP):8200\"\n            - name: SKIP_CHOWN\n              value: \"true\"\n            - name: SKIP_SETCAP\n              value: \"true\"\n            - name: HOSTNAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_CLUSTER_ADDR\n              value: \"https://$(HOSTNAME).vault-internal:8201\"\n            - name: HOME\n              value: \"/home/vault\"\n            \n            \n            \n          volumeMounts:\n          \n  \n    \n            - name: data\n              mountPath: /vault/data\n    \n  \n  \n            - name: config\n              mountPath: /vault/config\n  \n            - name: home\n              mountPath: /home/vault\n          ports:\n            - containerPort: 8200\n              name: http\n            - containerPort: 8201\n              name: https-internal\n            - containerPort: 8202\n              name: http-rep\n          readinessProbe:\n            # Check status; unsealed vault servers return 0\n            # The exit code reflects the seal status:\n            #   0 - unsealed\n            #   1 - error\n            #   2 - sealed\n            exec:\n              command: [\"/bin/sh\", \"-ec\", \"vault status -tls-skip-verify\"]\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 3\n          lifecycle:\n            # Vault container doesn't receive SIGTERM from Kubernetes\n            # and after the grace period ends, Kube sends SIGKILL.  This\n            # causes issues with graceful shutdowns such as deregistering itself\n            # from Consul (zombie services).\n            preStop:\n              exec:\n                command: [\n                  \"/bin/sh\", \"-c\",\n                  # Adding a sleep here to give the pod eviction a\n                  # chance to propagate, so requests will not be made\n                  # to this pod while it's terminating\n                  \"sleep 5 && kill -SIGTERM $(pidof vault)\",\n                ]\n      \n  \n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      \n      spec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 10Gi\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/server-statefulset.yaml"}, "region": {"startLine": 4, "endLine": 178, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: vault\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.20.1\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: vault\n        component: server\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault\n                  app.kubernetes.io/instance: \"vault\"\n                  component: server\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      terminationGracePeriodSeconds: 10\n      serviceAccountName: vault\n      \n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      volumes:\n        \n        - name: config\n          configMap:\n            name: vault-config\n  \n        - name: home\n          emptyDir: {}\n      containers:\n        - name: vault\n          \n          image: hashicorp/vault:1.10.3\n          imagePullPolicy: IfNotPresent\n          command:\n          - \"/bin/sh\"\n          - \"-ec\"\n          args: \n          - |\n            cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\n            [ -n \"${HOST_IP}\" ] && sed -Ei \"s|HOST_IP|${HOST_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${POD_IP}\" ] && sed -Ei \"s|POD_IP|${POD_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${HOSTNAME}\" ] && sed -Ei \"s|HOSTNAME|${HOSTNAME?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${API_ADDR}\" ] && sed -Ei \"s|API_ADDR|${API_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${TRANSIT_ADDR}\" ] && sed -Ei \"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${RAFT_ADDR}\" ] && sed -Ei \"s|RAFT_ADDR|${RAFT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            /usr/local/bin/docker-entrypoint.sh vault server -config=/tmp/storageconfig.hcl \n   \n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: HOST_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.hostIP\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n            - name: VAULT_K8S_POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_K8S_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: VAULT_ADDR\n              value: \"http://127.0.0.1:8200\"\n            - name: VAULT_API_ADDR\n              value: \"http://$(POD_IP):8200\"\n            - name: SKIP_CHOWN\n              value: \"true\"\n            - name: SKIP_SETCAP\n              value: \"true\"\n            - name: HOSTNAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_CLUSTER_ADDR\n              value: \"https://$(HOSTNAME).vault-internal:8201\"\n            - name: HOME\n              value: \"/home/vault\"\n            \n            \n            \n          volumeMounts:\n          \n  \n    \n            - name: data\n              mountPath: /vault/data\n    \n  \n  \n            - name: config\n              mountPath: /vault/config\n  \n            - name: home\n              mountPath: /home/vault\n          ports:\n            - containerPort: 8200\n              name: http\n            - containerPort: 8201\n              name: https-internal\n            - containerPort: 8202\n              name: http-rep\n          readinessProbe:\n            # Check status; unsealed vault servers return 0\n            # The exit code reflects the seal status:\n            #   0 - unsealed\n            #   1 - error\n            #   2 - sealed\n            exec:\n              command: [\"/bin/sh\", \"-ec\", \"vault status -tls-skip-verify\"]\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 3\n          lifecycle:\n            # Vault container doesn't receive SIGTERM from Kubernetes\n            # and after the grace period ends, Kube sends SIGKILL.  This\n            # causes issues with graceful shutdowns such as deregistering itself\n            # from Consul (zombie services).\n            preStop:\n              exec:\n                command: [\n                  \"/bin/sh\", \"-c\",\n                  # Adding a sleep here to give the pod eviction a\n                  # chance to propagate, so requests will not be made\n                  # to this pod while it's terminating\n                  \"sleep 5 && kill -SIGTERM $(pidof vault)\",\n                ]\n      \n  \n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      \n      spec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 10Gi\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/server-statefulset.yaml"}, "region": {"startLine": 4, "endLine": 178, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: vault\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.20.1\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: vault\n        component: server\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault\n                  app.kubernetes.io/instance: \"vault\"\n                  component: server\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      terminationGracePeriodSeconds: 10\n      serviceAccountName: vault\n      \n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      volumes:\n        \n        - name: config\n          configMap:\n            name: vault-config\n  \n        - name: home\n          emptyDir: {}\n      containers:\n        - name: vault\n          \n          image: hashicorp/vault:1.10.3\n          imagePullPolicy: IfNotPresent\n          command:\n          - \"/bin/sh\"\n          - \"-ec\"\n          args: \n          - |\n            cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\n            [ -n \"${HOST_IP}\" ] && sed -Ei \"s|HOST_IP|${HOST_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${POD_IP}\" ] && sed -Ei \"s|POD_IP|${POD_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${HOSTNAME}\" ] && sed -Ei \"s|HOSTNAME|${HOSTNAME?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${API_ADDR}\" ] && sed -Ei \"s|API_ADDR|${API_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${TRANSIT_ADDR}\" ] && sed -Ei \"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${RAFT_ADDR}\" ] && sed -Ei \"s|RAFT_ADDR|${RAFT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            /usr/local/bin/docker-entrypoint.sh vault server -config=/tmp/storageconfig.hcl \n   \n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: HOST_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.hostIP\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n            - name: VAULT_K8S_POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_K8S_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: VAULT_ADDR\n              value: \"http://127.0.0.1:8200\"\n            - name: VAULT_API_ADDR\n              value: \"http://$(POD_IP):8200\"\n            - name: SKIP_CHOWN\n              value: \"true\"\n            - name: SKIP_SETCAP\n              value: \"true\"\n            - name: HOSTNAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_CLUSTER_ADDR\n              value: \"https://$(HOSTNAME).vault-internal:8201\"\n            - name: HOME\n              value: \"/home/vault\"\n            \n            \n            \n          volumeMounts:\n          \n  \n    \n            - name: data\n              mountPath: /vault/data\n    \n  \n  \n            - name: config\n              mountPath: /vault/config\n  \n            - name: home\n              mountPath: /home/vault\n          ports:\n            - containerPort: 8200\n              name: http\n            - containerPort: 8201\n              name: https-internal\n            - containerPort: 8202\n              name: http-rep\n          readinessProbe:\n            # Check status; unsealed vault servers return 0\n            # The exit code reflects the seal status:\n            #   0 - unsealed\n            #   1 - error\n            #   2 - sealed\n            exec:\n              command: [\"/bin/sh\", \"-ec\", \"vault status -tls-skip-verify\"]\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 3\n          lifecycle:\n            # Vault container doesn't receive SIGTERM from Kubernetes\n            # and after the grace period ends, Kube sends SIGKILL.  This\n            # causes issues with graceful shutdowns such as deregistering itself\n            # from Consul (zombie services).\n            preStop:\n              exec:\n                command: [\n                  \"/bin/sh\", \"-c\",\n                  # Adding a sleep here to give the pod eviction a\n                  # chance to propagate, so requests will not be made\n                  # to this pod while it's terminating\n                  \"sleep 5 && kill -SIGTERM $(pidof vault)\",\n                ]\n      \n  \n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      \n      spec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 10Gi\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/server-statefulset.yaml"}, "region": {"startLine": 4, "endLine": 178, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: vault\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.20.1\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: vault\n        component: server\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault\n                  app.kubernetes.io/instance: \"vault\"\n                  component: server\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      terminationGracePeriodSeconds: 10\n      serviceAccountName: vault\n      \n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      volumes:\n        \n        - name: config\n          configMap:\n            name: vault-config\n  \n        - name: home\n          emptyDir: {}\n      containers:\n        - name: vault\n          \n          image: hashicorp/vault:1.10.3\n          imagePullPolicy: IfNotPresent\n          command:\n          - \"/bin/sh\"\n          - \"-ec\"\n          args: \n          - |\n            cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\n            [ -n \"${HOST_IP}\" ] && sed -Ei \"s|HOST_IP|${HOST_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${POD_IP}\" ] && sed -Ei \"s|POD_IP|${POD_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${HOSTNAME}\" ] && sed -Ei \"s|HOSTNAME|${HOSTNAME?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${API_ADDR}\" ] && sed -Ei \"s|API_ADDR|${API_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${TRANSIT_ADDR}\" ] && sed -Ei \"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${RAFT_ADDR}\" ] && sed -Ei \"s|RAFT_ADDR|${RAFT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            /usr/local/bin/docker-entrypoint.sh vault server -config=/tmp/storageconfig.hcl \n   \n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: HOST_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.hostIP\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n            - name: VAULT_K8S_POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_K8S_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: VAULT_ADDR\n              value: \"http://127.0.0.1:8200\"\n            - name: VAULT_API_ADDR\n              value: \"http://$(POD_IP):8200\"\n            - name: SKIP_CHOWN\n              value: \"true\"\n            - name: SKIP_SETCAP\n              value: \"true\"\n            - name: HOSTNAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_CLUSTER_ADDR\n              value: \"https://$(HOSTNAME).vault-internal:8201\"\n            - name: HOME\n              value: \"/home/vault\"\n            \n            \n            \n          volumeMounts:\n          \n  \n    \n            - name: data\n              mountPath: /vault/data\n    \n  \n  \n            - name: config\n              mountPath: /vault/config\n  \n            - name: home\n              mountPath: /home/vault\n          ports:\n            - containerPort: 8200\n              name: http\n            - containerPort: 8201\n              name: https-internal\n            - containerPort: 8202\n              name: http-rep\n          readinessProbe:\n            # Check status; unsealed vault servers return 0\n            # The exit code reflects the seal status:\n            #   0 - unsealed\n            #   1 - error\n            #   2 - sealed\n            exec:\n              command: [\"/bin/sh\", \"-ec\", \"vault status -tls-skip-verify\"]\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 3\n          lifecycle:\n            # Vault container doesn't receive SIGTERM from Kubernetes\n            # and after the grace period ends, Kube sends SIGKILL.  This\n            # causes issues with graceful shutdowns such as deregistering itself\n            # from Consul (zombie services).\n            preStop:\n              exec:\n                command: [\n                  \"/bin/sh\", \"-c\",\n                  # Adding a sleep here to give the pod eviction a\n                  # chance to propagate, so requests will not be made\n                  # to this pod while it's terminating\n                  \"sleep 5 && kill -SIGTERM $(pidof vault)\",\n                ]\n      \n  \n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      \n      spec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 10Gi\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/server-statefulset.yaml"}, "region": {"startLine": 4, "endLine": 178, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: vault\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.20.1\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: vault\n        component: server\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault\n                  app.kubernetes.io/instance: \"vault\"\n                  component: server\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      terminationGracePeriodSeconds: 10\n      serviceAccountName: vault\n      \n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      volumes:\n        \n        - name: config\n          configMap:\n            name: vault-config\n  \n        - name: home\n          emptyDir: {}\n      containers:\n        - name: vault\n          \n          image: hashicorp/vault:1.10.3\n          imagePullPolicy: IfNotPresent\n          command:\n          - \"/bin/sh\"\n          - \"-ec\"\n          args: \n          - |\n            cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\n            [ -n \"${HOST_IP}\" ] && sed -Ei \"s|HOST_IP|${HOST_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${POD_IP}\" ] && sed -Ei \"s|POD_IP|${POD_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${HOSTNAME}\" ] && sed -Ei \"s|HOSTNAME|${HOSTNAME?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${API_ADDR}\" ] && sed -Ei \"s|API_ADDR|${API_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${TRANSIT_ADDR}\" ] && sed -Ei \"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${RAFT_ADDR}\" ] && sed -Ei \"s|RAFT_ADDR|${RAFT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            /usr/local/bin/docker-entrypoint.sh vault server -config=/tmp/storageconfig.hcl \n   \n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: HOST_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.hostIP\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n            - name: VAULT_K8S_POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_K8S_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: VAULT_ADDR\n              value: \"http://127.0.0.1:8200\"\n            - name: VAULT_API_ADDR\n              value: \"http://$(POD_IP):8200\"\n            - name: SKIP_CHOWN\n              value: \"true\"\n            - name: SKIP_SETCAP\n              value: \"true\"\n            - name: HOSTNAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_CLUSTER_ADDR\n              value: \"https://$(HOSTNAME).vault-internal:8201\"\n            - name: HOME\n              value: \"/home/vault\"\n            \n            \n            \n          volumeMounts:\n          \n  \n    \n            - name: data\n              mountPath: /vault/data\n    \n  \n  \n            - name: config\n              mountPath: /vault/config\n  \n            - name: home\n              mountPath: /home/vault\n          ports:\n            - containerPort: 8200\n              name: http\n            - containerPort: 8201\n              name: https-internal\n            - containerPort: 8202\n              name: http-rep\n          readinessProbe:\n            # Check status; unsealed vault servers return 0\n            # The exit code reflects the seal status:\n            #   0 - unsealed\n            #   1 - error\n            #   2 - sealed\n            exec:\n              command: [\"/bin/sh\", \"-ec\", \"vault status -tls-skip-verify\"]\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 3\n          lifecycle:\n            # Vault container doesn't receive SIGTERM from Kubernetes\n            # and after the grace period ends, Kube sends SIGKILL.  This\n            # causes issues with graceful shutdowns such as deregistering itself\n            # from Consul (zombie services).\n            preStop:\n              exec:\n                command: [\n                  \"/bin/sh\", \"-c\",\n                  # Adding a sleep here to give the pod eviction a\n                  # chance to propagate, so requests will not be made\n                  # to this pod while it's terminating\n                  \"sleep 5 && kill -SIGTERM $(pidof vault)\",\n                ]\n      \n  \n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      \n      spec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 10Gi\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/server-statefulset.yaml"}, "region": {"startLine": 4, "endLine": 178, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: vault\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.20.1\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: vault\n        component: server\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault\n                  app.kubernetes.io/instance: \"vault\"\n                  component: server\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      terminationGracePeriodSeconds: 10\n      serviceAccountName: vault\n      \n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      volumes:\n        \n        - name: config\n          configMap:\n            name: vault-config\n  \n        - name: home\n          emptyDir: {}\n      containers:\n        - name: vault\n          \n          image: hashicorp/vault:1.10.3\n          imagePullPolicy: IfNotPresent\n          command:\n          - \"/bin/sh\"\n          - \"-ec\"\n          args: \n          - |\n            cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\n            [ -n \"${HOST_IP}\" ] && sed -Ei \"s|HOST_IP|${HOST_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${POD_IP}\" ] && sed -Ei \"s|POD_IP|${POD_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${HOSTNAME}\" ] && sed -Ei \"s|HOSTNAME|${HOSTNAME?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${API_ADDR}\" ] && sed -Ei \"s|API_ADDR|${API_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${TRANSIT_ADDR}\" ] && sed -Ei \"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${RAFT_ADDR}\" ] && sed -Ei \"s|RAFT_ADDR|${RAFT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            /usr/local/bin/docker-entrypoint.sh vault server -config=/tmp/storageconfig.hcl \n   \n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: HOST_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.hostIP\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n            - name: VAULT_K8S_POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_K8S_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: VAULT_ADDR\n              value: \"http://127.0.0.1:8200\"\n            - name: VAULT_API_ADDR\n              value: \"http://$(POD_IP):8200\"\n            - name: SKIP_CHOWN\n              value: \"true\"\n            - name: SKIP_SETCAP\n              value: \"true\"\n            - name: HOSTNAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_CLUSTER_ADDR\n              value: \"https://$(HOSTNAME).vault-internal:8201\"\n            - name: HOME\n              value: \"/home/vault\"\n            \n            \n            \n          volumeMounts:\n          \n  \n    \n            - name: data\n              mountPath: /vault/data\n    \n  \n  \n            - name: config\n              mountPath: /vault/config\n  \n            - name: home\n              mountPath: /home/vault\n          ports:\n            - containerPort: 8200\n              name: http\n            - containerPort: 8201\n              name: https-internal\n            - containerPort: 8202\n              name: http-rep\n          readinessProbe:\n            # Check status; unsealed vault servers return 0\n            # The exit code reflects the seal status:\n            #   0 - unsealed\n            #   1 - error\n            #   2 - sealed\n            exec:\n              command: [\"/bin/sh\", \"-ec\", \"vault status -tls-skip-verify\"]\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 3\n          lifecycle:\n            # Vault container doesn't receive SIGTERM from Kubernetes\n            # and after the grace period ends, Kube sends SIGKILL.  This\n            # causes issues with graceful shutdowns such as deregistering itself\n            # from Consul (zombie services).\n            preStop:\n              exec:\n                command: [\n                  \"/bin/sh\", \"-c\",\n                  # Adding a sleep here to give the pod eviction a\n                  # chance to propagate, so requests will not be made\n                  # to this pod while it's terminating\n                  \"sleep 5 && kill -SIGTERM $(pidof vault)\",\n                ]\n      \n  \n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      \n      spec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 10Gi\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/server-statefulset.yaml"}, "region": {"startLine": 4, "endLine": 178, "snippet": {"text": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: vault\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.20.1\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: vault\n        component: server\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault\n                  app.kubernetes.io/instance: \"vault\"\n                  component: server\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      terminationGracePeriodSeconds: 10\n      serviceAccountName: vault\n      \n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      volumes:\n        \n        - name: config\n          configMap:\n            name: vault-config\n  \n        - name: home\n          emptyDir: {}\n      containers:\n        - name: vault\n          \n          image: hashicorp/vault:1.10.3\n          imagePullPolicy: IfNotPresent\n          command:\n          - \"/bin/sh\"\n          - \"-ec\"\n          args: \n          - |\n            cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\n            [ -n \"${HOST_IP}\" ] && sed -Ei \"s|HOST_IP|${HOST_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${POD_IP}\" ] && sed -Ei \"s|POD_IP|${POD_IP?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${HOSTNAME}\" ] && sed -Ei \"s|HOSTNAME|${HOSTNAME?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${API_ADDR}\" ] && sed -Ei \"s|API_ADDR|${API_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${TRANSIT_ADDR}\" ] && sed -Ei \"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            [ -n \"${RAFT_ADDR}\" ] && sed -Ei \"s|RAFT_ADDR|${RAFT_ADDR?}|g\" /tmp/storageconfig.hcl;\n            /usr/local/bin/docker-entrypoint.sh vault server -config=/tmp/storageconfig.hcl \n   \n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: HOST_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.hostIP\n            - name: POD_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n            - name: VAULT_K8S_POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_K8S_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: VAULT_ADDR\n              value: \"http://127.0.0.1:8200\"\n            - name: VAULT_API_ADDR\n              value: \"http://$(POD_IP):8200\"\n            - name: SKIP_CHOWN\n              value: \"true\"\n            - name: SKIP_SETCAP\n              value: \"true\"\n            - name: HOSTNAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: VAULT_CLUSTER_ADDR\n              value: \"https://$(HOSTNAME).vault-internal:8201\"\n            - name: HOME\n              value: \"/home/vault\"\n            \n            \n            \n          volumeMounts:\n          \n  \n    \n            - name: data\n              mountPath: /vault/data\n    \n  \n  \n            - name: config\n              mountPath: /vault/config\n  \n            - name: home\n              mountPath: /home/vault\n          ports:\n            - containerPort: 8200\n              name: http\n            - containerPort: 8201\n              name: https-internal\n            - containerPort: 8202\n              name: http-rep\n          readinessProbe:\n            # Check status; unsealed vault servers return 0\n            # The exit code reflects the seal status:\n            #   0 - unsealed\n            #   1 - error\n            #   2 - sealed\n            exec:\n              command: [\"/bin/sh\", \"-ec\", \"vault status -tls-skip-verify\"]\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 3\n          lifecycle:\n            # Vault container doesn't receive SIGTERM from Kubernetes\n            # and after the grace period ends, Kube sends SIGKILL.  This\n            # causes issues with graceful shutdowns such as deregistering itself\n            # from Consul (zombie services).\n            preStop:\n              exec:\n                command: [\n                  \"/bin/sh\", \"-c\",\n                  # Adding a sleep here to give the pod eviction a\n                  # chance to propagate, so requests will not be made\n                  # to this pod while it's terminating\n                  \"sleep 5 && kill -SIGTERM $(pidof vault)\",\n                ]\n      \n  \n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      \n      spec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 10Gi\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/server-headless-service.yaml"}, "region": {"startLine": 4, "endLine": 29, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: vault-internal\n  namespace: default\n  labels:\n    helm.sh/chart: vault-0.20.1\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n\nspec:\n  clusterIP: None\n  publishNotReadyAddresses: true\n  ports:\n    - name: \"http\"\n      port: 8200\n      targetPort: 8200\n    - name: https-internal\n      port: 8201\n      targetPort: 8201\n  selector:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: vault\n    component: server\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/injector-deployment.yaml"}, "region": {"startLine": 4, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: vault\n      component: webhook\n  \n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: vault\n        component: webhook\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault-agent-injector\n                  app.kubernetes.io/instance: \"vault\"\n                  component: webhook\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      serviceAccountName: \"vault-agent-injector\"\n      hostNetwork: false\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n        - name: sidecar-injector\n          \n          image: \"hashicorp/vault-k8s:0.16.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: AGENT_INJECT_LISTEN\n              value: :8080\n            - name: AGENT_INJECT_LOG_LEVEL\n              value: info\n            - name: AGENT_INJECT_VAULT_ADDR\n              value: http://vault.default.svc:8200\n            - name: AGENT_INJECT_VAULT_AUTH_PATH\n              value: auth/kubernetes\n            - name: AGENT_INJECT_VAULT_IMAGE\n              value: \"hashicorp/vault:1.10.3\"\n            - name: AGENT_INJECT_TLS_AUTO\n              value: vault-agent-injector-cfg\n            - name: AGENT_INJECT_TLS_AUTO_HOSTS\n              value: vault-agent-injector-svc,vault-agent-injector-svc.default,vault-agent-injector-svc.default.svc\n            - name: AGENT_INJECT_LOG_FORMAT\n              value: standard\n            - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n              value: \"false\"\n            - name: AGENT_INJECT_CPU_REQUEST\n              value: \"250m\"\n            - name: AGENT_INJECT_CPU_LIMIT\n              value: \"500m\"\n            - name: AGENT_INJECT_MEM_REQUEST\n              value: \"64Mi\"\n            - name: AGENT_INJECT_MEM_LIMIT\n              value: \"128Mi\"\n            - name: AGENT_INJECT_DEFAULT_TEMPLATE\n              value: \"map\"\n            - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n              value: \"true\"\n            \n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n          args:\n            - agent-inject\n            - 2>&1\n          livenessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n          readinessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/injector-deployment.yaml"}, "region": {"startLine": 4, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: vault\n      component: webhook\n  \n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: vault\n        component: webhook\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault-agent-injector\n                  app.kubernetes.io/instance: \"vault\"\n                  component: webhook\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      serviceAccountName: \"vault-agent-injector\"\n      hostNetwork: false\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n        - name: sidecar-injector\n          \n          image: \"hashicorp/vault-k8s:0.16.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: AGENT_INJECT_LISTEN\n              value: :8080\n            - name: AGENT_INJECT_LOG_LEVEL\n              value: info\n            - name: AGENT_INJECT_VAULT_ADDR\n              value: http://vault.default.svc:8200\n            - name: AGENT_INJECT_VAULT_AUTH_PATH\n              value: auth/kubernetes\n            - name: AGENT_INJECT_VAULT_IMAGE\n              value: \"hashicorp/vault:1.10.3\"\n            - name: AGENT_INJECT_TLS_AUTO\n              value: vault-agent-injector-cfg\n            - name: AGENT_INJECT_TLS_AUTO_HOSTS\n              value: vault-agent-injector-svc,vault-agent-injector-svc.default,vault-agent-injector-svc.default.svc\n            - name: AGENT_INJECT_LOG_FORMAT\n              value: standard\n            - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n              value: \"false\"\n            - name: AGENT_INJECT_CPU_REQUEST\n              value: \"250m\"\n            - name: AGENT_INJECT_CPU_LIMIT\n              value: \"500m\"\n            - name: AGENT_INJECT_MEM_REQUEST\n              value: \"64Mi\"\n            - name: AGENT_INJECT_MEM_LIMIT\n              value: \"128Mi\"\n            - name: AGENT_INJECT_DEFAULT_TEMPLATE\n              value: \"map\"\n            - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n              value: \"true\"\n            \n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n          args:\n            - agent-inject\n            - 2>&1\n          livenessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n          readinessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/injector-deployment.yaml"}, "region": {"startLine": 4, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: vault\n      component: webhook\n  \n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: vault\n        component: webhook\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault-agent-injector\n                  app.kubernetes.io/instance: \"vault\"\n                  component: webhook\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      serviceAccountName: \"vault-agent-injector\"\n      hostNetwork: false\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n        - name: sidecar-injector\n          \n          image: \"hashicorp/vault-k8s:0.16.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: AGENT_INJECT_LISTEN\n              value: :8080\n            - name: AGENT_INJECT_LOG_LEVEL\n              value: info\n            - name: AGENT_INJECT_VAULT_ADDR\n              value: http://vault.default.svc:8200\n            - name: AGENT_INJECT_VAULT_AUTH_PATH\n              value: auth/kubernetes\n            - name: AGENT_INJECT_VAULT_IMAGE\n              value: \"hashicorp/vault:1.10.3\"\n            - name: AGENT_INJECT_TLS_AUTO\n              value: vault-agent-injector-cfg\n            - name: AGENT_INJECT_TLS_AUTO_HOSTS\n              value: vault-agent-injector-svc,vault-agent-injector-svc.default,vault-agent-injector-svc.default.svc\n            - name: AGENT_INJECT_LOG_FORMAT\n              value: standard\n            - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n              value: \"false\"\n            - name: AGENT_INJECT_CPU_REQUEST\n              value: \"250m\"\n            - name: AGENT_INJECT_CPU_LIMIT\n              value: \"500m\"\n            - name: AGENT_INJECT_MEM_REQUEST\n              value: \"64Mi\"\n            - name: AGENT_INJECT_MEM_LIMIT\n              value: \"128Mi\"\n            - name: AGENT_INJECT_DEFAULT_TEMPLATE\n              value: \"map\"\n            - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n              value: \"true\"\n            \n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n          args:\n            - agent-inject\n            - 2>&1\n          livenessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n          readinessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/injector-deployment.yaml"}, "region": {"startLine": 4, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: vault\n      component: webhook\n  \n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: vault\n        component: webhook\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault-agent-injector\n                  app.kubernetes.io/instance: \"vault\"\n                  component: webhook\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      serviceAccountName: \"vault-agent-injector\"\n      hostNetwork: false\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n        - name: sidecar-injector\n          \n          image: \"hashicorp/vault-k8s:0.16.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: AGENT_INJECT_LISTEN\n              value: :8080\n            - name: AGENT_INJECT_LOG_LEVEL\n              value: info\n            - name: AGENT_INJECT_VAULT_ADDR\n              value: http://vault.default.svc:8200\n            - name: AGENT_INJECT_VAULT_AUTH_PATH\n              value: auth/kubernetes\n            - name: AGENT_INJECT_VAULT_IMAGE\n              value: \"hashicorp/vault:1.10.3\"\n            - name: AGENT_INJECT_TLS_AUTO\n              value: vault-agent-injector-cfg\n            - name: AGENT_INJECT_TLS_AUTO_HOSTS\n              value: vault-agent-injector-svc,vault-agent-injector-svc.default,vault-agent-injector-svc.default.svc\n            - name: AGENT_INJECT_LOG_FORMAT\n              value: standard\n            - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n              value: \"false\"\n            - name: AGENT_INJECT_CPU_REQUEST\n              value: \"250m\"\n            - name: AGENT_INJECT_CPU_LIMIT\n              value: \"500m\"\n            - name: AGENT_INJECT_MEM_REQUEST\n              value: \"64Mi\"\n            - name: AGENT_INJECT_MEM_LIMIT\n              value: \"128Mi\"\n            - name: AGENT_INJECT_DEFAULT_TEMPLATE\n              value: \"map\"\n            - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n              value: \"true\"\n            \n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n          args:\n            - agent-inject\n            - 2>&1\n          livenessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n          readinessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/injector-deployment.yaml"}, "region": {"startLine": 4, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: vault\n      component: webhook\n  \n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: vault\n        component: webhook\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault-agent-injector\n                  app.kubernetes.io/instance: \"vault\"\n                  component: webhook\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      serviceAccountName: \"vault-agent-injector\"\n      hostNetwork: false\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n        - name: sidecar-injector\n          \n          image: \"hashicorp/vault-k8s:0.16.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: AGENT_INJECT_LISTEN\n              value: :8080\n            - name: AGENT_INJECT_LOG_LEVEL\n              value: info\n            - name: AGENT_INJECT_VAULT_ADDR\n              value: http://vault.default.svc:8200\n            - name: AGENT_INJECT_VAULT_AUTH_PATH\n              value: auth/kubernetes\n            - name: AGENT_INJECT_VAULT_IMAGE\n              value: \"hashicorp/vault:1.10.3\"\n            - name: AGENT_INJECT_TLS_AUTO\n              value: vault-agent-injector-cfg\n            - name: AGENT_INJECT_TLS_AUTO_HOSTS\n              value: vault-agent-injector-svc,vault-agent-injector-svc.default,vault-agent-injector-svc.default.svc\n            - name: AGENT_INJECT_LOG_FORMAT\n              value: standard\n            - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n              value: \"false\"\n            - name: AGENT_INJECT_CPU_REQUEST\n              value: \"250m\"\n            - name: AGENT_INJECT_CPU_LIMIT\n              value: \"500m\"\n            - name: AGENT_INJECT_MEM_REQUEST\n              value: \"64Mi\"\n            - name: AGENT_INJECT_MEM_LIMIT\n              value: \"128Mi\"\n            - name: AGENT_INJECT_DEFAULT_TEMPLATE\n              value: \"map\"\n            - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n              value: \"true\"\n            \n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n          args:\n            - agent-inject\n            - 2>&1\n          livenessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n          readinessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/injector-deployment.yaml"}, "region": {"startLine": 4, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: vault\n      component: webhook\n  \n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: vault\n        component: webhook\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault-agent-injector\n                  app.kubernetes.io/instance: \"vault\"\n                  component: webhook\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      serviceAccountName: \"vault-agent-injector\"\n      hostNetwork: false\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n        - name: sidecar-injector\n          \n          image: \"hashicorp/vault-k8s:0.16.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: AGENT_INJECT_LISTEN\n              value: :8080\n            - name: AGENT_INJECT_LOG_LEVEL\n              value: info\n            - name: AGENT_INJECT_VAULT_ADDR\n              value: http://vault.default.svc:8200\n            - name: AGENT_INJECT_VAULT_AUTH_PATH\n              value: auth/kubernetes\n            - name: AGENT_INJECT_VAULT_IMAGE\n              value: \"hashicorp/vault:1.10.3\"\n            - name: AGENT_INJECT_TLS_AUTO\n              value: vault-agent-injector-cfg\n            - name: AGENT_INJECT_TLS_AUTO_HOSTS\n              value: vault-agent-injector-svc,vault-agent-injector-svc.default,vault-agent-injector-svc.default.svc\n            - name: AGENT_INJECT_LOG_FORMAT\n              value: standard\n            - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n              value: \"false\"\n            - name: AGENT_INJECT_CPU_REQUEST\n              value: \"250m\"\n            - name: AGENT_INJECT_CPU_LIMIT\n              value: \"500m\"\n            - name: AGENT_INJECT_MEM_REQUEST\n              value: \"64Mi\"\n            - name: AGENT_INJECT_MEM_LIMIT\n              value: \"128Mi\"\n            - name: AGENT_INJECT_DEFAULT_TEMPLATE\n              value: \"map\"\n            - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n              value: \"true\"\n            \n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n          args:\n            - agent-inject\n            - 2>&1\n          livenessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n          readinessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/injector-deployment.yaml"}, "region": {"startLine": 4, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: vault\n      component: webhook\n  \n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: vault\n        component: webhook\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault-agent-injector\n                  app.kubernetes.io/instance: \"vault\"\n                  component: webhook\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      serviceAccountName: \"vault-agent-injector\"\n      hostNetwork: false\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n        - name: sidecar-injector\n          \n          image: \"hashicorp/vault-k8s:0.16.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: AGENT_INJECT_LISTEN\n              value: :8080\n            - name: AGENT_INJECT_LOG_LEVEL\n              value: info\n            - name: AGENT_INJECT_VAULT_ADDR\n              value: http://vault.default.svc:8200\n            - name: AGENT_INJECT_VAULT_AUTH_PATH\n              value: auth/kubernetes\n            - name: AGENT_INJECT_VAULT_IMAGE\n              value: \"hashicorp/vault:1.10.3\"\n            - name: AGENT_INJECT_TLS_AUTO\n              value: vault-agent-injector-cfg\n            - name: AGENT_INJECT_TLS_AUTO_HOSTS\n              value: vault-agent-injector-svc,vault-agent-injector-svc.default,vault-agent-injector-svc.default.svc\n            - name: AGENT_INJECT_LOG_FORMAT\n              value: standard\n            - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n              value: \"false\"\n            - name: AGENT_INJECT_CPU_REQUEST\n              value: \"250m\"\n            - name: AGENT_INJECT_CPU_LIMIT\n              value: \"500m\"\n            - name: AGENT_INJECT_MEM_REQUEST\n              value: \"64Mi\"\n            - name: AGENT_INJECT_MEM_LIMIT\n              value: \"128Mi\"\n            - name: AGENT_INJECT_DEFAULT_TEMPLATE\n              value: \"map\"\n            - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n              value: \"true\"\n            \n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n          args:\n            - agent-inject\n            - 2>&1\n          livenessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n          readinessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/injector-deployment.yaml"}, "region": {"startLine": 4, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: vault\n      component: webhook\n  \n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: vault\n        component: webhook\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault-agent-injector\n                  app.kubernetes.io/instance: \"vault\"\n                  component: webhook\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      serviceAccountName: \"vault-agent-injector\"\n      hostNetwork: false\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n        - name: sidecar-injector\n          \n          image: \"hashicorp/vault-k8s:0.16.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: AGENT_INJECT_LISTEN\n              value: :8080\n            - name: AGENT_INJECT_LOG_LEVEL\n              value: info\n            - name: AGENT_INJECT_VAULT_ADDR\n              value: http://vault.default.svc:8200\n            - name: AGENT_INJECT_VAULT_AUTH_PATH\n              value: auth/kubernetes\n            - name: AGENT_INJECT_VAULT_IMAGE\n              value: \"hashicorp/vault:1.10.3\"\n            - name: AGENT_INJECT_TLS_AUTO\n              value: vault-agent-injector-cfg\n            - name: AGENT_INJECT_TLS_AUTO_HOSTS\n              value: vault-agent-injector-svc,vault-agent-injector-svc.default,vault-agent-injector-svc.default.svc\n            - name: AGENT_INJECT_LOG_FORMAT\n              value: standard\n            - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n              value: \"false\"\n            - name: AGENT_INJECT_CPU_REQUEST\n              value: \"250m\"\n            - name: AGENT_INJECT_CPU_LIMIT\n              value: \"500m\"\n            - name: AGENT_INJECT_MEM_REQUEST\n              value: \"64Mi\"\n            - name: AGENT_INJECT_MEM_LIMIT\n              value: \"128Mi\"\n            - name: AGENT_INJECT_DEFAULT_TEMPLATE\n              value: \"map\"\n            - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n              value: \"true\"\n            \n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n          args:\n            - agent-inject\n            - 2>&1\n          livenessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n          readinessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/injector-deployment.yaml"}, "region": {"startLine": 4, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: vault\n      component: webhook\n  \n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: vault\n        component: webhook\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault-agent-injector\n                  app.kubernetes.io/instance: \"vault\"\n                  component: webhook\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      serviceAccountName: \"vault-agent-injector\"\n      hostNetwork: false\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n        - name: sidecar-injector\n          \n          image: \"hashicorp/vault-k8s:0.16.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: AGENT_INJECT_LISTEN\n              value: :8080\n            - name: AGENT_INJECT_LOG_LEVEL\n              value: info\n            - name: AGENT_INJECT_VAULT_ADDR\n              value: http://vault.default.svc:8200\n            - name: AGENT_INJECT_VAULT_AUTH_PATH\n              value: auth/kubernetes\n            - name: AGENT_INJECT_VAULT_IMAGE\n              value: \"hashicorp/vault:1.10.3\"\n            - name: AGENT_INJECT_TLS_AUTO\n              value: vault-agent-injector-cfg\n            - name: AGENT_INJECT_TLS_AUTO_HOSTS\n              value: vault-agent-injector-svc,vault-agent-injector-svc.default,vault-agent-injector-svc.default.svc\n            - name: AGENT_INJECT_LOG_FORMAT\n              value: standard\n            - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n              value: \"false\"\n            - name: AGENT_INJECT_CPU_REQUEST\n              value: \"250m\"\n            - name: AGENT_INJECT_CPU_LIMIT\n              value: \"500m\"\n            - name: AGENT_INJECT_MEM_REQUEST\n              value: \"64Mi\"\n            - name: AGENT_INJECT_MEM_LIMIT\n              value: \"128Mi\"\n            - name: AGENT_INJECT_DEFAULT_TEMPLATE\n              value: \"map\"\n            - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n              value: \"true\"\n            \n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n          args:\n            - agent-inject\n            - 2>&1\n          livenessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n          readinessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/injector-deployment.yaml"}, "region": {"startLine": 4, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: vault\n      component: webhook\n  \n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: vault\n        component: webhook\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault-agent-injector\n                  app.kubernetes.io/instance: \"vault\"\n                  component: webhook\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      serviceAccountName: \"vault-agent-injector\"\n      hostNetwork: false\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n        - name: sidecar-injector\n          \n          image: \"hashicorp/vault-k8s:0.16.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: AGENT_INJECT_LISTEN\n              value: :8080\n            - name: AGENT_INJECT_LOG_LEVEL\n              value: info\n            - name: AGENT_INJECT_VAULT_ADDR\n              value: http://vault.default.svc:8200\n            - name: AGENT_INJECT_VAULT_AUTH_PATH\n              value: auth/kubernetes\n            - name: AGENT_INJECT_VAULT_IMAGE\n              value: \"hashicorp/vault:1.10.3\"\n            - name: AGENT_INJECT_TLS_AUTO\n              value: vault-agent-injector-cfg\n            - name: AGENT_INJECT_TLS_AUTO_HOSTS\n              value: vault-agent-injector-svc,vault-agent-injector-svc.default,vault-agent-injector-svc.default.svc\n            - name: AGENT_INJECT_LOG_FORMAT\n              value: standard\n            - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n              value: \"false\"\n            - name: AGENT_INJECT_CPU_REQUEST\n              value: \"250m\"\n            - name: AGENT_INJECT_CPU_LIMIT\n              value: \"500m\"\n            - name: AGENT_INJECT_MEM_REQUEST\n              value: \"64Mi\"\n            - name: AGENT_INJECT_MEM_LIMIT\n              value: \"128Mi\"\n            - name: AGENT_INJECT_DEFAULT_TEMPLATE\n              value: \"map\"\n            - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n              value: \"true\"\n            \n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n          args:\n            - agent-inject\n            - 2>&1\n          livenessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n          readinessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/injector-deployment.yaml"}, "region": {"startLine": 4, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: vault\n      component: webhook\n  \n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: vault\n        component: webhook\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault-agent-injector\n                  app.kubernetes.io/instance: \"vault\"\n                  component: webhook\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      serviceAccountName: \"vault-agent-injector\"\n      hostNetwork: false\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n        - name: sidecar-injector\n          \n          image: \"hashicorp/vault-k8s:0.16.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: AGENT_INJECT_LISTEN\n              value: :8080\n            - name: AGENT_INJECT_LOG_LEVEL\n              value: info\n            - name: AGENT_INJECT_VAULT_ADDR\n              value: http://vault.default.svc:8200\n            - name: AGENT_INJECT_VAULT_AUTH_PATH\n              value: auth/kubernetes\n            - name: AGENT_INJECT_VAULT_IMAGE\n              value: \"hashicorp/vault:1.10.3\"\n            - name: AGENT_INJECT_TLS_AUTO\n              value: vault-agent-injector-cfg\n            - name: AGENT_INJECT_TLS_AUTO_HOSTS\n              value: vault-agent-injector-svc,vault-agent-injector-svc.default,vault-agent-injector-svc.default.svc\n            - name: AGENT_INJECT_LOG_FORMAT\n              value: standard\n            - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n              value: \"false\"\n            - name: AGENT_INJECT_CPU_REQUEST\n              value: \"250m\"\n            - name: AGENT_INJECT_CPU_LIMIT\n              value: \"500m\"\n            - name: AGENT_INJECT_MEM_REQUEST\n              value: \"64Mi\"\n            - name: AGENT_INJECT_MEM_LIMIT\n              value: \"128Mi\"\n            - name: AGENT_INJECT_DEFAULT_TEMPLATE\n              value: \"map\"\n            - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n              value: \"true\"\n            \n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n          args:\n            - agent-inject\n            - 2>&1\n          livenessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n          readinessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/injector-deployment.yaml"}, "region": {"startLine": 4, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: vault\n      component: webhook\n  \n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: vault\n        component: webhook\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault-agent-injector\n                  app.kubernetes.io/instance: \"vault\"\n                  component: webhook\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      serviceAccountName: \"vault-agent-injector\"\n      hostNetwork: false\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n        - name: sidecar-injector\n          \n          image: \"hashicorp/vault-k8s:0.16.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: AGENT_INJECT_LISTEN\n              value: :8080\n            - name: AGENT_INJECT_LOG_LEVEL\n              value: info\n            - name: AGENT_INJECT_VAULT_ADDR\n              value: http://vault.default.svc:8200\n            - name: AGENT_INJECT_VAULT_AUTH_PATH\n              value: auth/kubernetes\n            - name: AGENT_INJECT_VAULT_IMAGE\n              value: \"hashicorp/vault:1.10.3\"\n            - name: AGENT_INJECT_TLS_AUTO\n              value: vault-agent-injector-cfg\n            - name: AGENT_INJECT_TLS_AUTO_HOSTS\n              value: vault-agent-injector-svc,vault-agent-injector-svc.default,vault-agent-injector-svc.default.svc\n            - name: AGENT_INJECT_LOG_FORMAT\n              value: standard\n            - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n              value: \"false\"\n            - name: AGENT_INJECT_CPU_REQUEST\n              value: \"250m\"\n            - name: AGENT_INJECT_CPU_LIMIT\n              value: \"500m\"\n            - name: AGENT_INJECT_MEM_REQUEST\n              value: \"64Mi\"\n            - name: AGENT_INJECT_MEM_LIMIT\n              value: \"128Mi\"\n            - name: AGENT_INJECT_DEFAULT_TEMPLATE\n              value: \"map\"\n            - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n              value: \"true\"\n            \n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n          args:\n            - agent-inject\n            - 2>&1\n          livenessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n          readinessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/injector-deployment.yaml"}, "region": {"startLine": 4, "endLine": 114, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: vault\n      component: webhook\n  \n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: vault\n        component: webhook\n    spec:\n      \n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: vault-agent-injector\n                  app.kubernetes.io/instance: \"vault\"\n                  component: webhook\n              topologyKey: kubernetes.io/hostname\n  \n      \n      \n      \n      serviceAccountName: \"vault-agent-injector\"\n      hostNetwork: false\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n        - name: sidecar-injector\n          \n          image: \"hashicorp/vault-k8s:0.16.1\"\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            allowPrivilegeEscalation: false\n          env:\n            - name: AGENT_INJECT_LISTEN\n              value: :8080\n            - name: AGENT_INJECT_LOG_LEVEL\n              value: info\n            - name: AGENT_INJECT_VAULT_ADDR\n              value: http://vault.default.svc:8200\n            - name: AGENT_INJECT_VAULT_AUTH_PATH\n              value: auth/kubernetes\n            - name: AGENT_INJECT_VAULT_IMAGE\n              value: \"hashicorp/vault:1.10.3\"\n            - name: AGENT_INJECT_TLS_AUTO\n              value: vault-agent-injector-cfg\n            - name: AGENT_INJECT_TLS_AUTO_HOSTS\n              value: vault-agent-injector-svc,vault-agent-injector-svc.default,vault-agent-injector-svc.default.svc\n            - name: AGENT_INJECT_LOG_FORMAT\n              value: standard\n            - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n              value: \"false\"\n            - name: AGENT_INJECT_CPU_REQUEST\n              value: \"250m\"\n            - name: AGENT_INJECT_CPU_LIMIT\n              value: \"500m\"\n            - name: AGENT_INJECT_MEM_REQUEST\n              value: \"64Mi\"\n            - name: AGENT_INJECT_MEM_LIMIT\n              value: \"128Mi\"\n            - name: AGENT_INJECT_DEFAULT_TEMPLATE\n              value: \"map\"\n            - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n              value: \"true\"\n            \n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n          args:\n            - agent-inject\n            - 2>&1\n          livenessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n          readinessProbe:\n            httpGet:\n              path: /health/ready\n              port: 8080\n              scheme: HTTPS\n            failureThreshold: 2\n            initialDelaySeconds: 5\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 5\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/server-serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 12, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: vault\n  namespace: default\n  labels:\n    helm.sh/chart: vault-0.20.1\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/injector-serviceaccount.yaml"}, "region": {"startLine": 3, "endLine": 11, "snippet": {"text": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/injector-service.yaml"}, "region": {"startLine": 3, "endLine": 21, "snippet": {"text": "apiVersion: v1\nkind: Service\nmetadata:\n  name: vault-agent-injector-svc\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/managed-by: Helm\n  \nspec:\n  ports:\n  - name: https\n    port: 443\n    targetPort: 8080\n  selector:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: vault\n    component: webhook\n"}}}}]}, {"ruleId": "CKV_K8S_37", "ruleIndex": 0, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with capabilities assigned"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/tests/server-test.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vault-server-test\"\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  \n  containers:\n    - name: vault-server-test\n      image: hashicorp/vault:1.10.3\n      imagePullPolicy: IfNotPresent\n      env:\n        - name: VAULT_ADDR\n          value: http://vault.default.svc:8200\n        \n      command:\n        - /bin/sh\n        - -c\n        - |\n          echo \"Checking for sealed info in 'vault status' output\"\n          ATTEMPTS=10\n          n=0\n          until [ \"$n\" -ge $ATTEMPTS ]\n          do\n            echo \"Attempt\" $n...\n            vault status -format yaml | grep -E '^sealed: (true|false)' && break\n            n=$((n+1))\n            sleep 5\n          done\n          if [ $n -ge $ATTEMPTS ]; then\n            echo \"timed out looking for sealed info in 'vault status' output\"\n            exit 1\n          fi\n\n          exit 0\n      volumeMounts:\n  volumes:\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/tests/server-test.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vault-server-test\"\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  \n  containers:\n    - name: vault-server-test\n      image: hashicorp/vault:1.10.3\n      imagePullPolicy: IfNotPresent\n      env:\n        - name: VAULT_ADDR\n          value: http://vault.default.svc:8200\n        \n      command:\n        - /bin/sh\n        - -c\n        - |\n          echo \"Checking for sealed info in 'vault status' output\"\n          ATTEMPTS=10\n          n=0\n          until [ \"$n\" -ge $ATTEMPTS ]\n          do\n            echo \"Attempt\" $n...\n            vault status -format yaml | grep -E '^sealed: (true|false)' && break\n            n=$((n+1))\n            sleep 5\n          done\n          if [ $n -ge $ATTEMPTS ]; then\n            echo \"timed out looking for sealed info in 'vault status' output\"\n            exit 1\n          fi\n\n          exit 0\n      volumeMounts:\n  volumes:\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_8", "ruleIndex": 14, "level": "error", "attachments": [], "message": {"text": "Liveness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/tests/server-test.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vault-server-test\"\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  \n  containers:\n    - name: vault-server-test\n      image: hashicorp/vault:1.10.3\n      imagePullPolicy: IfNotPresent\n      env:\n        - name: VAULT_ADDR\n          value: http://vault.default.svc:8200\n        \n      command:\n        - /bin/sh\n        - -c\n        - |\n          echo \"Checking for sealed info in 'vault status' output\"\n          ATTEMPTS=10\n          n=0\n          until [ \"$n\" -ge $ATTEMPTS ]\n          do\n            echo \"Attempt\" $n...\n            vault status -format yaml | grep -E '^sealed: (true|false)' && break\n            n=$((n+1))\n            sleep 5\n          done\n          if [ $n -ge $ATTEMPTS ]; then\n            echo \"timed out looking for sealed info in 'vault status' output\"\n            exit 1\n          fi\n\n          exit 0\n      volumeMounts:\n  volumes:\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_12", "ruleIndex": 15, "level": "error", "attachments": [], "message": {"text": "Memory requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/tests/server-test.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vault-server-test\"\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  \n  containers:\n    - name: vault-server-test\n      image: hashicorp/vault:1.10.3\n      imagePullPolicy: IfNotPresent\n      env:\n        - name: VAULT_ADDR\n          value: http://vault.default.svc:8200\n        \n      command:\n        - /bin/sh\n        - -c\n        - |\n          echo \"Checking for sealed info in 'vault status' output\"\n          ATTEMPTS=10\n          n=0\n          until [ \"$n\" -ge $ATTEMPTS ]\n          do\n            echo \"Attempt\" $n...\n            vault status -format yaml | grep -E '^sealed: (true|false)' && break\n            n=$((n+1))\n            sleep 5\n          done\n          if [ $n -ge $ATTEMPTS ]; then\n            echo \"timed out looking for sealed info in 'vault status' output\"\n            exit 1\n          fi\n\n          exit 0\n      volumeMounts:\n  volumes:\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_20", "ruleIndex": 16, "level": "error", "attachments": [], "message": {"text": "Containers should not run with allowPrivilegeEscalation"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/tests/server-test.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vault-server-test\"\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  \n  containers:\n    - name: vault-server-test\n      image: hashicorp/vault:1.10.3\n      imagePullPolicy: IfNotPresent\n      env:\n        - name: VAULT_ADDR\n          value: http://vault.default.svc:8200\n        \n      command:\n        - /bin/sh\n        - -c\n        - |\n          echo \"Checking for sealed info in 'vault status' output\"\n          ATTEMPTS=10\n          n=0\n          until [ \"$n\" -ge $ATTEMPTS ]\n          do\n            echo \"Attempt\" $n...\n            vault status -format yaml | grep -E '^sealed: (true|false)' && break\n            n=$((n+1))\n            sleep 5\n          done\n          if [ $n -ge $ATTEMPTS ]; then\n            echo \"timed out looking for sealed info in 'vault status' output\"\n            exit 1\n          fi\n\n          exit 0\n      volumeMounts:\n  volumes:\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/tests/server-test.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vault-server-test\"\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  \n  containers:\n    - name: vault-server-test\n      image: hashicorp/vault:1.10.3\n      imagePullPolicy: IfNotPresent\n      env:\n        - name: VAULT_ADDR\n          value: http://vault.default.svc:8200\n        \n      command:\n        - /bin/sh\n        - -c\n        - |\n          echo \"Checking for sealed info in 'vault status' output\"\n          ATTEMPTS=10\n          n=0\n          until [ \"$n\" -ge $ATTEMPTS ]\n          do\n            echo \"Attempt\" $n...\n            vault status -format yaml | grep -E '^sealed: (true|false)' && break\n            n=$((n+1))\n            sleep 5\n          done\n          if [ $n -ge $ATTEMPTS ]; then\n            echo \"timed out looking for sealed info in 'vault status' output\"\n            exit 1\n          fi\n\n          exit 0\n      volumeMounts:\n  volumes:\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/tests/server-test.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vault-server-test\"\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  \n  containers:\n    - name: vault-server-test\n      image: hashicorp/vault:1.10.3\n      imagePullPolicy: IfNotPresent\n      env:\n        - name: VAULT_ADDR\n          value: http://vault.default.svc:8200\n        \n      command:\n        - /bin/sh\n        - -c\n        - |\n          echo \"Checking for sealed info in 'vault status' output\"\n          ATTEMPTS=10\n          n=0\n          until [ \"$n\" -ge $ATTEMPTS ]\n          do\n            echo \"Attempt\" $n...\n            vault status -format yaml | grep -E '^sealed: (true|false)' && break\n            n=$((n+1))\n            sleep 5\n          done\n          if [ $n -ge $ATTEMPTS ]; then\n            echo \"timed out looking for sealed info in 'vault status' output\"\n            exit 1\n          fi\n\n          exit 0\n      volumeMounts:\n  volumes:\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/tests/server-test.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vault-server-test\"\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  \n  containers:\n    - name: vault-server-test\n      image: hashicorp/vault:1.10.3\n      imagePullPolicy: IfNotPresent\n      env:\n        - name: VAULT_ADDR\n          value: http://vault.default.svc:8200\n        \n      command:\n        - /bin/sh\n        - -c\n        - |\n          echo \"Checking for sealed info in 'vault status' output\"\n          ATTEMPTS=10\n          n=0\n          until [ \"$n\" -ge $ATTEMPTS ]\n          do\n            echo \"Attempt\" $n...\n            vault status -format yaml | grep -E '^sealed: (true|false)' && break\n            n=$((n+1))\n            sleep 5\n          done\n          if [ $n -ge $ATTEMPTS ]; then\n            echo \"timed out looking for sealed info in 'vault status' output\"\n            exit 1\n          fi\n\n          exit 0\n      volumeMounts:\n  volumes:\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_10", "ruleIndex": 17, "level": "error", "attachments": [], "message": {"text": "CPU requests should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/tests/server-test.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vault-server-test\"\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  \n  containers:\n    - name: vault-server-test\n      image: hashicorp/vault:1.10.3\n      imagePullPolicy: IfNotPresent\n      env:\n        - name: VAULT_ADDR\n          value: http://vault.default.svc:8200\n        \n      command:\n        - /bin/sh\n        - -c\n        - |\n          echo \"Checking for sealed info in 'vault status' output\"\n          ATTEMPTS=10\n          n=0\n          until [ \"$n\" -ge $ATTEMPTS ]\n          do\n            echo \"Attempt\" $n...\n            vault status -format yaml | grep -E '^sealed: (true|false)' && break\n            n=$((n+1))\n            sleep 5\n          done\n          if [ $n -ge $ATTEMPTS ]; then\n            echo \"timed out looking for sealed info in 'vault status' output\"\n            exit 1\n          fi\n\n          exit 0\n      volumeMounts:\n  volumes:\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/tests/server-test.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vault-server-test\"\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  \n  containers:\n    - name: vault-server-test\n      image: hashicorp/vault:1.10.3\n      imagePullPolicy: IfNotPresent\n      env:\n        - name: VAULT_ADDR\n          value: http://vault.default.svc:8200\n        \n      command:\n        - /bin/sh\n        - -c\n        - |\n          echo \"Checking for sealed info in 'vault status' output\"\n          ATTEMPTS=10\n          n=0\n          until [ \"$n\" -ge $ATTEMPTS ]\n          do\n            echo \"Attempt\" $n...\n            vault status -format yaml | grep -E '^sealed: (true|false)' && break\n            n=$((n+1))\n            sleep 5\n          done\n          if [ $n -ge $ATTEMPTS ]; then\n            echo \"timed out looking for sealed info in 'vault status' output\"\n            exit 1\n          fi\n\n          exit 0\n      volumeMounts:\n  volumes:\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_9", "ruleIndex": 18, "level": "error", "attachments": [], "message": {"text": "Readiness Probe Should be Configured"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/tests/server-test.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vault-server-test\"\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  \n  containers:\n    - name: vault-server-test\n      image: hashicorp/vault:1.10.3\n      imagePullPolicy: IfNotPresent\n      env:\n        - name: VAULT_ADDR\n          value: http://vault.default.svc:8200\n        \n      command:\n        - /bin/sh\n        - -c\n        - |\n          echo \"Checking for sealed info in 'vault status' output\"\n          ATTEMPTS=10\n          n=0\n          until [ \"$n\" -ge $ATTEMPTS ]\n          do\n            echo \"Attempt\" $n...\n            vault status -format yaml | grep -E '^sealed: (true|false)' && break\n            n=$((n+1))\n            sleep 5\n          done\n          if [ $n -ge $ATTEMPTS ]; then\n            echo \"timed out looking for sealed info in 'vault status' output\"\n            exit 1\n          fi\n\n          exit 0\n      volumeMounts:\n  volumes:\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_28", "ruleIndex": 7, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of containers with the NET_RAW capability"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/tests/server-test.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vault-server-test\"\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  \n  containers:\n    - name: vault-server-test\n      image: hashicorp/vault:1.10.3\n      imagePullPolicy: IfNotPresent\n      env:\n        - name: VAULT_ADDR\n          value: http://vault.default.svc:8200\n        \n      command:\n        - /bin/sh\n        - -c\n        - |\n          echo \"Checking for sealed info in 'vault status' output\"\n          ATTEMPTS=10\n          n=0\n          until [ \"$n\" -ge $ATTEMPTS ]\n          do\n            echo \"Attempt\" $n...\n            vault status -format yaml | grep -E '^sealed: (true|false)' && break\n            n=$((n+1))\n            sleep 5\n          done\n          if [ $n -ge $ATTEMPTS ]; then\n            echo \"timed out looking for sealed info in 'vault status' output\"\n            exit 1\n          fi\n\n          exit 0\n      volumeMounts:\n  volumes:\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_29", "ruleIndex": 19, "level": "error", "attachments": [], "message": {"text": "Apply security context to your pods and containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/tests/server-test.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vault-server-test\"\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  \n  containers:\n    - name: vault-server-test\n      image: hashicorp/vault:1.10.3\n      imagePullPolicy: IfNotPresent\n      env:\n        - name: VAULT_ADDR\n          value: http://vault.default.svc:8200\n        \n      command:\n        - /bin/sh\n        - -c\n        - |\n          echo \"Checking for sealed info in 'vault status' output\"\n          ATTEMPTS=10\n          n=0\n          until [ \"$n\" -ge $ATTEMPTS ]\n          do\n            echo \"Attempt\" $n...\n            vault status -format yaml | grep -E '^sealed: (true|false)' && break\n            n=$((n+1))\n            sleep 5\n          done\n          if [ $n -ge $ATTEMPTS ]; then\n            echo \"timed out looking for sealed info in 'vault status' output\"\n            exit 1\n          fi\n\n          exit 0\n      volumeMounts:\n  volumes:\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_30", "ruleIndex": 20, "level": "error", "attachments": [], "message": {"text": "Apply security context to your containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/tests/server-test.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vault-server-test\"\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  \n  containers:\n    - name: vault-server-test\n      image: hashicorp/vault:1.10.3\n      imagePullPolicy: IfNotPresent\n      env:\n        - name: VAULT_ADDR\n          value: http://vault.default.svc:8200\n        \n      command:\n        - /bin/sh\n        - -c\n        - |\n          echo \"Checking for sealed info in 'vault status' output\"\n          ATTEMPTS=10\n          n=0\n          until [ \"$n\" -ge $ATTEMPTS ]\n          do\n            echo \"Attempt\" $n...\n            vault status -format yaml | grep -E '^sealed: (true|false)' && break\n            n=$((n+1))\n            sleep 5\n          done\n          if [ $n -ge $ATTEMPTS ]; then\n            echo \"timed out looking for sealed info in 'vault status' output\"\n            exit 1\n          fi\n\n          exit 0\n      volumeMounts:\n  volumes:\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/tests/server-test.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vault-server-test\"\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  \n  containers:\n    - name: vault-server-test\n      image: hashicorp/vault:1.10.3\n      imagePullPolicy: IfNotPresent\n      env:\n        - name: VAULT_ADDR\n          value: http://vault.default.svc:8200\n        \n      command:\n        - /bin/sh\n        - -c\n        - |\n          echo \"Checking for sealed info in 'vault status' output\"\n          ATTEMPTS=10\n          n=0\n          until [ \"$n\" -ge $ATTEMPTS ]\n          do\n            echo \"Attempt\" $n...\n            vault status -format yaml | grep -E '^sealed: (true|false)' && break\n            n=$((n+1))\n            sleep 5\n          done\n          if [ $n -ge $ATTEMPTS ]; then\n            echo \"timed out looking for sealed info in 'vault status' output\"\n            exit 1\n          fi\n\n          exit 0\n      volumeMounts:\n  volumes:\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/tests/server-test.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vault-server-test\"\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  \n  containers:\n    - name: vault-server-test\n      image: hashicorp/vault:1.10.3\n      imagePullPolicy: IfNotPresent\n      env:\n        - name: VAULT_ADDR\n          value: http://vault.default.svc:8200\n        \n      command:\n        - /bin/sh\n        - -c\n        - |\n          echo \"Checking for sealed info in 'vault status' output\"\n          ATTEMPTS=10\n          n=0\n          until [ \"$n\" -ge $ATTEMPTS ]\n          do\n            echo \"Attempt\" $n...\n            vault status -format yaml | grep -E '^sealed: (true|false)' && break\n            n=$((n+1))\n            sleep 5\n          done\n          if [ $n -ge $ATTEMPTS ]; then\n            echo \"timed out looking for sealed info in 'vault status' output\"\n            exit 1\n          fi\n\n          exit 0\n      volumeMounts:\n  volumes:\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_23", "ruleIndex": 11, "level": "error", "attachments": [], "message": {"text": "Minimize the admission of root containers"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/tests/server-test.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vault-server-test\"\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  \n  containers:\n    - name: vault-server-test\n      image: hashicorp/vault:1.10.3\n      imagePullPolicy: IfNotPresent\n      env:\n        - name: VAULT_ADDR\n          value: http://vault.default.svc:8200\n        \n      command:\n        - /bin/sh\n        - -c\n        - |\n          echo \"Checking for sealed info in 'vault status' output\"\n          ATTEMPTS=10\n          n=0\n          until [ \"$n\" -ge $ATTEMPTS ]\n          do\n            echo \"Attempt\" $n...\n            vault status -format yaml | grep -E '^sealed: (true|false)' && break\n            n=$((n+1))\n            sleep 5\n          done\n          if [ $n -ge $ATTEMPTS ]; then\n            echo \"timed out looking for sealed info in 'vault status' output\"\n            exit 1\n          fi\n\n          exit 0\n      volumeMounts:\n  volumes:\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/tests/server-test.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vault-server-test\"\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  \n  containers:\n    - name: vault-server-test\n      image: hashicorp/vault:1.10.3\n      imagePullPolicy: IfNotPresent\n      env:\n        - name: VAULT_ADDR\n          value: http://vault.default.svc:8200\n        \n      command:\n        - /bin/sh\n        - -c\n        - |\n          echo \"Checking for sealed info in 'vault status' output\"\n          ATTEMPTS=10\n          n=0\n          until [ \"$n\" -ge $ATTEMPTS ]\n          do\n            echo \"Attempt\" $n...\n            vault status -format yaml | grep -E '^sealed: (true|false)' && break\n            n=$((n+1))\n            sleep 5\n          done\n          if [ $n -ge $ATTEMPTS ]; then\n            echo \"timed out looking for sealed info in 'vault status' output\"\n            exit 1\n          fi\n\n          exit 0\n      volumeMounts:\n  volumes:\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/vault/templates/tests/server-test.yaml"}, "region": {"startLine": 3, "endLine": 42, "snippet": {"text": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: \"vault-server-test\"\n  namespace: default\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  \n  containers:\n    - name: vault-server-test\n      image: hashicorp/vault:1.10.3\n      imagePullPolicy: IfNotPresent\n      env:\n        - name: VAULT_ADDR\n          value: http://vault.default.svc:8200\n        \n      command:\n        - /bin/sh\n        - -c\n        - |\n          echo \"Checking for sealed info in 'vault status' output\"\n          ATTEMPTS=10\n          n=0\n          until [ \"$n\" -ge $ATTEMPTS ]\n          do\n            echo \"Attempt\" $n...\n            vault status -format yaml | grep -E '^sealed: (true|false)' && break\n            n=$((n+1))\n            sleep 5\n          done\n          if [ $n -ge $ATTEMPTS ]; then\n            echo \"timed out looking for sealed info in 'vault status' output\"\n            exit 1\n          fi\n\n          exit 0\n      volumeMounts:\n  volumes:\n  restartPolicy: Never\n"}}}}]}, {"ruleId": "CKV_K8S_31", "ruleIndex": 1, "level": "error", "attachments": [], "message": {"text": "Ensure that the seccomp profile is set to docker/default or runtime/default"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-runner/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 108, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gitlab-runner\n  namespace: \"default\"\n  labels:\n    app: gitlab-runner\n    chart: gitlab-runner-0.48.1\n    release: \"gitlab-runner\"\n    heritage: \"Helm\"\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: gitlab-runner\n  template:\n    metadata:\n      labels:\n        app: gitlab-runner\n        chart: gitlab-runner-0.48.1\n        release: \"gitlab-runner\"\n        heritage: \"Helm\"\n      annotations:\n        checksum/configmap: 4672bdd8bf9d5f63bcdb0f43209485755b45242539db65056005c6cdc29f4e77\n        checksum/secrets: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n    spec:\n      securityContext: \n        fsGroup: 65533\n        runAsUser: 100\n      terminationGracePeriodSeconds: 3600\n      serviceAccountName: \"\"\n      containers:\n      - name: gitlab-runner\n        image: registry.gitlab.com/gitlab-org/gitlab-runner:alpine-v15.7.2\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext: \n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        \n        command: [\"/usr/bin/dumb-init\", \"--\", \"/bin/bash\", \"/configmaps/entrypoint\"]\n        env:\n                \n        - name: CI_SERVER_URL\n          value: \n        - name: CLONE_URL\n          value: \"\"\n        - name: RUNNER_EXECUTOR\n          value: \"kubernetes\"\n        - name: REGISTER_LOCKED\n          value: \"true\"\n        - name: RUNNER_TAG_LIST\n          value: \"\"\n        livenessProbe:\n          exec:\n            command: [\"/bin/bash\", \"/configmaps/check-live\"]\n          initialDelaySeconds: 60\n          timeoutSeconds: 1\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          exec:\n            command: [\"/usr/bin/pgrep\",\"gitlab.*runner\"]\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 3\n        ports:\n        - name: \"metrics\"\n          containerPort: 9252\n        volumeMounts:\n        - name: projected-secrets\n          mountPath: /secrets\n        - name: etc-gitlab-runner\n          mountPath: /home/gitlab-runner/.gitlab-runner\n        - name: configmaps\n          mountPath: /configmaps\n        resources:\n          {}\n      volumes:\n      - name: runner-secrets\n        emptyDir:\n          medium: \"Memory\"\n      - name: etc-gitlab-runner\n        emptyDir:\n          medium: \"Memory\"\n      - name: projected-secrets\n        projected:\n          sources:\n            - secret:\n                name: \"gitlab-runner\"\n                items:\n                  - key: runner-registration-token\n                    path: runner-registration-token\n                  - key: runner-token\n                    path: runner-token\n      - name: configmaps\n        configMap:\n          name: gitlab-runner\n"}}}}]}, {"ruleId": "CKV_K8S_15", "ruleIndex": 2, "level": "error", "attachments": [], "message": {"text": "Image Pull Policy should be Always"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-runner/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 108, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gitlab-runner\n  namespace: \"default\"\n  labels:\n    app: gitlab-runner\n    chart: gitlab-runner-0.48.1\n    release: \"gitlab-runner\"\n    heritage: \"Helm\"\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: gitlab-runner\n  template:\n    metadata:\n      labels:\n        app: gitlab-runner\n        chart: gitlab-runner-0.48.1\n        release: \"gitlab-runner\"\n        heritage: \"Helm\"\n      annotations:\n        checksum/configmap: 4672bdd8bf9d5f63bcdb0f43209485755b45242539db65056005c6cdc29f4e77\n        checksum/secrets: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n    spec:\n      securityContext: \n        fsGroup: 65533\n        runAsUser: 100\n      terminationGracePeriodSeconds: 3600\n      serviceAccountName: \"\"\n      containers:\n      - name: gitlab-runner\n        image: registry.gitlab.com/gitlab-org/gitlab-runner:alpine-v15.7.2\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext: \n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        \n        command: [\"/usr/bin/dumb-init\", \"--\", \"/bin/bash\", \"/configmaps/entrypoint\"]\n        env:\n                \n        - name: CI_SERVER_URL\n          value: \n        - name: CLONE_URL\n          value: \"\"\n        - name: RUNNER_EXECUTOR\n          value: \"kubernetes\"\n        - name: REGISTER_LOCKED\n          value: \"true\"\n        - name: RUNNER_TAG_LIST\n          value: \"\"\n        livenessProbe:\n          exec:\n            command: [\"/bin/bash\", \"/configmaps/check-live\"]\n          initialDelaySeconds: 60\n          timeoutSeconds: 1\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          exec:\n            command: [\"/usr/bin/pgrep\",\"gitlab.*runner\"]\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 3\n        ports:\n        - name: \"metrics\"\n          containerPort: 9252\n        volumeMounts:\n        - name: projected-secrets\n          mountPath: /secrets\n        - name: etc-gitlab-runner\n          mountPath: /home/gitlab-runner/.gitlab-runner\n        - name: configmaps\n          mountPath: /configmaps\n        resources:\n          {}\n      volumes:\n      - name: runner-secrets\n        emptyDir:\n          medium: \"Memory\"\n      - name: etc-gitlab-runner\n        emptyDir:\n          medium: \"Memory\"\n      - name: projected-secrets\n        projected:\n          sources:\n            - secret:\n                name: \"gitlab-runner\"\n                items:\n                  - key: runner-registration-token\n                    path: runner-registration-token\n                  - key: runner-token\n                    path: runner-token\n      - name: configmaps\n        configMap:\n          name: gitlab-runner\n"}}}}]}, {"ruleId": "CKV_K8S_13", "ruleIndex": 3, "level": "error", "attachments": [], "message": {"text": "Memory limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-runner/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 108, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gitlab-runner\n  namespace: \"default\"\n  labels:\n    app: gitlab-runner\n    chart: gitlab-runner-0.48.1\n    release: \"gitlab-runner\"\n    heritage: \"Helm\"\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: gitlab-runner\n  template:\n    metadata:\n      labels:\n        app: gitlab-runner\n        chart: gitlab-runner-0.48.1\n        release: \"gitlab-runner\"\n        heritage: \"Helm\"\n      annotations:\n        checksum/configmap: 4672bdd8bf9d5f63bcdb0f43209485755b45242539db65056005c6cdc29f4e77\n        checksum/secrets: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n    spec:\n      securityContext: \n        fsGroup: 65533\n        runAsUser: 100\n      terminationGracePeriodSeconds: 3600\n      serviceAccountName: \"\"\n      containers:\n      - name: gitlab-runner\n        image: registry.gitlab.com/gitlab-org/gitlab-runner:alpine-v15.7.2\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext: \n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        \n        command: [\"/usr/bin/dumb-init\", \"--\", \"/bin/bash\", \"/configmaps/entrypoint\"]\n        env:\n                \n        - name: CI_SERVER_URL\n          value: \n        - name: CLONE_URL\n          value: \"\"\n        - name: RUNNER_EXECUTOR\n          value: \"kubernetes\"\n        - name: REGISTER_LOCKED\n          value: \"true\"\n        - name: RUNNER_TAG_LIST\n          value: \"\"\n        livenessProbe:\n          exec:\n            command: [\"/bin/bash\", \"/configmaps/check-live\"]\n          initialDelaySeconds: 60\n          timeoutSeconds: 1\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          exec:\n            command: [\"/usr/bin/pgrep\",\"gitlab.*runner\"]\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 3\n        ports:\n        - name: \"metrics\"\n          containerPort: 9252\n        volumeMounts:\n        - name: projected-secrets\n          mountPath: /secrets\n        - name: etc-gitlab-runner\n          mountPath: /home/gitlab-runner/.gitlab-runner\n        - name: configmaps\n          mountPath: /configmaps\n        resources:\n          {}\n      volumes:\n      - name: runner-secrets\n        emptyDir:\n          medium: \"Memory\"\n      - name: etc-gitlab-runner\n        emptyDir:\n          medium: \"Memory\"\n      - name: projected-secrets\n        projected:\n          sources:\n            - secret:\n                name: \"gitlab-runner\"\n                items:\n                  - key: runner-registration-token\n                    path: runner-registration-token\n                  - key: runner-token\n                    path: runner-token\n      - name: configmaps\n        configMap:\n          name: gitlab-runner\n"}}}}]}, {"ruleId": "CKV_K8S_40", "ruleIndex": 4, "level": "error", "attachments": [], "message": {"text": "Containers should run as a high UID to avoid host conflict"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-runner/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 108, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gitlab-runner\n  namespace: \"default\"\n  labels:\n    app: gitlab-runner\n    chart: gitlab-runner-0.48.1\n    release: \"gitlab-runner\"\n    heritage: \"Helm\"\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: gitlab-runner\n  template:\n    metadata:\n      labels:\n        app: gitlab-runner\n        chart: gitlab-runner-0.48.1\n        release: \"gitlab-runner\"\n        heritage: \"Helm\"\n      annotations:\n        checksum/configmap: 4672bdd8bf9d5f63bcdb0f43209485755b45242539db65056005c6cdc29f4e77\n        checksum/secrets: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n    spec:\n      securityContext: \n        fsGroup: 65533\n        runAsUser: 100\n      terminationGracePeriodSeconds: 3600\n      serviceAccountName: \"\"\n      containers:\n      - name: gitlab-runner\n        image: registry.gitlab.com/gitlab-org/gitlab-runner:alpine-v15.7.2\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext: \n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        \n        command: [\"/usr/bin/dumb-init\", \"--\", \"/bin/bash\", \"/configmaps/entrypoint\"]\n        env:\n                \n        - name: CI_SERVER_URL\n          value: \n        - name: CLONE_URL\n          value: \"\"\n        - name: RUNNER_EXECUTOR\n          value: \"kubernetes\"\n        - name: REGISTER_LOCKED\n          value: \"true\"\n        - name: RUNNER_TAG_LIST\n          value: \"\"\n        livenessProbe:\n          exec:\n            command: [\"/bin/bash\", \"/configmaps/check-live\"]\n          initialDelaySeconds: 60\n          timeoutSeconds: 1\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          exec:\n            command: [\"/usr/bin/pgrep\",\"gitlab.*runner\"]\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 3\n        ports:\n        - name: \"metrics\"\n          containerPort: 9252\n        volumeMounts:\n        - name: projected-secrets\n          mountPath: /secrets\n        - name: etc-gitlab-runner\n          mountPath: /home/gitlab-runner/.gitlab-runner\n        - name: configmaps\n          mountPath: /configmaps\n        resources:\n          {}\n      volumes:\n      - name: runner-secrets\n        emptyDir:\n          medium: \"Memory\"\n      - name: etc-gitlab-runner\n        emptyDir:\n          medium: \"Memory\"\n      - name: projected-secrets\n        projected:\n          sources:\n            - secret:\n                name: \"gitlab-runner\"\n                items:\n                  - key: runner-registration-token\n                    path: runner-registration-token\n                  - key: runner-token\n                    path: runner-token\n      - name: configmaps\n        configMap:\n          name: gitlab-runner\n"}}}}]}, {"ruleId": "CKV_K8S_22", "ruleIndex": 5, "level": "error", "attachments": [], "message": {"text": "Use read-only filesystem for containers where possible"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-runner/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 108, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gitlab-runner\n  namespace: \"default\"\n  labels:\n    app: gitlab-runner\n    chart: gitlab-runner-0.48.1\n    release: \"gitlab-runner\"\n    heritage: \"Helm\"\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: gitlab-runner\n  template:\n    metadata:\n      labels:\n        app: gitlab-runner\n        chart: gitlab-runner-0.48.1\n        release: \"gitlab-runner\"\n        heritage: \"Helm\"\n      annotations:\n        checksum/configmap: 4672bdd8bf9d5f63bcdb0f43209485755b45242539db65056005c6cdc29f4e77\n        checksum/secrets: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n    spec:\n      securityContext: \n        fsGroup: 65533\n        runAsUser: 100\n      terminationGracePeriodSeconds: 3600\n      serviceAccountName: \"\"\n      containers:\n      - name: gitlab-runner\n        image: registry.gitlab.com/gitlab-org/gitlab-runner:alpine-v15.7.2\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext: \n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        \n        command: [\"/usr/bin/dumb-init\", \"--\", \"/bin/bash\", \"/configmaps/entrypoint\"]\n        env:\n                \n        - name: CI_SERVER_URL\n          value: \n        - name: CLONE_URL\n          value: \"\"\n        - name: RUNNER_EXECUTOR\n          value: \"kubernetes\"\n        - name: REGISTER_LOCKED\n          value: \"true\"\n        - name: RUNNER_TAG_LIST\n          value: \"\"\n        livenessProbe:\n          exec:\n            command: [\"/bin/bash\", \"/configmaps/check-live\"]\n          initialDelaySeconds: 60\n          timeoutSeconds: 1\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          exec:\n            command: [\"/usr/bin/pgrep\",\"gitlab.*runner\"]\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 3\n        ports:\n        - name: \"metrics\"\n          containerPort: 9252\n        volumeMounts:\n        - name: projected-secrets\n          mountPath: /secrets\n        - name: etc-gitlab-runner\n          mountPath: /home/gitlab-runner/.gitlab-runner\n        - name: configmaps\n          mountPath: /configmaps\n        resources:\n          {}\n      volumes:\n      - name: runner-secrets\n        emptyDir:\n          medium: \"Memory\"\n      - name: etc-gitlab-runner\n        emptyDir:\n          medium: \"Memory\"\n      - name: projected-secrets\n        projected:\n          sources:\n            - secret:\n                name: \"gitlab-runner\"\n                items:\n                  - key: runner-registration-token\n                    path: runner-registration-token\n                  - key: runner-token\n                    path: runner-token\n      - name: configmaps\n        configMap:\n          name: gitlab-runner\n"}}}}]}, {"ruleId": "CKV_K8S_38", "ruleIndex": 9, "level": "error", "attachments": [], "message": {"text": "Ensure that Service Account Tokens are only mounted where necessary"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-runner/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 108, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gitlab-runner\n  namespace: \"default\"\n  labels:\n    app: gitlab-runner\n    chart: gitlab-runner-0.48.1\n    release: \"gitlab-runner\"\n    heritage: \"Helm\"\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: gitlab-runner\n  template:\n    metadata:\n      labels:\n        app: gitlab-runner\n        chart: gitlab-runner-0.48.1\n        release: \"gitlab-runner\"\n        heritage: \"Helm\"\n      annotations:\n        checksum/configmap: 4672bdd8bf9d5f63bcdb0f43209485755b45242539db65056005c6cdc29f4e77\n        checksum/secrets: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n    spec:\n      securityContext: \n        fsGroup: 65533\n        runAsUser: 100\n      terminationGracePeriodSeconds: 3600\n      serviceAccountName: \"\"\n      containers:\n      - name: gitlab-runner\n        image: registry.gitlab.com/gitlab-org/gitlab-runner:alpine-v15.7.2\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext: \n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        \n        command: [\"/usr/bin/dumb-init\", \"--\", \"/bin/bash\", \"/configmaps/entrypoint\"]\n        env:\n                \n        - name: CI_SERVER_URL\n          value: \n        - name: CLONE_URL\n          value: \"\"\n        - name: RUNNER_EXECUTOR\n          value: \"kubernetes\"\n        - name: REGISTER_LOCKED\n          value: \"true\"\n        - name: RUNNER_TAG_LIST\n          value: \"\"\n        livenessProbe:\n          exec:\n            command: [\"/bin/bash\", \"/configmaps/check-live\"]\n          initialDelaySeconds: 60\n          timeoutSeconds: 1\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          exec:\n            command: [\"/usr/bin/pgrep\",\"gitlab.*runner\"]\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 3\n        ports:\n        - name: \"metrics\"\n          containerPort: 9252\n        volumeMounts:\n        - name: projected-secrets\n          mountPath: /secrets\n        - name: etc-gitlab-runner\n          mountPath: /home/gitlab-runner/.gitlab-runner\n        - name: configmaps\n          mountPath: /configmaps\n        resources:\n          {}\n      volumes:\n      - name: runner-secrets\n        emptyDir:\n          medium: \"Memory\"\n      - name: etc-gitlab-runner\n        emptyDir:\n          medium: \"Memory\"\n      - name: projected-secrets\n        projected:\n          sources:\n            - secret:\n                name: \"gitlab-runner\"\n                items:\n                  - key: runner-registration-token\n                    path: runner-registration-token\n                  - key: runner-token\n                    path: runner-token\n      - name: configmaps\n        configMap:\n          name: gitlab-runner\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-runner/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 108, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gitlab-runner\n  namespace: \"default\"\n  labels:\n    app: gitlab-runner\n    chart: gitlab-runner-0.48.1\n    release: \"gitlab-runner\"\n    heritage: \"Helm\"\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: gitlab-runner\n  template:\n    metadata:\n      labels:\n        app: gitlab-runner\n        chart: gitlab-runner-0.48.1\n        release: \"gitlab-runner\"\n        heritage: \"Helm\"\n      annotations:\n        checksum/configmap: 4672bdd8bf9d5f63bcdb0f43209485755b45242539db65056005c6cdc29f4e77\n        checksum/secrets: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n    spec:\n      securityContext: \n        fsGroup: 65533\n        runAsUser: 100\n      terminationGracePeriodSeconds: 3600\n      serviceAccountName: \"\"\n      containers:\n      - name: gitlab-runner\n        image: registry.gitlab.com/gitlab-org/gitlab-runner:alpine-v15.7.2\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext: \n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        \n        command: [\"/usr/bin/dumb-init\", \"--\", \"/bin/bash\", \"/configmaps/entrypoint\"]\n        env:\n                \n        - name: CI_SERVER_URL\n          value: \n        - name: CLONE_URL\n          value: \"\"\n        - name: RUNNER_EXECUTOR\n          value: \"kubernetes\"\n        - name: REGISTER_LOCKED\n          value: \"true\"\n        - name: RUNNER_TAG_LIST\n          value: \"\"\n        livenessProbe:\n          exec:\n            command: [\"/bin/bash\", \"/configmaps/check-live\"]\n          initialDelaySeconds: 60\n          timeoutSeconds: 1\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          exec:\n            command: [\"/usr/bin/pgrep\",\"gitlab.*runner\"]\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 3\n        ports:\n        - name: \"metrics\"\n          containerPort: 9252\n        volumeMounts:\n        - name: projected-secrets\n          mountPath: /secrets\n        - name: etc-gitlab-runner\n          mountPath: /home/gitlab-runner/.gitlab-runner\n        - name: configmaps\n          mountPath: /configmaps\n        resources:\n          {}\n      volumes:\n      - name: runner-secrets\n        emptyDir:\n          medium: \"Memory\"\n      - name: etc-gitlab-runner\n        emptyDir:\n          medium: \"Memory\"\n      - name: projected-secrets\n        projected:\n          sources:\n            - secret:\n                name: \"gitlab-runner\"\n                items:\n                  - key: runner-registration-token\n                    path: runner-registration-token\n                  - key: runner-token\n                    path: runner-token\n      - name: configmaps\n        configMap:\n          name: gitlab-runner\n"}}}}]}, {"ruleId": "CKV_K8S_43", "ruleIndex": 12, "level": "error", "attachments": [], "message": {"text": "Image should use digest"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-runner/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 108, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gitlab-runner\n  namespace: \"default\"\n  labels:\n    app: gitlab-runner\n    chart: gitlab-runner-0.48.1\n    release: \"gitlab-runner\"\n    heritage: \"Helm\"\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: gitlab-runner\n  template:\n    metadata:\n      labels:\n        app: gitlab-runner\n        chart: gitlab-runner-0.48.1\n        release: \"gitlab-runner\"\n        heritage: \"Helm\"\n      annotations:\n        checksum/configmap: 4672bdd8bf9d5f63bcdb0f43209485755b45242539db65056005c6cdc29f4e77\n        checksum/secrets: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n    spec:\n      securityContext: \n        fsGroup: 65533\n        runAsUser: 100\n      terminationGracePeriodSeconds: 3600\n      serviceAccountName: \"\"\n      containers:\n      - name: gitlab-runner\n        image: registry.gitlab.com/gitlab-org/gitlab-runner:alpine-v15.7.2\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext: \n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        \n        command: [\"/usr/bin/dumb-init\", \"--\", \"/bin/bash\", \"/configmaps/entrypoint\"]\n        env:\n                \n        - name: CI_SERVER_URL\n          value: \n        - name: CLONE_URL\n          value: \"\"\n        - name: RUNNER_EXECUTOR\n          value: \"kubernetes\"\n        - name: REGISTER_LOCKED\n          value: \"true\"\n        - name: RUNNER_TAG_LIST\n          value: \"\"\n        livenessProbe:\n          exec:\n            command: [\"/bin/bash\", \"/configmaps/check-live\"]\n          initialDelaySeconds: 60\n          timeoutSeconds: 1\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          exec:\n            command: [\"/usr/bin/pgrep\",\"gitlab.*runner\"]\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 3\n        ports:\n        - name: \"metrics\"\n          containerPort: 9252\n        volumeMounts:\n        - name: projected-secrets\n          mountPath: /secrets\n        - name: etc-gitlab-runner\n          mountPath: /home/gitlab-runner/.gitlab-runner\n        - name: configmaps\n          mountPath: /configmaps\n        resources:\n          {}\n      volumes:\n      - name: runner-secrets\n        emptyDir:\n          medium: \"Memory\"\n      - name: etc-gitlab-runner\n        emptyDir:\n          medium: \"Memory\"\n      - name: projected-secrets\n        projected:\n          sources:\n            - secret:\n                name: \"gitlab-runner\"\n                items:\n                  - key: runner-registration-token\n                    path: runner-registration-token\n                  - key: runner-token\n                    path: runner-token\n      - name: configmaps\n        configMap:\n          name: gitlab-runner\n"}}}}]}, {"ruleId": "CKV_K8S_11", "ruleIndex": 13, "level": "error", "attachments": [], "message": {"text": "CPU limits should be set"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-runner/templates/deployment.yaml"}, "region": {"startLine": 3, "endLine": 108, "snippet": {"text": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gitlab-runner\n  namespace: \"default\"\n  labels:\n    app: gitlab-runner\n    chart: gitlab-runner-0.48.1\n    release: \"gitlab-runner\"\n    heritage: \"Helm\"\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: gitlab-runner\n  template:\n    metadata:\n      labels:\n        app: gitlab-runner\n        chart: gitlab-runner-0.48.1\n        release: \"gitlab-runner\"\n        heritage: \"Helm\"\n      annotations:\n        checksum/configmap: 4672bdd8bf9d5f63bcdb0f43209485755b45242539db65056005c6cdc29f4e77\n        checksum/secrets: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n    spec:\n      securityContext: \n        fsGroup: 65533\n        runAsUser: 100\n      terminationGracePeriodSeconds: 3600\n      serviceAccountName: \"\"\n      containers:\n      - name: gitlab-runner\n        image: registry.gitlab.com/gitlab-org/gitlab-runner:alpine-v15.7.2\n        imagePullPolicy: \"IfNotPresent\"\n        securityContext: \n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        \n        command: [\"/usr/bin/dumb-init\", \"--\", \"/bin/bash\", \"/configmaps/entrypoint\"]\n        env:\n                \n        - name: CI_SERVER_URL\n          value: \n        - name: CLONE_URL\n          value: \"\"\n        - name: RUNNER_EXECUTOR\n          value: \"kubernetes\"\n        - name: REGISTER_LOCKED\n          value: \"true\"\n        - name: RUNNER_TAG_LIST\n          value: \"\"\n        livenessProbe:\n          exec:\n            command: [\"/bin/bash\", \"/configmaps/check-live\"]\n          initialDelaySeconds: 60\n          timeoutSeconds: 1\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          exec:\n            command: [\"/usr/bin/pgrep\",\"gitlab.*runner\"]\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 3\n        ports:\n        - name: \"metrics\"\n          containerPort: 9252\n        volumeMounts:\n        - name: projected-secrets\n          mountPath: /secrets\n        - name: etc-gitlab-runner\n          mountPath: /home/gitlab-runner/.gitlab-runner\n        - name: configmaps\n          mountPath: /configmaps\n        resources:\n          {}\n      volumes:\n      - name: runner-secrets\n        emptyDir:\n          medium: \"Memory\"\n      - name: etc-gitlab-runner\n        emptyDir:\n          medium: \"Memory\"\n      - name: projected-secrets\n        projected:\n          sources:\n            - secret:\n                name: \"gitlab-runner\"\n                items:\n                  - key: runner-registration-token\n                    path: runner-registration-token\n                  - key: runner-token\n                    path: runner-token\n      - name: configmaps\n        configMap:\n          name: gitlab-runner\n"}}}}]}, {"ruleId": "CKV_K8S_21", "ruleIndex": 10, "level": "error", "attachments": [], "message": {"text": "The default namespace should not be used"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": "helm_manifests/all_helm_charts/gitlab-runner/templates/configmap.yaml"}, "region": {"startLine": 3, "endLine": 119, "snippet": {"text": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: gitlab-runner\n  namespace: \"default\"\n  labels:\n    app: gitlab-runner\n    chart: gitlab-runner-0.48.1\n    release: \"gitlab-runner\"\n    heritage: \"Helm\"\ndata:\n  entrypoint: |\n    #!/bin/bash\n    set -e\n\n    mkdir -p /home/gitlab-runner/.gitlab-runner/\n\n    cp /configmaps/config.toml /home/gitlab-runner/.gitlab-runner/\n\n    # Set up environment variables for cache\n    if [[ -f /secrets/accesskey && -f /secrets/secretkey ]]; then\n      export CACHE_S3_ACCESS_KEY=$(cat /secrets/accesskey)\n      export CACHE_S3_SECRET_KEY=$(cat /secrets/secretkey)\n    fi\n\n    if [[ -f /secrets/gcs-applicaton-credentials-file ]]; then\n      export GOOGLE_APPLICATION_CREDENTIALS=\"/secrets/gcs-applicaton-credentials-file\"\n    elif [[ -f /secrets/gcs-application-credentials-file ]]; then\n      export GOOGLE_APPLICATION_CREDENTIALS=\"/secrets/gcs-application-credentials-file\"\n    else\n      if [[ -f /secrets/gcs-access-id && -f /secrets/gcs-private-key ]]; then\n        export CACHE_GCS_ACCESS_ID=$(cat /secrets/gcs-access-id)\n        # echo -e used to make private key multiline (in google json auth key private key is oneline with \\n)\n        export CACHE_GCS_PRIVATE_KEY=$(echo -e $(cat /secrets/gcs-private-key))\n      fi\n    fi\n\n    if [[ -f /secrets/azure-account-name && -f /secrets/azure-account-key ]]; then\n      export CACHE_AZURE_ACCOUNT_NAME=$(cat /secrets/azure-account-name)\n      export CACHE_AZURE_ACCOUNT_KEY=$(cat /secrets/azure-account-key)\n    fi\n\n    if [[ -f /secrets/runner-registration-token ]]; then\n      export REGISTRATION_TOKEN=$(cat /secrets/runner-registration-token)\n    fi\n\n    if [[ -f /secrets/runner-token ]]; then\n      export CI_SERVER_TOKEN=$(cat /secrets/runner-token)\n    fi\n\n    # Validate this also at runtime in case the user has set a custom secret\n    if [[ ! -z \"$CI_SERVER_TOKEN\" && \"1\" -ne \"1\" ]]; then\n      echo \"Using a runner token with more than 1 replica is not supported.\"\n      exit 1\n    fi\n\n    # Register the runner\n    if ! sh /configmaps/register-the-runner; then\n      exit 1\n    fi\n\n    # Run pre-entrypoint-script\n    if ! bash /configmaps/pre-entrypoint-script; then\n      exit 1\n    fi\n\n    # Start the runner\n    exec /entrypoint run --user=gitlab-runner \\\n      --working-directory=/home/gitlab-runner\n\n  config.toml: |\n    concurrent = 10\n    check_interval = 30\n    log_level = \"info\"\n\n  \n  config.template.toml:   |\n    [[runners]]\n      [runners.kubernetes]\n        namespace = \"default\"\n        image = \"ubuntu:16.04\"\n  \n\n  register-the-runner: |\n    #!/bin/bash\n    MAX_REGISTER_ATTEMPTS=30\n\n    for i in $(seq 1 \"${MAX_REGISTER_ATTEMPTS}\"); do\n      echo \"Registration attempt ${i} of ${MAX_REGISTER_ATTEMPTS}\"\n      /entrypoint register \\\n        --template-config /configmaps/config.template.toml \\\n        --non-interactive\n\n      retval=$?\n\n      if [ ${retval} = 0 ]; then\n        break\n      elif [ ${i} = ${MAX_REGISTER_ATTEMPTS} ]; then\n        exit 1\n      fi\n\n      sleep 5\n    done\n\n    exit 0\n\n  check-live: |\n    #!/bin/bash\n    if /usr/bin/pgrep -f .*register-the-runner; then\n      exit 0\n    elif /usr/bin/pgrep gitlab.*runner; then\n      exit 0\n    else\n      exit 1\n    fi\n\n  pre-entrypoint-script: |\n"}}}}]}]}]}